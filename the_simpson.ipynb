{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un'analisi predittiva sulla serie tv più amata del mondo - I Simpson\n",
    "## Esame di Programmazione di Applicazioni Data Intensive\n",
    "#### Laurea in Ingegneria e Scienze Informatiche - A.A. 2019/2020\n",
    "#### Matteo Castellucci - matteo.castellucci3@studio.unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrizione del problema e della variabile da predire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vogliono analizzare le caratteristiche degli episodi della serie tv \"I Simpson\" (in originale \"The Simpsons\") con il fine di predire quanto più accuratamente possibile il [rating su IMDB](https://www.imdb.com/title/tt0096697/) di ciascun episodio. Immaginiamo perciò che tutti gli episodi non siano ancora andati in onda, in modo tale che gli spettatori non abbiano ancora potuto farsene un'opinione, ma che siano già stati prodotti e ne sia stata programmata la trasmissione, in modo tale da sapere il contenuto di ciascuno di essi e il momento in cui saranno trasmessi.\n",
    "\n",
    "Poichè il valore assoluto del rating di per sè è per noi poco significativo, essendo legato alle esperienze personali degli spettatori, cercheremo di predire una variabile categorica ottenuta da esso che ci dica se l'episodio complessivamente è ritenuto \"bello\" oppure \"brutto\" dal pubblico.\n",
    "Scegliamo le categorie secondo la seguente logica:\n",
    "- **Bello**: rating superiore a 7,5 incluso\n",
    "- **Brutto**: rating inferiore a 7,5 escluso  \n",
    "\n",
    "Dovremo perciò affrontare un problema di classificazione binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importazione delle librerie e impostazione delle opzioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa ci preoccupiamo di importare tutte le librerie, le funzioni e gli oggetti che utilizzeremo. Inoltre, scarichiamo tutti i componenti della libreria NLTK che ci serviranno e impostiamo tutte le proprietà delle librerie utili, in modo da non doverlo fare più avanti. Sopprimiamo inoltre i warning inutili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /home/matteo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/matteo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/matteo/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/matteo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os.path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from scipy.stats import norm\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Conv1D, Flatten, Dropout, Reshape\n",
    "from keras.regularizers import l2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importazione del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verranno utilizzati i dataset contenuti nel post \"The Simpsons by the data\" originariamente pubblicati da William Kukierski e, data la loro cancellazione da Kaggle, aggiunti nuovamente da Prashant Banerjee. Mi sono riservato di applicare alcune modifiche al dataset originale per renderlo importabile tramite Pandas, visto che presentava alcuni errori di formattazione. Detto questo, lo scarichiamo e lo importiamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_filename = \"the_simpsons.zip\"\n",
    "if not os.path.exists(\"the_simpsons.zip\"):\n",
    "    from urllib.request import urlretrieve\n",
    "    urlretrieve(\"https://drive.google.com/uc?export=download&id=1M4xsnaYl5KfsQ9Wx7wQ9Jc5oBdeCPcTx\", zip_filename)\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(zip_filename) as file:\n",
    "        file.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprensione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo per prima cosa i personaggi. Notiamo innanzitutto che ogni riga ha già un suo identificatore dato dalla colonna ``id`` e che, oltre a questa colonna, nel file sono presenti altre tre feature:\n",
    "\n",
    "- ``name``: il nome del personaggio\n",
    "- ``normalized_name``: il nome del personaggio costituito da soli caratteri alfabetici minuscoli\n",
    "- ``gender``: il genere sessuale del personaggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dewey Largo</td>\n",
       "      <td>dewey largo</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Children</td>\n",
       "      <td>children</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>bart simpson</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>lisa simpson</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Patty Bouvier</td>\n",
       "      <td>patty bouvier</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ned Flanders</td>\n",
       "      <td>ned flanders</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mechanical Santa</td>\n",
       "      <td>mechanical santa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tattoo Man</td>\n",
       "      <td>tattoo man</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Waylon Smithers</td>\n",
       "      <td>waylon smithers</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C. Montgomery Burns</td>\n",
       "      <td>c montgomery burns</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name     normalized_name gender\n",
       "id                                                \n",
       "6           Dewey Largo         dewey largo      m\n",
       "7              Children            children    NaN\n",
       "8          Bart Simpson        bart simpson      m\n",
       "9          Lisa Simpson        lisa simpson      f\n",
       "10        Patty Bouvier       patty bouvier      f\n",
       "11         Ned Flanders        ned flanders      m\n",
       "12     Mechanical Santa    mechanical santa    NaN\n",
       "13           Tattoo Man          tattoo man    NaN\n",
       "14      Waylon Smithers     waylon smithers      m\n",
       "15  C. Montgomery Burns  c montgomery burns      m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_characters = pd.read_csv(\"simpsons_characters.csv\", index_col=\"id\").sort_values(\"id\")\n",
    "simpsons_characters.iloc[5:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può notare, la stragrande maggioranza dei personaggi non ha genere (perchè è sconosciuto o perché è un insieme di persone) e perciò non è un elemento particolarmente significativo. Questo DataFrame verrà perciò utilizzato solo per identificare il nome dei personaggi, in quanto non riporta altro che tutti i possibili nomi di personaggi presenti nei copioni dei vari episodi, essendo presenti solo nomi unici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6722</td>\n",
       "      <td>6722</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6722</td>\n",
       "      <td>6722</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name normalized_name gender\n",
       "count   6722            6722    323\n",
       "unique  6722            6722      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_characters.describe().loc[[\"count\", \"unique\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo perciò già provvedere alla rimozione delle colonne non necessarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marge Simpson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homer Simpson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Seymour Skinner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JANEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Todd Flanders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name\n",
       "id                 \n",
       "1     Marge Simpson\n",
       "2     Homer Simpson\n",
       "3   Seymour Skinner\n",
       "4             JANEY\n",
       "5     Todd Flanders"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_characters.drop(columns=[\"normalized_name\", \"gender\"], inplace=True)\n",
    "simpsons_characters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo, come vedremo anche per gli altri, è un dataset molto pesante in memoria perchè contiene un numero molto alto di istanze (6722 per la precisione). Per questo, modifichiamo il tipo delle sue colonne in modo da ottimizzare al massimo l'uso della memoria sfruttando i tipi di Pandas. In questo modo, riusciamo a ridurre lo spazio occupato di quasi 5 volte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6722 entries, 1 to 6749\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    6722 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 506.4 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_characters.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6722 entries, 1 to 6749\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    6722 non-null   string\n",
      "dtypes: string(1)\n",
      "memory usage: 105.0 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_characters = simpsons_characters.astype({\n",
    "    \"name\": \"string\",\n",
    "})\n",
    "simpsons_characters.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osservando invece le location notiamo un pattern simile a quello già visto per i personaggi: abbiamo una colonna ``id`` che determina l'identificatore della location, mentre le feature vere e proprie sono:\n",
    "\n",
    "- ``name``: il nome della location\n",
    "- ``normalized_name``: il nome della location fatto di soli caratteri alfabetici in minuscolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Street</td>\n",
       "      <td>street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Car</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>springfield elementary school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Auditorium</td>\n",
       "      <td>auditorium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>simpson home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name                normalized_name\n",
       "id                                                              \n",
       "1                          Street                         street\n",
       "2                             Car                            car\n",
       "3   Springfield Elementary School  springfield elementary school\n",
       "4                      Auditorium                     auditorium\n",
       "5                    Simpson Home                   simpson home"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_locations = pd.read_csv(\"simpsons_locations.csv\", index_col=\"id\").sort_values(\"id\")\n",
    "simpsons_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso, il DataFrame è utile solo per poter identificare il nome dei vari luoghi in cui gli episodi si svolgono, contenendo solamente i luoghi unici che compaiono nei vari copioni degli episodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4459</td>\n",
       "      <td>4459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4459</td>\n",
       "      <td>4459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name normalized_name\n",
       "count   4459            4459\n",
       "unique  4459            4459"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_locations.describe().loc[[\"count\", \"unique\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo perciò rimuovere ``normalized_name``, non essendoci di alcuna utilità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Springfield Elementary School</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Auditorium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Simpson Home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name\n",
       "id                               \n",
       "1                          Street\n",
       "2                             Car\n",
       "3   Springfield Elementary School\n",
       "4                      Auditorium\n",
       "5                    Simpson Home"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_locations.drop(columns=\"normalized_name\", inplace=True)\n",
    "simpsons_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso l'uso di memoria incide di meno rispetto al dataset precedente. Decidiamo lo stesso di scegliere i tipi adeguati per le colonne del DataFrame, anche solo per coerenza con il caso precedente, e otteniamo anche in questo caso una diminuzione nell'uso di memoria di un fattore 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4459 entries, 1 to 4459\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    4459 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 356.8 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_locations.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4459 entries, 1 to 4459\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    4459 non-null   string\n",
      "dtypes: string(1)\n",
      "memory usage: 69.7 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_locations = simpsons_locations.astype({\n",
    "    \"name\": \"string\",\n",
    "})\n",
    "simpsons_locations.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo dataset invece è quello che contiene i dati relativi ai singoli episodi. In esso troviamo un identificatore per ciascun episodio contraddistinto dal nome ``id``, come nei precedenti dataset, mentre le altre sue feature sono:\n",
    "\n",
    "- ``image_url``: l'URL del frame che rappresenta l'episodio\n",
    "- ``imdb_rating``: il rating che l'episodio ha su IMDB\n",
    "- ``imdb_votes``: il numero di voti che hanno contribuito al rating su IMDB\n",
    "- ``number_in_season``: il numero dell'episodio all'interno della stagione\n",
    "- ``number_in_series``: il numero dell'episodio all'interno dell'intera serie\n",
    "- ``original_air_date``: la data della prima trasmissione originale dell'episodio\n",
    "- ``original_air_year``: l'anno della prima trasmissione originale dell'episodio\n",
    "- ``production_code``: il codice di produzione dell'episodio\n",
    "- ``season``: il numero della stagione dell'episodio\n",
    "- ``title``: il titolo dell'episodio\n",
    "- ``us_viewers_in_millions``: il numero di spettatori per quell'episodio in milioni\n",
    "- ``video_url``: l'URL per vedere l'episodio sul sito di Fox\n",
    "- ``views``: il numero di visualizzazioni dell'episodio sul sito di Fox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>number_in_season</th>\n",
       "      <th>number_in_series</th>\n",
       "      <th>original_air_date</th>\n",
       "      <th>original_air_year</th>\n",
       "      <th>production_code</th>\n",
       "      <th>season</th>\n",
       "      <th>title</th>\n",
       "      <th>us_viewers_in_millions</th>\n",
       "      <th>video_url</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/617/479/Simpsons_01_08.jpg</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3734.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1989-12-17</td>\n",
       "      <td>1989</td>\n",
       "      <td>7G08</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>26.7</td>\n",
       "      <td>http://www.simpsonsworld.com/video/273376835817</td>\n",
       "      <td>171408.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/265/167/Simpsons_01_02.jpg</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1990-01-14</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G02</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>24.5</td>\n",
       "      <td>http://www.simpsonsworld.com/video/283744835990</td>\n",
       "      <td>91423.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/621/883/Simpsons_01_03.jpg</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1990-01-21</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G03</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer's Odyssey</td>\n",
       "      <td>27.5</td>\n",
       "      <td>http://www.simpsonsworld.com/video/273381443699</td>\n",
       "      <td>78072.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/632/119/Simpsons_01_04__343617.jpg</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1701.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1990-01-28</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G04</td>\n",
       "      <td>1</td>\n",
       "      <td>There's No Disgrace Like Home</td>\n",
       "      <td>20.2</td>\n",
       "      <td>http://www.simpsonsworld.com/video/273392195780</td>\n",
       "      <td>67378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/274/735/Simpsons_01_05.jpg</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1732.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1990-02-04</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G05</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart the General</td>\n",
       "      <td>27.1</td>\n",
       "      <td>http://www.simpsonsworld.com/video/300934723994</td>\n",
       "      <td>63129.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               image_url  \\\n",
       "id                                                                                         \n",
       "1           http://static-media.fxx.com/img/FX_Networks_-_FXX/617/479/Simpsons_01_08.jpg   \n",
       "2           http://static-media.fxx.com/img/FX_Networks_-_FXX/265/167/Simpsons_01_02.jpg   \n",
       "3           http://static-media.fxx.com/img/FX_Networks_-_FXX/621/883/Simpsons_01_03.jpg   \n",
       "4   http://static-media.fxx.com/img/FX_Networks_-_FXX/632/119/Simpsons_01_04__343617.jpg   \n",
       "5           http://static-media.fxx.com/img/FX_Networks_-_FXX/274/735/Simpsons_01_05.jpg   \n",
       "\n",
       "    imdb_rating  imdb_votes  number_in_season  number_in_series  \\\n",
       "id                                                                \n",
       "1           8.2      3734.0                 1                 1   \n",
       "2           7.8      1973.0                 2                 2   \n",
       "3           7.5      1709.0                 3                 3   \n",
       "4           7.8      1701.0                 4                 4   \n",
       "5           8.1      1732.0                 5                 5   \n",
       "\n",
       "   original_air_date  original_air_year production_code  season  \\\n",
       "id                                                                \n",
       "1         1989-12-17               1989            7G08       1   \n",
       "2         1990-01-14               1990            7G02       1   \n",
       "3         1990-01-21               1990            7G03       1   \n",
       "4         1990-01-28               1990            7G04       1   \n",
       "5         1990-02-04               1990            7G05       1   \n",
       "\n",
       "                                title  us_viewers_in_millions  \\\n",
       "id                                                              \n",
       "1   Simpsons Roasting on an Open Fire                    26.7   \n",
       "2                     Bart the Genius                    24.5   \n",
       "3                     Homer's Odyssey                    27.5   \n",
       "4       There's No Disgrace Like Home                    20.2   \n",
       "5                    Bart the General                    27.1   \n",
       "\n",
       "                                          video_url     views  \n",
       "id                                                             \n",
       "1   http://www.simpsonsworld.com/video/273376835817  171408.0  \n",
       "2   http://www.simpsonsworld.com/video/283744835990   91423.0  \n",
       "3   http://www.simpsonsworld.com/video/273381443699   78072.0  \n",
       "4   http://www.simpsonsworld.com/video/273392195780   67378.0  \n",
       "5   http://www.simpsonsworld.com/video/300934723994   63129.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes = pd.read_csv(\"simpsons_episodes.csv\", index_col=\"id\").sort_values(\"id\")\n",
    "simpsons_episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colonne come ``image_url`` e ``video_url`` non sono di alcuna utilità, così come quelle per identificare l'episodio, ``number_in_season``, ``number_in_series``, ``production_code``, avendo già un identificatore. Le feature ``imdb_votes``, ``us_viewers_in_millions`` e ``views`` non sono note in anticipo per come abbiamo descritto il problema e ``original_air_year`` è una feature duplicata. Per tutte queste ragioni, le colonne descritte verranno eliminate dal DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>original_air_date</th>\n",
       "      <th>season</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.2</td>\n",
       "      <td>1989-12-17</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>1990-01-14</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart the Genius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.5</td>\n",
       "      <td>1990-01-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer's Odyssey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.8</td>\n",
       "      <td>1990-01-28</td>\n",
       "      <td>1</td>\n",
       "      <td>There's No Disgrace Like Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>1990-02-04</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart the General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    imdb_rating original_air_date  season                              title\n",
       "id                                                                          \n",
       "1           8.2        1989-12-17       1  Simpsons Roasting on an Open Fire\n",
       "2           7.8        1990-01-14       1                    Bart the Genius\n",
       "3           7.5        1990-01-21       1                    Homer's Odyssey\n",
       "4           7.8        1990-01-28       1      There's No Disgrace Like Home\n",
       "5           8.1        1990-02-04       1                   Bart the General"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes.drop(columns=simpsons_episodes.columns.difference([\"imdb_rating\", \"original_air_date\", \"season\", \"title\"]), inplace=True)\n",
    "simpsons_episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso l'uso di memoria è insignificante, ma per poter essere compatibile con i precedenti dataset dobbiamo trasformarlo con i tipi di dato adeguati, che inevitabilmente ci farà risparmiare non poca memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 600 entries, 1 to 600\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   imdb_rating        597 non-null    float64\n",
      " 1   original_air_date  600 non-null    object \n",
      " 2   season             600 non-null    int64  \n",
      " 3   title              600 non-null    object \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 99.0 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_episodes.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 600 entries, 1 to 600\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   imdb_rating        597 non-null    float64       \n",
      " 1   original_air_date  600 non-null    datetime64[ns]\n",
      " 2   season             600 non-null    int64         \n",
      " 3   title              600 non-null    string        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), string(1)\n",
      "memory usage: 23.4 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_episodes = simpsons_episodes.astype({\n",
    "    \"title\": \"string\"\n",
    "})\n",
    "simpsons_episodes[\"original_air_date\"] = pd.to_datetime(simpsons_episodes[\"original_air_date\"])\n",
    "simpsons_episodes.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In quest'utlimo dataset sono invece contenute tutte le battute dei copioni dei vari episodi. Oltre alla colonna ``id`` che fa da identificatore, vista più volte, troviamo:\n",
    "- ``episode_id``: l'identificatore dell'episodio, utile per effettuare join con il DataFrame che contiene gli episodi\n",
    "- ``number``: il numero della battuta all'interno del copione dell'episodio\n",
    "- ``raw_text``: il testo della battuta così com'è stato trovato nel copione\n",
    "- ``timestamp_in_ms``: il timestamp dell'istante in cui la battuta viene pronunciata all'interno dell'episodio\n",
    "- ``speaking_line``: se la battuta viene pronunciata da qualcuno o semplicemente è un appunto dello sceneggiatore\n",
    "- ``character_id``: l'identificatore del personaggio che pronuncia la battuta\n",
    "- ``location_id``: l'identificatore della location in cui viene pronunciata la battuta\n",
    "- ``raw_character_text``: il nome del personaggio che pronuncia la battuta\n",
    "- ``raw_location_text``: il nome del luogo in cui la battuta viene pronunciata\n",
    "- ``spoken_words``: il testo della battuta con rimossi i suggerimenti di interpretazione e il nome di chi la pronuncia\n",
    "- ``normalized_text``: come ``spoken_words`` ma sono stati rimossi tutti i caratteri non alfabetici e quelli rimasti posti in minuscolo\n",
    "- ``word_count``: il conteggio delle parole nella battuta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>(Springfield Elementary School: Ext. springfield elementary school - establishing - night)</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>(Auditorium: int. auditorium - night)</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Marge Simpson: (HUSHED VOICE) Sorry, Excuse us. Pardon me...</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>Sorry, Excuse us. Pardon me...</td>\n",
       "      <td>sorry excuse us pardon me</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Homer Simpson: (SIMULTANEOUSLY) Hey, Norman. How's it going? So you got dragged down here, too.....</td>\n",
       "      <td>26000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? ...</td>\n",
       "      <td>hey norman hows it going so you got dragged down here too heh heh how ya doing fred excuse me fred</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Homer Simpson: Pardon my galoshes. (CHUCKLES)</td>\n",
       "      <td>34000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>Pardon my galoshes.</td>\n",
       "      <td>pardon my galoshes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number  \\\n",
       "id                       \n",
       "6            1       5   \n",
       "7            1       6   \n",
       "8            1       7   \n",
       "9            1       8   \n",
       "10           1       9   \n",
       "\n",
       "                                                                                               raw_text  \\\n",
       "id                                                                                                        \n",
       "6            (Springfield Elementary School: Ext. springfield elementary school - establishing - night)   \n",
       "7                                                                 (Auditorium: int. auditorium - night)   \n",
       "8                                          Marge Simpson: (HUSHED VOICE) Sorry, Excuse us. Pardon me...   \n",
       "9   Homer Simpson: (SIMULTANEOUSLY) Hey, Norman. How's it going? So you got dragged down here, too.....   \n",
       "10                                                        Homer Simpson: Pardon my galoshes. (CHUCKLES)   \n",
       "\n",
       "    timestamp_in_ms  speaking_line  character_id  location_id  \\\n",
       "id                                                              \n",
       "6           24000.0          False           NaN          3.0   \n",
       "7           24000.0          False           NaN          4.0   \n",
       "8           24000.0           True           1.0          4.0   \n",
       "9           26000.0           True           2.0          4.0   \n",
       "10          34000.0           True           2.0          4.0   \n",
       "\n",
       "   raw_character_text              raw_location_text  \\\n",
       "id                                                     \n",
       "6                 NaN  Springfield Elementary School   \n",
       "7                 NaN                     Auditorium   \n",
       "8       Marge Simpson                     Auditorium   \n",
       "9       Homer Simpson                     Auditorium   \n",
       "10      Homer Simpson                     Auditorium   \n",
       "\n",
       "                                                                                           spoken_words  \\\n",
       "id                                                                                                        \n",
       "6                                                                                                   NaN   \n",
       "7                                                                                                   NaN   \n",
       "8                                                                        Sorry, Excuse us. Pardon me...   \n",
       "9   Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? ...   \n",
       "10                                                                                  Pardon my galoshes.   \n",
       "\n",
       "                                                                                       normalized_text  \\\n",
       "id                                                                                                       \n",
       "6                                                                                                  NaN   \n",
       "7                                                                                                  NaN   \n",
       "8                                                                            sorry excuse us pardon me   \n",
       "9   hey norman hows it going so you got dragged down here too heh heh how ya doing fred excuse me fred   \n",
       "10                                                                                  pardon my galoshes   \n",
       "\n",
       "    word_count  \n",
       "id              \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          5.0  \n",
       "9         21.0  \n",
       "10         3.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines = pd.read_csv(\"simpsons_script_lines.csv\", index_col=\"id\").sort_values(\"id\")\n",
    "simpsons_script_lines.iloc[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La colonna ``raw_text``, benchè possa sembrare utile, in realtà può essere eliminata, perchè sono le colonne successive a mantenere i suoi dati utili in una forma più leggibile. L'unico caso in cui si perdono delle informazioni si ha quando la battuta non è pronunciata e perciò indica un cambio di location o uno specifico momento del giorno. Il luogo indicato però è presente alla feature ``location_id`` e ``raw_location_text``, mentre il momento del giorno non è un'informazione particolarmente significativa. Inoltre, ``spoken_words`` è migliore di ``raw_text`` perchè non presenta i suggerimenti di interpretazione, che non fanno capire che cosa sta succedendo durante quella battuta, e perchè chi la pronuncia è già inserito nelle colonne ``character_id`` e ``raw_character_text``. Tratterremo però ``raw_text`` al momento perchè ci servirà per l'elaborazione dei dati succssiva. La feature ``speaking_line`` non è necessaria da trattenere poichè tutte le righe che hanno la colonna ``spoken_words`` diversa da NA hanno anche ``speaking_line`` posto a True, rendendolo un attributo derivato al pari di ``normalized_text``, ``word_count``, ``raw_character_text`` e ``raw_location_text``. Possiamo perciò eliminare queste colonne tranquillamente. La feature ``timestamp_in_ms`` non ci dà alcuna informazione utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines[(simpsons_script_lines[\"speaking_line\"] == True) & (simpsons_script_lines[\"spoken_words\"].isna())].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>We're late.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number                                       raw_text  \\\n",
       "id                                                                      \n",
       "1            1       0   (Street: ext. street - establishing - night)   \n",
       "2            1       1                        (Car: int. car - night)   \n",
       "3            1       2            Marge Simpson: Ooo, careful, Homer.   \n",
       "4            1       3  Homer Simpson: There's no time to be careful.   \n",
       "5            1       4                     Homer Simpson: We're late.   \n",
       "\n",
       "    character_id  location_id                    spoken_words  \n",
       "id                                                             \n",
       "1            NaN          1.0                             NaN  \n",
       "2            NaN          2.0                             NaN  \n",
       "3            1.0          2.0            Ooo, careful, Homer.  \n",
       "4            2.0          2.0  There's no time to be careful.  \n",
       "5            2.0          2.0                     We're late.  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines.drop(columns=simpsons_script_lines.columns.difference([\"episode_id\", \"number\", \"raw_text\", \"character_id\", \"location_id\", \"spoken_words\"]), inplace=True)\n",
    "simpsons_script_lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per quest'ultimo DataFrame l'uso della memoria è importantissimo, dato che occupa oltre 20 MiB. Anche in questo caso ci affidiamo all'assegnamento dei tipi di dato di Pandas per migliorare l'uso della memoria, che migliora in questo caso solo di un fattore 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158301 entries, 1 to 158315\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   episode_id    158301 non-null  int64  \n",
      " 1   number        158301 non-null  int64  \n",
      " 2   raw_text      158301 non-null  object \n",
      " 3   character_id  140760 non-null  float64\n",
      " 4   location_id   157878 non-null  float64\n",
      " 5   spoken_words  132139 non-null  object \n",
      "dtypes: float64(2), int64(2), object(2)\n",
      "memory usage: 39.7 MB\n"
     ]
    }
   ],
   "source": [
    "simpsons_script_lines.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>We're late.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number                                       raw_text  \\\n",
       "id                                                                      \n",
       "1            1       0   (Street: ext. street - establishing - night)   \n",
       "2            1       1                        (Car: int. car - night)   \n",
       "3            1       2            Marge Simpson: Ooo, careful, Homer.   \n",
       "4            1       3  Homer Simpson: There's no time to be careful.   \n",
       "5            1       4                     Homer Simpson: We're late.   \n",
       "\n",
       "    character_id  location_id                    spoken_words  \n",
       "id                                                             \n",
       "1           <NA>            1                            <NA>  \n",
       "2           <NA>            2                            <NA>  \n",
       "3              1            2            Ooo, careful, Homer.  \n",
       "4              2            2  There's no time to be careful.  \n",
       "5              2            2                     We're late.  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines = simpsons_script_lines.astype({\n",
    "    \"character_id\": pd.Int64Dtype(),\n",
    "    \"location_id\": pd.Int64Dtype(),\n",
    "    \"spoken_words\": \"string\"\n",
    "})\n",
    "simpsons_script_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158301 entries, 1 to 158315\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   episode_id    158301 non-null  int64 \n",
      " 1   number        158301 non-null  int64 \n",
      " 2   raw_text      158301 non-null  object\n",
      " 3   character_id  140760 non-null  Int64 \n",
      " 4   location_id   157878 non-null  Int64 \n",
      " 5   spoken_words  132139 non-null  string\n",
      "dtypes: Int64(2), int64(2), object(1), string(1)\n",
      "memory usage: 26.5 MB\n"
     ]
    }
   ],
   "source": [
    "simpsons_script_lines.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli episodi contenuti nel dataset sono 600, ma solo i primi 597 hanno un rating su IMDB. Questo è segno del fatto che quando questi dati sono stati raccolti quegli episodi erano ancora molto recenti e non c'erano sufficienti valutazioni per estrarre un rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>597.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.386097</td>\n",
       "      <td>14.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.732439</td>\n",
       "      <td>7.755444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.900000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       imdb_rating      season\n",
       "count   597.000000  600.000000\n",
       "mean      7.386097   14.100000\n",
       "std       0.732439    7.755444\n",
       "min       4.500000    1.000000\n",
       "25%       6.900000    7.000000\n",
       "50%       7.300000   14.000000\n",
       "75%       8.000000   21.000000\n",
       "max       9.200000   28.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes[simpsons_episodes[\"imdb_rating\"].isna()].equals(simpsons_episodes.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sui 597 episodi, in media le recensioni li hanno considerati nella media, cioè tra il \"bello\" e il \"brutto\", con un valore però che tende maggiormente verso il \"brutto\", seguendo le classi che abbiamo definito all'inizio. Questo vuol dire che il punto di separazione tra i valori inclusi tra le due classi è corretto, nell'ottica di avere due classi bilanciate. La mediana non si discosta molto dal valore medio, segno che le recensioni sono state molto equilibrate, senza troppi valori estremi, come ci conferma anche il boxplot. Esistono però due episodi, considerati i peggiori della serie, che sono *outliers* rispetto alla distribuzione di tutti i voti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(simpsons_episodes[\"imdb_rating\"].dropna(), showmeans=True, labels=[\"ratings\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>imdb_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Lisa Goes Gaga</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>All Singing, All Dancing</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        title  imdb_rating\n",
       "id                                        \n",
       "508            Lisa Goes Gaga          4.5\n",
       "189  All Singing, All Dancing          5.1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes.sort_values(\"imdb_rating\").head(2)[[\"title\", \"imdb_rating\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osservando l'istogramma, si nota come la distribuzione dei valori si avvicini sufficientemente a quella di una gaussiana, fatto testimoniato anche dalla deviazione standard che ha un valore prossimo a 1 (0,73). Possiamo considerare le recensioni come ben distribuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes[\"imdb_rating\"].plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes[\"imdb_rating\"].plot.density();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset considera gli episodi de \"I Simpson\" delle prime 28 stagioni, e nell'ultima non tiene in considerazione tutti gli episodi, ma solamente i primi quattro, fatto facilmente verificabile consultando la [pagina di Wikipedia](https://it.wikipedia.org/wiki/Episodi_de_I_Simpson_(ventottesima_stagione)) che riguarda quella stagione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes[\"season\"].value_counts().sort_index().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni nuovo episodio è andato in onda quasi esclusivamente di domenica, fatta eccezione per un centinaio di episodi, andati in onda di giovedì, e poi in maniera insignificante alcuni altri nuovi episodi sono andati in onda nei restanti giorni della settimana. Questo ci dice che questa possibile feature ha scarsa variabilità e non è utile ternerne conto durante la classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes.original_air_date.dt.day_name().value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I nuovi episodi sono andati in onda durante tutto l'anno con all'incirca sempre la stessa frequenza, ad eccezione del periodo esitvo. Questi dati non ci sorprendono però, dato che la stagione televisiva si svolge da settembre a maggio. Quindi anche questo dato non è particolarmente significativo, non indica alcuna particolarità espressa dalla serie tv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes.original_air_date.dt.month.value_counts().sort_index().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un'altra particolarità che notiamo è un fatto di cui tutti gli appassionati della serie sono convinti: le prime stagioni sono considerate mediamente più belle di quelle successive. Dal grafico a barre si nota infatti che per le stagioni dalla 2 alla 8 il punteggio medio è pari o superiore a 8 (episodi \"belli\" o \"molto belli\"), mentre per le stagioni successive la media si attesta attorno a 7 (\"nella media\") o meno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes.groupby(\"season\").mean().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediamente, nel copione di ogni episodio vengono inserite 281 battute di cui 234 vengono pronunciate da un attore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_text        280.675532\n",
       "spoken_words    234.289007\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines.groupby(\"episode_id\")[[\"raw_text\", \"spoken_words\"]].count().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può osservare, il numero di battute tra tutti i copioni che vengono pronunciate è inferiore (26.000 istanze) a quello delle battute in generale nei copioni. Questo per noi non rappresenta un problema, ciò che invece potrebbe rappresentare un problema sono quelle battute dove non è dato il personaggio che le pronuncia, dato che non può essere sconosciuto. Discorso diverso vale per i luoghi, infatti è possibile che la battuta sia pronunciata in un luogo che si ignora, perchè magari non viene mostrato. Si nota infatti che le battute pronunciate senza personaggio sono pochissime, segno di una mancanza di dati dovuta a degli errori, mentre invece le battute senza un luogo specificato sono molte di più, anche se sempre molto poche in confronto a quelle dell'intero dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_text        158301\n",
       "spoken_words    132139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines[[\"raw_text\", \"spoken_words\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4213</th>\n",
       "      <td>14</td>\n",
       "      <td>205</td>\n",
       "      <td>Martin Prince: Who would have thought that pushing a boy into the girls' lavatory could be such ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Who would have thought that pushing a boy into the girls' lavatory could be such a thrill? The s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>14</td>\n",
       "      <td>254</td>\n",
       "      <td>Entire Town: A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND.\" /</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND. /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>14</td>\n",
       "      <td>258</td>\n",
       "      <td>Bart Simpson: (READING ALOUD, WITH HEART) Chapter Six: Four Days in Philadelphia. The first Cont...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Chapter Six: Four Days in Philadelphia. The first Continental Congress faced a difficult job. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>18</td>\n",
       "      <td>200</td>\n",
       "      <td>Tony Bennett: CALLED CAPITAL CITY...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CALLED CAPITAL CITY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5349</th>\n",
       "      <td>18</td>\n",
       "      <td>203</td>\n",
       "      <td>Tony Bennett: IN CAPITAL CITY...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>IN CAPITAL CITY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_id  number  \\\n",
       "id                         \n",
       "4213          14     205   \n",
       "4262          14     254   \n",
       "4266          14     258   \n",
       "5346          18     200   \n",
       "5349          18     203   \n",
       "\n",
       "                                                                                                 raw_text  \\\n",
       "id                                                                                                          \n",
       "4213  Martin Prince: Who would have thought that pushing a boy into the girls' lavatory could be such ...   \n",
       "4262             Entire Town: A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND.\" /   \n",
       "4266  Bart Simpson: (READING ALOUD, WITH HEART) Chapter Six: Four Days in Philadelphia. The first Cont...   \n",
       "5346                                                                 Tony Bennett: CALLED CAPITAL CITY...   \n",
       "5349                                                                     Tony Bennett: IN CAPITAL CITY...   \n",
       "\n",
       "      character_id  location_id  \\\n",
       "id                                \n",
       "4213          <NA>         <NA>   \n",
       "4262          <NA>         <NA>   \n",
       "4266          <NA>         <NA>   \n",
       "5346          <NA>         <NA>   \n",
       "5349          <NA>         <NA>   \n",
       "\n",
       "                                                                                             spoken_words  \n",
       "id                                                                                                         \n",
       "4213  Who would have thought that pushing a boy into the girls' lavatory could be such a thrill? The s...  \n",
       "4262                           A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND. /  \n",
       "4266  Chapter Six: Four Days in Philadelphia. The first Continental Congress faced a difficult job. Co...  \n",
       "5346                                                                               CALLED CAPITAL CITY...  \n",
       "5349                                                                                   IN CAPITAL CITY...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines[~simpsons_script_lines[\"spoken_words\"].isna() & simpsons_script_lines[\"character_id\"].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4213</th>\n",
       "      <td>14</td>\n",
       "      <td>205</td>\n",
       "      <td>Martin Prince: Who would have thought that pushing a boy into the girls' lavatory could be such ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Who would have thought that pushing a boy into the girls' lavatory could be such a thrill? The s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>14</td>\n",
       "      <td>254</td>\n",
       "      <td>Entire Town: A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND.\" /</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND. /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>14</td>\n",
       "      <td>258</td>\n",
       "      <td>Bart Simpson: (READING ALOUD, WITH HEART) Chapter Six: Four Days in Philadelphia. The first Cont...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Chapter Six: Four Days in Philadelphia. The first Continental Congress faced a difficult job. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>Marge Simpson: (CLEARING HER THROAT) Hello, everyone. (CLEARING HER THROAT) You know, Halloween ...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Hello, everyone. You know, Halloween is a very strange holiday. Personally, I don't understand i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>18</td>\n",
       "      <td>200</td>\n",
       "      <td>Tony Bennett: CALLED CAPITAL CITY...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CALLED CAPITAL CITY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_id  number  \\\n",
       "id                         \n",
       "4213          14     205   \n",
       "4262          14     254   \n",
       "4266          14     258   \n",
       "4588          16       0   \n",
       "5346          18     200   \n",
       "\n",
       "                                                                                                 raw_text  \\\n",
       "id                                                                                                          \n",
       "4213  Martin Prince: Who would have thought that pushing a boy into the girls' lavatory could be such ...   \n",
       "4262             Entire Town: A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND.\" /   \n",
       "4266  Bart Simpson: (READING ALOUD, WITH HEART) Chapter Six: Four Days in Philadelphia. The first Cont...   \n",
       "4588  Marge Simpson: (CLEARING HER THROAT) Hello, everyone. (CLEARING HER THROAT) You know, Halloween ...   \n",
       "5346                                                                 Tony Bennett: CALLED CAPITAL CITY...   \n",
       "\n",
       "      character_id  location_id  \\\n",
       "id                                \n",
       "4213          <NA>         <NA>   \n",
       "4262          <NA>         <NA>   \n",
       "4266          <NA>         <NA>   \n",
       "4588             1         <NA>   \n",
       "5346          <NA>         <NA>   \n",
       "\n",
       "                                                                                             spoken_words  \n",
       "id                                                                                                         \n",
       "4213  Who would have thought that pushing a boy into the girls' lavatory could be such a thrill? The s...  \n",
       "4262                           A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND. /  \n",
       "4266  Chapter Six: Four Days in Philadelphia. The first Continental Congress faced a difficult job. Co...  \n",
       "4588  Hello, everyone. You know, Halloween is a very strange holiday. Personally, I don't understand i...  \n",
       "5346                                                                               CALLED CAPITAL CITY...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines[~simpsons_script_lines[\"spoken_words\"].isna() & simpsons_script_lines[\"location_id\"].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È possibile osservare che in media all'interno di ciascun episodio della serie della serie i personaggi che compaiono hanno sei battute a testa, anche se questo dato è fortemente sbilanciato: ad esempio la metà dei personaggi a livello di singolo episodio ha una o due battute al massimo e ci sono personaggi che in un singolo episodio pronunciano fino a 131 battute. Questo significa che pochi personaggi sono veramente rilevanti nella serie tv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19995.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.607702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.653093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>131.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               size\n",
       "count  19995.000000\n",
       "mean       6.607702\n",
       "std       12.653093\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        2.000000\n",
       "75%        5.000000\n",
       "max      131.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.DataFrame(simpsons_script_lines.dropna(axis=\"index\", subset=[\"spoken_words\"]) \\\n",
    "                                  .groupby([\"episode_id\", \"character_id\"]) \\\n",
    "                                  .size(), columns=[\"size\"])\n",
    "lines.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lines.plot.hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discorso analogo si può fare per le location, in media ciascuna location a livello di singolo episodio compare 13 volte nel copione, però un quarto delle location non compare mai più di tre volte ed esistono location che compaiono fino a 203 volte. Quindi, poche location nella serie sono veramente importanti per caratterizzare un episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10041.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.120805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.007592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>203.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               size\n",
       "count  10041.000000\n",
       "mean      13.120805\n",
       "std       18.007592\n",
       "min        1.000000\n",
       "25%        3.000000\n",
       "50%        7.000000\n",
       "75%       15.000000\n",
       "max      203.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = pd.DataFrame(simpsons_script_lines.dropna(axis=\"index\", subset=[\"spoken_words\"]) \\\n",
    "                                              .groupby([\"episode_id\", \"location_id\"]) \\\n",
    "                                              .size(), columns=[\"size\"])\n",
    "locations.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "locations.plot.hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rimozione dei valori mancati e preparazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sulla base di quanto detto, possiamo rimuovere ``original_air_date`` e ``season`` dal DataFrame sugli episodi perchè le feature che potremmo estrarre hanno una scarsa variabilità o comunque presentano delle tendenze che vorremmo non influenzassero la nostra classificazione. Inoltre, possiamo rimuovere ``title`` perchè non ci serve più."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes.drop(columns=[\"original_air_date\", \"title\", \"season\"], inplace=True)\n",
    "simpsons_episodes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto, cerchiamo di sostituire i valori NA di ``character_id`` con quelli corretti quando anche ``spoken_words`` è diverso da NA utilizzando le peculiarità di ``raw_text``. Infatti, generalmente le battute pronunciate nella colonna ``raw_text`` sono precedute dal nome di chi le pronuncia, seguito dai due punti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = simpsons_script_lines[(~simpsons_script_lines[\"spoken_words\"].isna()) & (simpsons_script_lines[\"character_id\"].isna())]\n",
    "for index, row in missing.iterrows():\n",
    "    name = row[\"raw_text\"].split(\":\")[0]\n",
    "    if name != \"\":\n",
    "        simpsons_script_lines.loc[index, \"character_id\"] = simpsons_characters[simpsons_characters[\"name\"] == name].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatto questo, ``raw_text`` non ci serve più e può essere eliminato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>We're late.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Sorry, Excuse us. Pardon me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number  character_id  location_id  \\\n",
       "id                                                  \n",
       "3            1       2             1            2   \n",
       "4            1       3             2            2   \n",
       "5            1       4             2            2   \n",
       "8            1       7             1            4   \n",
       "9            1       8             2            4   \n",
       "\n",
       "                                                                                           spoken_words  \n",
       "id                                                                                                       \n",
       "3                                                                                  Ooo, careful, Homer.  \n",
       "4                                                                        There's no time to be careful.  \n",
       "5                                                                                           We're late.  \n",
       "8                                                                        Sorry, Excuse us. Pardon me...  \n",
       "9   Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines.drop(columns=\"raw_text\", inplace=True)\n",
    "simpsons_script_lines.dropna(axis=\"index\", subset=[\"spoken_words\"], inplace=True)\n",
    "simpsons_script_lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selezione delle feature rilevanti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto, abbiamo ripulito i nostri DataFrame in modo che potessero essere rielaborati per poter estrarre la variabile da predire e le feature di nostro interesse. Possiamo procedere dunque con l'unione delle singole battute dei copioni per ottenere nuovamente i copioni originali di ciascun episodio e successivamente effettuare un join con il primo DataFrame per associare a ciascun episodio il suo rating su IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.2</td>\n",
       "      <td>Ooo, careful, Homer. There's no time to be careful. We're late. Sorry, Excuse us. Pardon me... H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>Come on, Mom. Yeah, Mom, hurry up. All right... hmmm... How about \"he\"? Two points. Your turn, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.5</td>\n",
       "      <td>Now, class, I don't want this field trip to be a repeat of our infamous visit to the Springfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.8</td>\n",
       "      <td>Oh, yeah? Yeah! Oh, yeah? Yeah! Oh, yeah? Yeah! Oh, yeah? Yeah! Hey! What's the problem here? We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>Do I smell cupcakes? Oooo, Do I ever! Uh-uh, Homer. Lisa's making these for her teacher. Ah. Say...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    imdb_rating  \\\n",
       "id                \n",
       "1           8.2   \n",
       "2           7.8   \n",
       "3           7.5   \n",
       "4           7.8   \n",
       "5           8.1   \n",
       "\n",
       "                                                                                           spoken_words  \n",
       "id                                                                                                       \n",
       "1   Ooo, careful, Homer. There's no time to be careful. We're late. Sorry, Excuse us. Pardon me... H...  \n",
       "2   Come on, Mom. Yeah, Mom, hurry up. All right... hmmm... How about \"he\"? Two points. Your turn, d...  \n",
       "3   Now, class, I don't want this field trip to be a repeat of our infamous visit to the Springfield...  \n",
       "4   Oh, yeah? Yeah! Oh, yeah? Yeah! Oh, yeah? Yeah! Oh, yeah? Yeah! Hey! What's the problem here? We...  \n",
       "5   Do I smell cupcakes? Oooo, Do I ever! Uh-uh, Homer. Lisa's making these for her teacher. Ah. Say...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts = pd.DataFrame()\n",
    "simpsons_script_lines.sort_values([\"episode_id\", \"number\"], inplace=True)\n",
    "scripts[\"spoken_words\"] = simpsons_script_lines.groupby(\"episode_id\")[\"spoken_words\"].apply(' '.join)\n",
    "episodes = simpsons_episodes.join(scripts)\n",
    "episodes = episodes.astype({\n",
    "    \"spoken_words\": \"string\"\n",
    "})\n",
    "episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può notare, ci sono 597 episodi con rating su IMDB, mentre quelli di cui era presente il copione erano solo 564. Non è possibile effetuare la classificazione se il rating IMDB è assente e non è molto utile tentare di farlo se il copione non è presente e non abbiamo modo di sapere cosa conteneva, perciò, eliminiamo tutte le righe che presentano valori NA, consapevoli che ci rimangono molte istanze da sfruttare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 600 entries, 1 to 600\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   imdb_rating   597 non-null    float64\n",
      " 1   spoken_words  564 non-null    string \n",
      "dtypes: float64(1), string(1)\n",
      "memory usage: 14.1 KB\n"
     ]
    }
   ],
   "source": [
    "episodes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerchiamo quindi di costruire delle altre feature rilevanti che possiamo estrarre da ``simpsons_script_lines``. Possiamo ad esempio associare a ciascun episodio il numero di volte che ciascun personaggio pronuncia una battuta. Poichè i personaggi sono molti, salviamo questi dati in una matrice sparsa e, poichè sappiamo che a livello di singolo episodio un personaggio non pronuncia mai più di 131 battute, possiamo scegliere un tipo di dato più adeguato per salvare questi valori per risparmiare spazio in memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = simpsons_script_lines.drop(columns=[\"number\", \"location_id\", \"spoken_words\"]) \\\n",
    "                                  .dropna() \\\n",
    "                                  .join(simpsons_characters, on=\"character_id\") \\\n",
    "                                  .drop(columns=\"character_id\") \\\n",
    "                                  .groupby([\"episode_id\", \"name\"]) \\\n",
    "                                  .size() \\\n",
    "                                  .unstack() \\\n",
    "                                  .fillna(0) \\\n",
    "                                  .astype(np.int8) \\\n",
    "                                  .reindex(episodes.index)\n",
    "characters_cols = characters.columns\n",
    "characters = csr_matrix(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo ripetere lo stesso procedimento per le location, associando a ciascun episodio il numero di volte che una battuta è pronunciata in una determinata location, per ciascuna location. Poichè le location sono molte, salviamo anche questi dati in una matrice sparsa. Inoltre, sappiamo che a livello di singolo episodio in una data location non sono mai pronunciate più di 203 battute, perciò possiamo scegliere un tipo di dato più adeguato per salvare questi valori e risparmiare così memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = simpsons_script_lines.drop(columns=[\"number\", \"character_id\", \"spoken_words\"]) \\\n",
    "                                 .dropna() \\\n",
    "                                 .join(simpsons_locations, on=\"location_id\") \\\n",
    "                                 .drop(columns=\"location_id\") \\\n",
    "                                 .groupby([\"episode_id\", \"name\"]) \\\n",
    "                                 .size() \\\n",
    "                                 .unstack() \\\n",
    "                                 .fillna(0) \\\n",
    "                                 .astype(np.int8) \\\n",
    "                                 .reindex(episodes.index)\n",
    "locations_cols = locations.columns\n",
    "locations = csr_matrix(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatto questo, possiamo anche sbarazzarci di ``simpsons_episodes`` e ``simpsons_script_lines``, perchè abbiamo estratto tutto ciò che potevamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "del simpsons_episodes\n",
    "del simpsons_script_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiungiamo quindi la variabile categorica da predire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lisa! Lisa, are you still in there? What's the problem? Did you fall in? Lisa! Sorry, Dad. Women...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rusty old hunk of junk. Howdy, Bart. Hot enough for ya? Shut up, Flanders. Hey dad, how come we ...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You know, Bart, when I was your age, I pulled a few boners, but I think you'll find that people ...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Whoops! Whoops! Oh, whoops... whoops! This is gonna be the best birthday breakfast Mom ever had....</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>So how was the office birthday party? Oh, it was delightful. The frosting on the cake was this t...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           spoken_words  \\\n",
       "id                                                                                                        \n",
       "6   Lisa! Lisa, are you still in there? What's the problem? Did you fall in? Lisa! Sorry, Dad. Women...   \n",
       "7   Rusty old hunk of junk. Howdy, Bart. Hot enough for ya? Shut up, Flanders. Hey dad, how come we ...   \n",
       "8   You know, Bart, when I was your age, I pulled a few boners, but I think you'll find that people ...   \n",
       "9   Whoops! Whoops! Oh, whoops... whoops! This is gonna be the best birthday breakfast Mom ever had....   \n",
       "10  So how was the office birthday party? Oh, it was delightful. The frosting on the cake was this t...   \n",
       "\n",
       "   label  \n",
       "id        \n",
       "6   good  \n",
       "7   good  \n",
       "8   good  \n",
       "9   good  \n",
       "10   bad  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[\"label\"] = np.where(episodes[\"imdb_rating\"] >= 7.5, \"good\", \"bad\")\n",
    "episodes.drop(columns=\"imdb_rating\", inplace=True)\n",
    "episodes = episodes.astype({\"label\": \"string\"})\n",
    "episodes.iloc[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il diagramma a torta ci mostra come le deduzioni fatte sulle distribuzioni dei rating fatte in precedenza si applichino correttamente. Infatti, le due classi sono legermente sbilanciate, si trovano infatti in un rapporto 60-40, verso il \"brutto\". Non ci preoccuperemo perciò di bilanciare le istanze tra le due classi, essendo molto vicine al perfetto bilanciamento, però terremo conto di questa proporzione quando effettueremo il calcolo dell'accuratezza nei modelli classificazione. Infatti, sarà molto più facile produrre un modello corretto se tende a classificare tutti gli episodi come \"brutto\" rispetto ad uno che li classifica tutti come \"bello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bad     325\n",
       "good    239\n",
       "Name: label, dtype: Int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, _ = plt.subplots()\n",
    "plt.pie(episodes[\"label\"].value_counts(), labels=[\"bad\", \"good\"], autopct=\"%1.0f%%\")\n",
    "fig.set_facecolor(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione delle feature testuali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto cerchiamo di estrarre le feature testuali, ovverosia le parole o parti di esse, più rilevanti dai copioni precedentemente ricostruiti. Prima di tutto dobbiamo ottenere i nostri train e validation set sia a partire dai copioni originali sia dalle classi dei rating, poi dobbiamo costruire i *fold* per la *cross-validation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = episodes[\"spoken_words\"]\n",
    "y = episodes[\"label\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=742)\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=742)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scelta del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa cerchiamo di trovare il miglior *tokenizer* da applicare sui copioni. Per questo motivo al momento non ci preoccupiamo dell'indice di importanza delle parole, scegliamo di default il *tf-idf index* che risulta più veloce su questo dataset. Detto questo, analizziamo il \"caso base\" del *vectorizer* e cerchiamo di ridurre via via il numero di feature senza mai compromettere in maniera significativa l'accuratezza del modello preso in esame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['00', '000', '007', '07s', '10'],\n",
       " ['_______', 'a647253', 'aa', 'aaaa', 'aaaaa'],\n",
       " ['aaannnd', 'aaannnnd', 'aaanyway', 'aag', 'aagh'],\n",
       " ['abbie', 'abbotan', 'abbots', 'abbreviate', 'abbreviations'],\n",
       " ['éclairios', 'êtes', 'être', 'ĉu', 'ĝi'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_base = TfidfVectorizer()\n",
    "vectorizer_base.fit_transform(X_train)\n",
    "features = vectorizer_base.get_feature_names()\n",
    "features[0:5], features[327:332], features[350:355], features[370:375], features[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Senza applicare nulla, otteniamo 32.933 features, ovvero 32.933 parole, molte delle quali osserviamo essere numeri oppure storpiature di parole inserite solo per rendere meglio l'interpretazione delle frasi. Ha perciò senso provare un metodo di *tokenization* differente per osservare se le feature che estrae sono più significative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32993"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiamo allora il *tokenizer* del modulo NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['!', '#', '$', '%', '&'],\n",
       " [\"'allo\", \"'angry\", \"'appen\", \"'applause\", \"'at\"],\n",
       " ['1977', '1977.', '1979', '1979.', '1980'],\n",
       " ['19:19', '19:19.', '19th', '1:00', '1:30'],\n",
       " ['é', 'éclairios', 'êtes', 'ĉu', 'ĝi'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tokenizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "vectorizer_tokenizer.fit_transform(X_train)\n",
    "features = vectorizer_tokenizer.get_feature_names()\n",
    "features[0:5], features[30:35], features[350:355], features[370:375], features[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'uso del *tokenizer* di NLTK ha generato all'incirca 5.000 feature in più, anche se notiamo che in questo caso le feature sembrano essere generate con più criterio. Compaiono dei simboli di punteggiatura che prima non avevamo incontrato, che ha senso che vengano considerati in maniera a sè stante, dato che non fanno parte di alcuna parola, e non troviamo più numeri dall'aspetto casuale. Notiamo che sono state aggiunte molte parole apostrofate che il *tokenizer* standard non aveva saputo riconoscere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37661"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a questo punto a rimuovere gli accenti e a rimuovere le *stop words*, per cercare di trattenere tutte e sole quelle parole che identificano con chiarezza di cosa si sta parlando, eliminando perciò le parole di contorno. Proviamo inoltre a rimuovere la punteggiatura, ovvero sia tutti quei token che non contengono neanche una lettera o un numero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_nltk(text):\n",
    "    return [token for token in nltk.word_tokenize(text) if re.match(r\"[a-zA-Z0-9]+\", token) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stoplist = nltk.corpus.stopwords.words(\"english\")\n",
    "stoplist = list(set([token for word in nltk_stoplist for token in tokenize_with_nltk(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['0', '0.', '007', '1', '1,000'],\n",
       " ['10th', '11', '111238,390,17', '1132.', '117th'],\n",
       " ['60', '600', '6000', '605', '6051'],\n",
       " ['69808,243,293', '6:45', '6th', '7', '7-10'],\n",
       " ['zz', 'zz-99', 'zzyzwiski', 'zzzapp', 'zzzzapp'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_stopwords = TfidfVectorizer(tokenizer=tokenize_with_nltk, stop_words=stoplist, strip_accents=\"unicode\")\n",
    "vectorizer_stopwords.fit_transform(X_train)\n",
    "features = vectorizer_stopwords.get_feature_names()\n",
    "features[0:5], features[30:35], features[350:355], features[370:375], features[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La rimozione dei token descritti in precedenza non ha giovato particolarmente. Sono state rimosse solamente 400 feature, all'incirca. Evidentemente il metodo usato finora non è stato capace di cogliere tutti quegli elementi che aveva senso rimuovere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37257"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applicazione di meccanismi avanzati: POS tagging, lemmatization, stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a questo punto a complicare la *tokenization* applicandole i meccanismi di *POS tagging*, di lemmatizzazione e di *stemming*, cercando di ridurre ancora il numero delle feature togliendo tutte quelle parole che non sono altro che la variante della stessa dove però cambia la declinazione grammaticale, che non va ad incidere particolarmente sul contenuto generale del discorso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_pos(text):\n",
    "    return nltk.pos_tag(tokenize_with_nltk(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_stoplist = list(set([token for word in nltk_stoplist for token in tokenize_with_pos(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "penn_to_wn = {\"N\": \"n\", \"V\": \"v\", \"J\": \"a\", \"R\": \"r\"}\n",
    "def tokenize_with_lemmatization(text):\n",
    "    return [(wnl.lemmatize(token, penn_to_wn[tag[0]]) if tag[0] in penn_to_wn else token) for token, tag in tokenize_with_pos(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization_stoplist = list(set([token for word in nltk_stoplist for token in tokenize_with_lemmatization(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "def tokenize_with_stemming(text):\n",
    "    return [ps.stem(word) for word in tokenize_with_nltk(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_stoplist = list(set([token for word in nltk_stoplist for token in tokenize_with_stemming(word)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa volta, avendo a disposizione un range di valori tra cui scegliere, sceglieremo il *tokenizer* effettuando delle regressioni logistiche di prova. Introduciamo  la standardizzazione delle feature visto che il conteggio delle parole è un indice non normalizzato. Notiamo come l'uso del *POS tagging* aumenta il numero delle feature perchè introduce varianti della stessa parola con però associati diversi tag, dato che una stessa parola può ricoprire più ruoli all'interno del discorso. Il risultato è che però l'accuratezza scende, come potevamo aspettarci, anche se non in maniera significativa. La lemmatizzazione invece riesce a ridurre il numero di feature con un aumento dell'accuratezza, ma il metodo migliore si rivela lo stemming. Esso infatti riduce più di tutti gli altri il numero delle feature, facendo di contro salire l'accuratezza il più possibile. Useremo perciò quest'ultimo metodo nel prosieguo dell'eliminazione delle feature non rilevanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nltk</th>\n",
       "      <td>0.746145</td>\n",
       "      <td>0.020577</td>\n",
       "      <td>30047.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>0.690338</td>\n",
       "      <td>0.013318</td>\n",
       "      <td>45446.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization</th>\n",
       "      <td>0.743562</td>\n",
       "      <td>0.032009</td>\n",
       "      <td>25304.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming</th>\n",
       "      <td>0.761335</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>22438.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean_test_score  std_test_score  number_features\n",
       "nltk                  0.746145        0.020577     30047.000000\n",
       "pos                   0.690338        0.013318     45446.333333\n",
       "lemmatization         0.743562        0.032009     25304.666667\n",
       "stemming              0.761335        0.032526     22438.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "values = [(nltk.word_tokenize, stoplist),\n",
    "          (tokenize_with_pos, pos_stoplist),\n",
    "          (tokenize_with_lemmatization, lemmatization_stoplist),\n",
    "          (tokenize_with_stemming, stemming_stoplist)]\n",
    "for i, value in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"vectorizer\", TfidfVectorizer(tokenizer=value[0], stop_words=value[1], strip_accents=\"unicode\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000))\n",
    "    ])\n",
    "    result = cross_validate(model, X_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: len(e.named_steps[\"vectorizer\"].get_feature_names()), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=[\"nltk\", \"pos\", \"lemmatization\", \"stemming\"], columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ricerca della minimum document frequency, degli ngram, dell'indice di importanza dei termini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo poi ad eliminare le parole che compaiono troppo di rado, così da rimuovere quelle feature che non sono veramente tali, ma che probabilmente sono solo uno scarto del processo di divisione delle parole o che comunque non sono significative perchè sono presenti poche volte in assoluto. Osserviamo che se scegliessimo come ``min_df`` il valore 5, lo score medio sarebbe il più alto in assoluto, ma scegliendo 10 avremmo una perdita in accuratezza solo dell'1,7% con una rimozione ulteriore di circa metà delle feature. Scegliamo perciò come valore per questo parametro 10, riuscendo così a ridurre le feature coinvolte di circa un fattore 8, in media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.761335</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>22438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.824736</td>\n",
       "      <td>0.041275</td>\n",
       "      <td>4915.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.807059</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>2959.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.814635</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>2135.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  std_test_score  number_features\n",
       "1          0.761335        0.032526     22438.000000\n",
       "5          0.824736        0.041275      4915.666667\n",
       "10         0.807059        0.025509      2959.333333\n",
       "15         0.814635        0.024138      2135.000000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "values = [1, 5, 10, 15]\n",
    "for i, value in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"vectorizer\", TfidfVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=value)),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000))\n",
    "    ])\n",
    "    result = cross_validate(model, X_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: len(e.named_steps[\"vectorizer\"].get_feature_names()), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=values, columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerchiamo poi di capire se l'utilizzo degli *ngram* permette di migliorare significativamente l'accuratezza del nostro modello oppure no. Come potevamo aspettarci, l'uso di *ngram* aumenta l'accuratezza dello score, perchè aumenta il numero di feature a disposizione. L'uso dei 2-gram e dei 3-gram migliora sensibilmente lo score del modello, perciò li terremo come feature aggiuntive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.837555</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>4411.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.832485</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>4338.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.807059</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>2959.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score  number_features\n",
       "3         0.837555        0.021943      4411.666667\n",
       "2         0.832485        0.012480      4338.333333\n",
       "1         0.807059        0.025509      2959.333333"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "values = [(1, 1), (1, 2), (1, 3)]\n",
    "for i, value in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"vectorizer\", TfidfVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=10, ngram_range=value)),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=5000))\n",
    "    ])\n",
    "    result = cross_validate(model, X_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: len(e.named_steps[\"vectorizer\"].get_feature_names()), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=[1, 2, 3], columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"]) \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, cerchiamo di capire quale indice di importanza delle parole, se il semplice conteggio o il *tf-idf index*, è il più adatto per l'utilizzo durante la regressione. L'indice *tf-idf* risulta migliore di circa il 2%, staccando decisamente CountVectorizer, perciò continueremo perciò ad utilizzare TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CountVectorizer</th>\n",
       "      <td>0.817237</td>\n",
       "      <td>0.011122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TfidfVectorizer</th>\n",
       "      <td>0.837555</td>\n",
       "      <td>0.021943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_test_score  std_test_score\n",
       "vectorizer                                      \n",
       "CountVectorizer         0.817237        0.011122\n",
       "TfidfVectorizer         0.837555        0.021943"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", None),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=5000))\n",
    "])\n",
    "grid = {\n",
    "    \"vectorizer\": [TfidfVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=10, ngram_range=(1, 3)),\n",
    "                   CountVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=10, ngram_range=(1, 3))]\n",
    "}\n",
    "gs = GridSearchCV(model, grid, cv=skf)\n",
    "gs.fit(X_train, y_train)\n",
    "results = pd.DataFrame(gs.cv_results_)\n",
    "results[\"vectorizer\"] = [\"TfidfVectorizer\", \"CountVectorizer\"]\n",
    "results[[\"vectorizer\", \"mean_test_score\", \"std_test_score\"]].set_index(\"vectorizer\").sort_values(\"mean_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unione di feature testuali e non"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridotte al minimo le feature testuali, si tratta di unirle con quelle non testuali precedentemente estratte per poter avere tutte le feature necessarie per portare avanti la classificazione. Ci occupiamo allora di ridurre il numero di feature anche per quelle non testuali, visto che dall'analisi dei dati molte sono risultate scarsissimamente popolate. Per prima cosa riduciamo il numero dei personaggi. L'uso di una regolarizzazione L1 più forte rende lo score più alto, ma solo se non è troppo forte. Questo è evidentemente segno del fatto che moltissimi personaggi sono irrilevanti ai fini dell'identificazione della qualità di un episodio. Usiamo perciò il coefficiente che ci dà lo score più alto, anche se questo significa sacrificare molti personaggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.733403</td>\n",
       "      <td>0.031875</td>\n",
       "      <td>134.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.00</th>\n",
       "      <td>0.720661</td>\n",
       "      <td>0.043197</td>\n",
       "      <td>1907.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.00</th>\n",
       "      <td>0.718136</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>2140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.00</th>\n",
       "      <td>0.718116</td>\n",
       "      <td>0.044412</td>\n",
       "      <td>2453.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.707957</td>\n",
       "      <td>0.045905</td>\n",
       "      <td>1070.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.652171</td>\n",
       "      <td>0.033029</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_test_score  std_test_score  number_features\n",
       "C                                                      \n",
       "0.10          0.733403        0.031875       134.333333\n",
       "5.00          0.720661        0.043197      1907.666667\n",
       "7.00          0.718136        0.038728      2140.000000\n",
       "10.00         0.718116        0.044412      2453.333333\n",
       "1.00          0.707957        0.045905      1070.666667\n",
       "0.05          0.652171        0.033029         9.333333"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_train, characters_val, y_train, y_val = train_test_split(characters, episodes[\"label\"], test_size=0.3, random_state=742)\n",
    "results = []\n",
    "values = [1, 5, 7, 10, 0.1, 0.05]\n",
    "for i, C in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000, penalty=\"l1\", C=C))\n",
    "    ])\n",
    "    result = cross_validate(model, characters_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: (e.named_steps[\"classifier\"].coef_[0] != 0).sum(), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=pd.Index(values, name=\"C\"), columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"]) \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo che tra i personaggi che abbiamo scartato, moltissimi apparivano solo in una puntata e che perciò non sono personaggi ricorrenti, ma semplici comparse di sfondo a cui sono date delle battute ma che non ci dicono chi effettivamente è il protagonista in un dato episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           \"For Dummies\" Author\n",
       "1    \"Just Stamp the Ticket\" Man\n",
       "2                     \"Mario\" #2\n",
       "3                  \"Shorts\" Bart\n",
       "4                 \"Shorts\" Homer\n",
       "5                  \"Shorts\" Lisa\n",
       "6                 \"Shorts\" Marge\n",
       "7                \"Yeeeessss\" Man\n",
       "8                    \"Yesss\" Man\n",
       "9                \"mad\" Writer #1\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000, penalty=\"l1\", C=0.1))\n",
    "    ])\n",
    "model.fit(characters_train, y_train)\n",
    "pd.Series(characters_cols[model.named_steps[\"classifier\"].coef_[0] == 0]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prcediamo quindi alla rimozione dei personaggi irrilevanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<564x384 sparse matrix of type '<class 'numpy.int8'>'\n",
       "\twith 4464 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = csr_matrix(pd.DataFrame(characters.toarray(), columns=characters_cols) \\\n",
    "                          .drop(columns=characters_cols[model.named_steps[\"classifier\"].coef_[0] == 0]))\n",
    "characters_cols = characters_cols.drop(characters_cols[model.named_steps[\"classifier\"].coef_[0] == 0])\n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ripetiamo quindi lo stesso procedimento con le locations per eliminare quelle non particolarmente significative. Anche in questo caso, riusciamo a ridurre significativamente le feature da usare, anche se meno rispetto a prima, usando una regolarizzazione un po' meno intensa. In effetti, le location erano comunque molto sparse su varie feature, ma non così tanto come per i personaggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.649684</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>972.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.644672</td>\n",
       "      <td>0.040517</td>\n",
       "      <td>886.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.637077</td>\n",
       "      <td>0.025611</td>\n",
       "      <td>735.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.00</th>\n",
       "      <td>0.637019</td>\n",
       "      <td>0.053961</td>\n",
       "      <td>1379.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.634571</td>\n",
       "      <td>0.021492</td>\n",
       "      <td>75.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.00</th>\n",
       "      <td>0.621752</td>\n",
       "      <td>0.051265</td>\n",
       "      <td>1571.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_score  std_test_score  number_features\n",
       "C                                                     \n",
       "1.00         0.649684        0.038428       972.666667\n",
       "0.50         0.644672        0.040517       886.666667\n",
       "0.25         0.637077        0.025611       735.000000\n",
       "3.00         0.637019        0.053961      1379.333333\n",
       "0.10         0.634571        0.021492        75.333333\n",
       "5.00         0.621752        0.051265      1571.333333"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_train, locations_val, y_train, y_val = train_test_split(locations, episodes[\"label\"], test_size=0.3, random_state=742)\n",
    "results = []\n",
    "values = [1, 3, 5, 0.5, 0.25, 0.1]\n",
    "for i, C in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=50000, penalty=\"l1\", C=C))\n",
    "    ])\n",
    "    result = cross_validate(model, locations_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: (e.named_steps[\"classifier\"].coef_[0] != 0).sum(), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=pd.Index(values, name=\"C\"), columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"]) \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo anche in queato caso che le feature rimosse sembrano essere quelle di location che compaiono solo in una puntata e che perciò non incidono sull'apprezzamento delle varie puntate in generale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \"ALL THE WRONG REASONS\" MOVIE THEATER\n",
       "1              \"BOOKS FOR DUMMIES\" SECTION\n",
       "2                     \"FAMILY MATTERS\" SET\n",
       "3                          \"GUT CHECK\" SET\n",
       "4                           \"HEADBUTT\" SET\n",
       "5                  \"INCEPTION\"-STYLE BEACH\n",
       "6                      \"IT NEVER ENDS\" SET\n",
       "7                         \"LAUGH IN\" STAGE\n",
       "8                           \"MAD\" BUILDING\n",
       "9                     \"MOON BOUNCE\" CASTLE\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000, penalty=\"l1\", C=1.0))\n",
    "])\n",
    "model.fit(locations_train, y_train)\n",
    "pd.Series(locations_cols[model.named_steps[\"classifier\"].coef_.sum(axis=0) == 0]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procediamo quindi come prima ad eliminare tutte quelle feature non usate per la classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<564x1563 sparse matrix of type '<class 'numpy.int8'>'\n",
       "\twith 5102 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = csr_matrix(pd.DataFrame(locations.toarray(), columns=locations_cols) \\\n",
    "                         .drop(columns=locations_cols[model.named_steps[\"classifier\"].coef_[0] == 0]))\n",
    "locations_cols = locations_cols.drop(locations_cols[model.named_steps[\"classifier\"].coef_[0] == 0])\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finita questa procedura di estrazione, possiamo procedere alla fusione dei nostri tre set di feature e alla costruzione di training set e validation set. Questo implica, per quanto riguarda i copioni, che dopo aver effettuato il loro split costruiamo anche la matrice documenti-termini su quelli presenti nel training set, su cui ci baseremo per costruire quella del validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train, words_val, y_train, y_val, characters_train, characters_val, locations_train, locations_val \\\n",
    "    = train_test_split(episodes[\"spoken_words\"],\n",
    "                       episodes[\"label\"],\n",
    "                       characters,\n",
    "                       locations,\n",
    "                       test_size=0.3,\n",
    "                       random_state=742)\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=10, ngram_range=(1, 3))\n",
    "dtm_train = vectorizer.fit_transform(words_train)\n",
    "dtm_val = vectorizer.transform(words_val)\n",
    "X_train = hstack([dtm_train, characters_train, locations_train])\n",
    "X_val = hstack([dtm_val, characters_val, locations_val])\n",
    "index = vectorizer.get_feature_names() + characters_cols.to_list() + locations_cols.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo perciò a disposizione circa 8.000 feature sulle quali effettuare la nostra classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 8476)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test di modelli di classificazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto, estratte tutte le feature che abbiamo intenzione di usare, possiamo cercare i modelli che classificano meglio i copioni degli episodi all'interno di ciascuna classe. Per prima cosa definiamo alcune funzioni, come quella per il calcolo dell'intervallo di confidenza, che utilizzeremo per valutare i nostri modelli. In ciascun test faremo attenzione a bilanciare i pesi delle classi. Infatti, pur non essendoci forti sbilanciamenti tra le due, cerchiamo comunque di favorire quei modelli che riescono ad individuare con maggior accuratezza la classe dalla dimensione più piccola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(model, X_val, y_val, confidence_level=0.95):\n",
    "    accuracy = model.score(X_val, y_val)\n",
    "    size = X_val.shape[0]\n",
    "    Z = norm.ppf((1 + confidence_level) / 2)\n",
    "    b = (2 * size * accuracy + Z ** 2) \n",
    "    delta_root = Z * np.sqrt(Z ** 2 + 4 * size * accuracy - 4 * size * accuracy ** 2) \n",
    "    return (b - delta_root) / (2 * (size + Z ** 2)), (b + delta_root) / (2 * (size + Z**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione tramite Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo i nostri test a partire dall'utilizzo di perceptron come metodo per l'individuazione di modelli migliori, il più semplice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perc = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", Perceptron(random_state=742, max_iter=10000, n_jobs=-1, class_weight=\"balanced\"))\n",
    "])\n",
    "grid = [{\n",
    "    \"classifier__penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "    \"classifier__alpha\": np.logspace(-2, 2, 5),\n",
    "    \"classifier__eta0\": [0.1, 0.5, 0.9]\n",
    "}, {\n",
    "    \"classifier__penalty\": [None],\n",
    "    \"classifier__eta0\": [0.1, 0.5, 0.9]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sua semplicità si rispecchia nei risultati che otteniamo: nessun tipo di regolarizzazione riesce a migliorare gli score dei modelli testati, che rimangono sempre uguali nonostante la variazione del parametro ``eta0``. Inoltre, i risultati si aggirano attorno all'84,8%, sono perciò decisamente migliorabili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__penalty</th>\n",
       "      <th>param_classifier__alpha</th>\n",
       "      <th>param_classifier__eta0</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.847695</td>\n",
       "      <td>0.006686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.847695</td>\n",
       "      <td>0.006686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.847695</td>\n",
       "      <td>0.006686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.822346</td>\n",
       "      <td>0.025104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.822346</td>\n",
       "      <td>0.025104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.786973</td>\n",
       "      <td>0.048481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.786973</td>\n",
       "      <td>0.048481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.707842</td>\n",
       "      <td>0.090954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.707842</td>\n",
       "      <td>0.090954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.695428</td>\n",
       "      <td>0.043643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_classifier__penalty param_classifier__alpha param_classifier__eta0  \\\n",
       "47                      None                     NaN                    0.9   \n",
       "45                      None                     NaN                    0.1   \n",
       "46                      None                     NaN                    0.5   \n",
       "0                         l2                    0.01                    0.1   \n",
       "2                 elasticnet                    0.01                    0.1   \n",
       "5                 elasticnet                    0.01                    0.5   \n",
       "3                         l2                    0.01                    0.5   \n",
       "11                elasticnet                     0.1                    0.1   \n",
       "9                         l2                     0.1                    0.1   \n",
       "1                         l1                    0.01                    0.1   \n",
       "\n",
       "    mean_test_score  std_test_score  \n",
       "47         0.847695        0.006686  \n",
       "45         0.847695        0.006686  \n",
       "46         0.847695        0.006686  \n",
       "0          0.822346        0.025104  \n",
       "2          0.822346        0.025104  \n",
       "5          0.786973        0.048481  \n",
       "3          0.786973        0.048481  \n",
       "11         0.707842        0.090954  \n",
       "9          0.707842        0.090954  \n",
       "1          0.695428        0.043643  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_perc = GridSearchCV(model_perc, grid, cv=skf)\n",
    "gs_perc.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_perc.cv_results_)[[\"param_classifier__penalty\", \"param_classifier__alpha\", \"param_classifier__eta0\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valutando l'intervallo di confidenza basato sul validation set, vediamo che l'accuratezza può oscillare tra il 67,7% e l'80,6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6767504808825887, 0.806448409473252)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_perc = gs_perc.best_estimator_\n",
    "y_pred = best_perc.predict(X_val)\n",
    "confidence_interval(best_perc, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori di *precision* e *recall* ci dicono che il migliore modello individuato tende ad essere più preciso nel distinguere dagli altri gli episodi \"brutti\", rispetto a quanto accade nell'individuare quelli \"belli\", mentre al contrario tende ad essere più capace, messo di fronte agli episodi \"belli\", di capire che sono effettivamente \"belli\", rispetto a quanto capita con quelli \"brutti\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>68</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>15</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    68    28\n",
       "good   15    59"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_perc.classes_, columns=best_perc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.82      0.71      0.76        96\n",
      "        good       0.68      0.80      0.73        74\n",
      "\n",
      "    accuracy                           0.75       170\n",
      "   macro avg       0.75      0.75      0.75       170\n",
      "weighted avg       0.76      0.75      0.75       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione tramite regressione logistica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo utilizzare al posto del perceptron un metodo più raffinato come la regressione logistica, in concomitanza con tutti i possibili tipi di regolarizzazione ammissibili, cioè Lasso, Ridge ed Elastic Net. Definiamo innanzitutto il nostro modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000, class_weight=\"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passiamo poi alla ricerca vera e propria. Se consideriamo le singole regolarizzazioni Lasso e Ridge, il risultato migliore si ha in corrispondenza di una regolarizzazione Ridge più intensa ($\\lambda = 0.01$). L'uso di una regolarizzazione sempre molto intensa, ma in concomitanza con la regressione Lasso, fa crollare a picco lo score. Il modello al secondo posto, che ha uno score comparabile con quello al primo, fa uso della regolarizzazione Lasso, ma con un peso molto più basso. Evidentemente limitare la dimensione dei coefficienti ha un effetto decisamente più significativo sulle performance di un ipotetico modello rispetto ad eliminare direttamente le feature inutili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__penalty</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.880716</td>\n",
       "      <td>0.015642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>l1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.878152</td>\n",
       "      <td>0.016629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>l1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.875646</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.875646</td>\n",
       "      <td>0.018959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>0.015639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>l2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>0.015639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>l2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>0.015639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.829921</td>\n",
       "      <td>0.009961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.761354</td>\n",
       "      <td>0.026484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.525657</td>\n",
       "      <td>0.077068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_classifier__penalty param_classifier__C  mean_test_score  \\\n",
       "1                        l2                0.01         0.880716   \n",
       "8                        l1                 100         0.878152   \n",
       "6                        l1                  10         0.875646   \n",
       "3                        l2                 0.1         0.875646   \n",
       "5                        l2                   1         0.873101   \n",
       "7                        l2                  10         0.873101   \n",
       "9                        l2                 100         0.873101   \n",
       "4                        l1                   1         0.829921   \n",
       "2                        l1                 0.1         0.761354   \n",
       "0                        l1                0.01         0.525657   \n",
       "\n",
       "   std_test_score  \n",
       "1        0.015642  \n",
       "8        0.016629  \n",
       "6        0.003135  \n",
       "3        0.018959  \n",
       "5        0.015639  \n",
       "7        0.015639  \n",
       "9        0.015639  \n",
       "4        0.009961  \n",
       "2        0.026484  \n",
       "0        0.077068  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_l1_l2 = {\n",
    "    \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "    \"classifier__C\": np.logspace(-2, 2, 5)\n",
    "}\n",
    "gs_l1_l2 = GridSearchCV(model_logreg, grid_l1_l2, cv=skf)\n",
    "gs_l1_l2.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_l1_l2.cv_results_)[[\"param_classifier__penalty\", \"param_classifier__C\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'uso di regolarizzazione Elastic Net riconferma il risultato già trovato in precedenza, ovverosia che i modelli migliori si hanno quando il peso della regolarizzazione è quasi esclusivamente concentrato sulla regolarizzazione Ridge anzichè Lasso, quindi con un valore di ``l1_ratio`` basso. In questo caso però, il miglior modello estratto si ha in concomitanza con un parametro di regolarizzazione relativamente basso. In ogni caso, il modello con lo score migliore ha uno score identico al secondo tra quelli osservati precedentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__l1_ratio</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.878152</td>\n",
       "      <td>0.016629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>0.009444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.870576</td>\n",
       "      <td>0.005813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.868031</td>\n",
       "      <td>0.006965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.860417</td>\n",
       "      <td>0.003081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.857815</td>\n",
       "      <td>0.014830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.837555</td>\n",
       "      <td>0.003916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.819743</td>\n",
       "      <td>0.014967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.781691</td>\n",
       "      <td>0.018372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_classifier__C param_classifier__l1_ratio  mean_test_score  \\\n",
       "8                  10                        0.1         0.878152   \n",
       "7                  10                        0.5         0.873101   \n",
       "6                  10                        0.9         0.870576   \n",
       "5                   1                        0.1         0.868031   \n",
       "2                 0.1                        0.1         0.860417   \n",
       "4                   1                        0.5         0.857815   \n",
       "3                   1                        0.9         0.837555   \n",
       "1                 0.1                        0.5         0.819743   \n",
       "0                 0.1                        0.9         0.781691   \n",
       "\n",
       "   std_test_score  \n",
       "8        0.016629  \n",
       "7        0.009444  \n",
       "6        0.005813  \n",
       "5        0.006965  \n",
       "2        0.003081  \n",
       "4        0.014830  \n",
       "3        0.003916  \n",
       "1        0.014967  \n",
       "0        0.018372  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_elasticnet = {\n",
    "    \"classifier__penalty\": [\"elasticnet\"],\n",
    "    \"classifier__C\": np.logspace(-1, 1, 3),\n",
    "    \"classifier__l1_ratio\": [0.9, 0.5, 0.1]\n",
    "}\n",
    "gs_elasticnet = GridSearchCV(model_logreg, grid_elasticnet, cv=skf)\n",
    "gs_elasticnet.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_elasticnet.cv_results_)[[\"param_classifier__C\", \"param_classifier__l1_ratio\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non usare nessuna regolarizzazione non porta ad un risultato interessante, perchè il modello ha sì uno score alto, ma è comunque più basso di quelli che utilizzano la regolarizzazione. Perciò, non prenderemo in considerazione quest'ultimo caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.873101</td>\n",
       "      <td>0.015639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score\n",
       "0         0.873101        0.015639"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_none = {\n",
    "    \"classifier__penalty\": [\"none\"]\n",
    "}\n",
    "gs_none = GridSearchCV(model_logreg, grid_none, cv=skf)\n",
    "gs_none.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_none.cv_results_)[[\"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osservando l'intervallo di confidenza, vediamo che il valore reale dell'accuratezza oscilla tra il 77,9% e l'88,8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7787826856194077, 0.8884919724910392)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logreg = gs_l1_l2.best_estimator_\n",
    "y_pred = best_logreg.predict(X_val)\n",
    "confidence_interval(best_logreg, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dai valori di *precision* e *recall* riusciamo a capire che il migliore modello individuato tramite regressione logstica è superiore a quello individuato tramite perceptron per quanto riguarda un po' tutte le metriche. Peggiora invece, anche se di poco, la situazione per quanto riguarda la *recall* degli episodi \"brutti\", ma questo viene compensato da un valore di *recall* della classe \"bad\" molto più alta. Questo miglioramento complessivo si riflette sull'*f1-score*, che è salito del 9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>86</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>17</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    86    10\n",
       "good   17    57"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_logreg.classes_, columns=best_logreg.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.83      0.90      0.86        96\n",
      "        good       0.85      0.77      0.81        74\n",
      "\n",
      "    accuracy                           0.84       170\n",
      "   macro avg       0.84      0.83      0.84       170\n",
      "weighted avg       0.84      0.84      0.84       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione mediante Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo le Support Vector Machines per poter utilizzare un metodo che sfrutta il *kernel trick* per permetterci di effettuare la classificazione utilizzando  molte più feature di quelle presenti nel dataset senza la loro effettiva generazione. Notiamo che l'uso di un kernel di tipo sigmoidale è la scelta migliore in combinazione con parametro $\\lambda$ della regolarizzazione Ridge pari a 1. Lo score del miglior modello è addirittura superiore allo score del miglior modello ottenuto tramite regressione logistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__kernel</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__degree</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.890913</td>\n",
       "      <td>0.018694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.888330</td>\n",
       "      <td>0.009452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.888330</td>\n",
       "      <td>0.009452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878171</td>\n",
       "      <td>0.018703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878171</td>\n",
       "      <td>0.018703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>linear</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878171</td>\n",
       "      <td>0.018703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878171</td>\n",
       "      <td>0.018703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878171</td>\n",
       "      <td>0.018703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.809604</td>\n",
       "      <td>0.013012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rbf</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.774231</td>\n",
       "      <td>0.043465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_classifier__kernel param_classifier__C param_classifier__degree  \\\n",
       "7                   sigmoid                   1                      NaN   \n",
       "10                  sigmoid                  10                      NaN   \n",
       "13                  sigmoid                 100                      NaN   \n",
       "2                    linear                0.01                      NaN   \n",
       "5                    linear                 0.1                      NaN   \n",
       "8                    linear                   1                      NaN   \n",
       "11                   linear                  10                      NaN   \n",
       "14                   linear                 100                      NaN   \n",
       "6                       rbf                   1                      NaN   \n",
       "12                      rbf                 100                      NaN   \n",
       "\n",
       "    mean_test_score  std_test_score  \n",
       "7          0.890913        0.018694  \n",
       "10         0.888330        0.009452  \n",
       "13         0.888330        0.009452  \n",
       "2          0.878171        0.018703  \n",
       "5          0.878171        0.018703  \n",
       "8          0.878171        0.018703  \n",
       "11         0.878171        0.018703  \n",
       "14         0.878171        0.018703  \n",
       "6          0.809604        0.013012  \n",
       "12         0.774231        0.043465  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", SVC(random_state=742, class_weight=\"balanced\"))\n",
    "])\n",
    "grid_svm = [{\n",
    "    \"classifier__kernel\": [\"rbf\", \"sigmoid\", \"linear\"],\n",
    "    \"classifier__C\": np.logspace(-2, 2, 5)\n",
    "}, {\n",
    "    \"classifier__kernel\": [\"poly\"],\n",
    "    \"classifier__degree\": [2, 3],\n",
    "    \"classifier__C\": np.logspace(-2, 2, 5)\n",
    "}]\n",
    "gs_svm = GridSearchCV(model_svm, grid_svm, cv=skf)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_svm.cv_results_)[[\"param_classifier__kernel\", \"param_classifier__C\", \"param_classifier__degree\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infatti, l'intervallo di confidenza è molto simile a quello del miglior modello ottenuto tramite regressione logistica, è infatti compreso tra il 77,2% e l'88,3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7722520572638996, 0.8835178653618844)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svm = gs_svm.best_estimator_\n",
    "y_pred = best_svm.predict(X_val)\n",
    "confidence_interval(best_svm, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice di confusione ci dice che l'*f1-score* identico a quello del miglior modello di regressione logistica è dovuto ad una maggiore precisione nell'individuare gli episodi \"brutti\", ma anche ad una minor precisione nell'individuare quelli \"brutti\". Questo fa sì che la *recall* per gli episodi \"brutti\" sia decisamente inferiore, mentre quella per gli episodi \"belli\" decisamente superiore, come ci testimoniano le metriche calcolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>79</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>11</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    79    17\n",
       "good   11    63"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_svm.classes_, columns=best_svm.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.88      0.82      0.85        96\n",
      "        good       0.79      0.85      0.82        74\n",
      "\n",
      "    accuracy                           0.84       170\n",
      "   macro avg       0.83      0.84      0.83       170\n",
      "weighted avg       0.84      0.84      0.84       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione tramite XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ad utilizzare anche XGBoost per vedere se è capace di darci risultati più interessanti. Per prima cosa lo testiamo senza applicare alcuna regolarizzazione per vedere quali iperparametri propri del classificatore sembrano essere migliori, poi vedremo se, applicando una regolarizzazione, il risultato è ancora migliorabile oppure no. I primi risultati non sono incoraggianti, lo score medio del modello migliore è addirittura inferiore di circa il 12% rispetto a quello dei migliori modelli considerati in precedenza. In particolare, il valore massimo dello score si ha in corrispondenza di un learning rate relativamente piccolo e di una profondità degli alberi realtivamente alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__learning_rate</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.766405</td>\n",
       "      <td>0.026638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.766405</td>\n",
       "      <td>0.026638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.766385</td>\n",
       "      <td>0.032637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.741036</td>\n",
       "      <td>0.023260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.738511</td>\n",
       "      <td>0.019842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.733480</td>\n",
       "      <td>0.007051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.733480</td>\n",
       "      <td>0.007051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.725827</td>\n",
       "      <td>0.017375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>15</td>\n",
       "      <td>0.725827</td>\n",
       "      <td>0.017375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.581213</td>\n",
       "      <td>0.001499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_classifier__learning_rate param_classifier__max_depth  \\\n",
       "4                             0.1                          10   \n",
       "5                             0.1                          15   \n",
       "3                             0.1                           5   \n",
       "0                            0.01                           5   \n",
       "6                               1                           5   \n",
       "7                               1                          10   \n",
       "8                               1                          15   \n",
       "1                            0.01                          10   \n",
       "2                            0.01                          15   \n",
       "9                              10                           5   \n",
       "\n",
       "   mean_test_score  std_test_score  \n",
       "4         0.766405        0.026638  \n",
       "5         0.766405        0.026638  \n",
       "3         0.766385        0.032637  \n",
       "0         0.741036        0.023260  \n",
       "6         0.738511        0.019842  \n",
       "7         0.733480        0.007051  \n",
       "8         0.733480        0.007051  \n",
       "1         0.725827        0.017375  \n",
       "2         0.725827        0.017375  \n",
       "9         0.581213        0.001499  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", XGBClassifier(objective=\"binary:logistic\", random_state=742, n_jobs=4))\n",
    "])\n",
    "grid = [{\n",
    "    \"classifier__learning_rate\": np.logspace(-2, 2, 5).tolist(),\n",
    "    \"classifier__max_depth\": [5, 10, 15]\n",
    "}]\n",
    "gs = GridSearchCV(model, grid, cv=skf)\n",
    "gs.fit(X_train, y_train)\n",
    "pd.DataFrame(gs.cv_results_)[[\"param_classifier__learning_rate\", \"param_classifier__max_depth\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'uso dei parametri individuati al passo precedente per costruire un modello su cui testare i parametri di regolarizzazione non ci fa ottenere un risultato migliore. Lo score infattti migliora solo dell'1% circa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__reg_alpha</th>\n",
       "      <th>param_classifier__reg_lambda</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.773922</td>\n",
       "      <td>0.054978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766308</td>\n",
       "      <td>0.055004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.761315</td>\n",
       "      <td>0.029497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.758809</td>\n",
       "      <td>0.020790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.753720</td>\n",
       "      <td>0.024354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753701</td>\n",
       "      <td>0.030175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.753701</td>\n",
       "      <td>0.030175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.753643</td>\n",
       "      <td>0.045742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.746106</td>\n",
       "      <td>0.024381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.743581</td>\n",
       "      <td>0.030011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_classifier__reg_alpha param_classifier__reg_lambda  mean_test_score  \\\n",
       "0                         0.01                         0.01         0.773922   \n",
       "7                          0.1                            1         0.766308   \n",
       "6                          0.1                          0.1         0.761315   \n",
       "10                           1                         0.01         0.758809   \n",
       "5                          0.1                         0.01         0.753720   \n",
       "2                         0.01                            1         0.753701   \n",
       "3                         0.01                           10         0.753701   \n",
       "1                         0.01                          0.1         0.753643   \n",
       "13                           1                           10         0.746106   \n",
       "19                          10                          100         0.743581   \n",
       "\n",
       "    std_test_score  \n",
       "0         0.054978  \n",
       "7         0.055004  \n",
       "6         0.029497  \n",
       "10        0.020790  \n",
       "5         0.024354  \n",
       "2         0.030175  \n",
       "3         0.030175  \n",
       "1         0.045742  \n",
       "13        0.024381  \n",
       "19        0.030011  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", XGBClassifier(objective=\"binary:logistic\", learning_rate=0.1, max_depth=10, random_state=742, n_jobs=4))\n",
    "])\n",
    "grid_xgb = {\n",
    "    \"classifier__reg_alpha\": np.logspace(-2, 2, 5),\n",
    "    \"classifier__reg_lambda\": np.logspace(-2, 2, 5),\n",
    "}\n",
    "gs_xgb = GridSearchCV(model_xgb, grid_xgb, cv=skf)\n",
    "gs_xgb.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_xgb.cv_results_)[[\"param_classifier__reg_alpha\", \"param_classifier__reg_lambda\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'intervallo di confidenza calcolato sul validation set però ci rivela che l'accuratezza è molto simile a quella del miglior modello ottenuto tramite regressione logistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7787826856194077, 0.8884919724910392)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb = gs_xgb.best_estimator_\n",
    "y_pred = best_xgb.predict(X_val)\n",
    "confidence_interval(best_xgb, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo infatti che ha una *precision* leggermente più alta del miglior modello ottenuto con regressione logistica per quanto riguarda gli episodi \"belli\", mentre quella per gli episodi \"brutti\" è rimasta invariata. La *recall* della classe \"bad\" è leggermente più alta che nel caso della regressione logistica, mentre quella della classe \"good\" è leggermente più bassa. Complessivamente, l'*f1-score* rimane invariato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>18</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    87     9\n",
       "good   18    56"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_xgb.classes_, columns=best_xgb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.83      0.91      0.87        96\n",
      "        good       0.86      0.76      0.81        74\n",
      "\n",
      "    accuracy                           0.84       170\n",
      "   macro avg       0.85      0.83      0.84       170\n",
      "weighted avg       0.84      0.84      0.84       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione mediante Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo come ultimo metodo quello basato su Random Forest, per vedere se, estraendo delle regole di partizione delle istanze dal nostro set di dati, è capace di ottenere dei risultati migliori rispetto ai metodi già utilizzati. Osserviamo che i risultati migliori non si hanno necessariamente in corrispondenza di un maggior numero di alberi, ma con una profondità degli stessi maggiore. Inoltre, tutti i migliori modelli hanno un numero di elementi da raggiungere per effettuare lo split molto alto. I risultati non sono però soddisfacenti, migliori rispetto a quelli ottenuti con XGBoost, ma sempre inferiori a quelli con tutti gli altri metodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>param_classifier__min_samples_leaf</th>\n",
       "      <th>param_classifier__min_samples_split</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.819763</td>\n",
       "      <td>0.013397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.804515</td>\n",
       "      <td>0.019520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.804476</td>\n",
       "      <td>0.026505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.801989</td>\n",
       "      <td>0.016926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.799464</td>\n",
       "      <td>0.014747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.799445</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.799445</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.799445</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.796939</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.796900</td>\n",
       "      <td>0.015048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_classifier__n_estimators param_classifier__max_depth  \\\n",
       "96                             100                          10   \n",
       "106                            200                          10   \n",
       "79                             200                           8   \n",
       "34                             200                           4   \n",
       "97                             200                          10   \n",
       "54                             100                           8   \n",
       "103                            200                          10   \n",
       "100                            200                          10   \n",
       "57                             100                           8   \n",
       "76                             200                           8   \n",
       "\n",
       "    param_classifier__min_samples_leaf param_classifier__min_samples_split  \\\n",
       "96                                   2                                  10   \n",
       "106                                  4                                  10   \n",
       "79                                   4                                  10   \n",
       "34                                   1                                  10   \n",
       "97                                   2                                  10   \n",
       "54                                   1                                   3   \n",
       "103                                  4                                   5   \n",
       "100                                  4                                   3   \n",
       "57                                   1                                   5   \n",
       "76                                   4                                   5   \n",
       "\n",
       "     mean_test_score  std_test_score  \n",
       "96          0.819763        0.013397  \n",
       "106         0.804515        0.019520  \n",
       "79          0.804476        0.026505  \n",
       "34          0.801989        0.016926  \n",
       "97          0.799464        0.014747  \n",
       "54          0.799445        0.013611  \n",
       "103         0.799445        0.013611  \n",
       "100         0.799445        0.013611  \n",
       "57          0.796939        0.007576  \n",
       "76          0.796900        0.015048  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnd = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", RandomForestClassifier(n_jobs=-1, random_state=742, class_weight=\"balanced\"))\n",
    "])\n",
    "grid_rnd = {\n",
    "    \"classifier__n_estimators\": [100, 200, 500],\n",
    "    \"classifier__max_depth\": [2, 4, 8, 10],\n",
    "    \"classifier__min_samples_leaf\": [1, 2, 4],\n",
    "    \"classifier__min_samples_split\": [3, 5, 10]\n",
    "}\n",
    "gs_rnd = GridSearchCV(model_rnd, grid_rnd, cv=skf)\n",
    "gs_rnd.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_rnd.cv_results_)[[\"param_classifier__n_estimators\", \"param_classifier__max_depth\", \"param_classifier__min_samples_leaf\", \"param_classifier__min_samples_split\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'intervallo di accuratezza mostra un miglioramento rispetto a quello di perceptron, essendo spostato rispetto a quest'ultimo leggermente verso destra, ma rimane più spostato verso sinistra rispetto a tutti gli altri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7463603813080039, 0.8633905993791287)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnd = gs_rnd.best_estimator_\n",
    "y_pred = best_rnd.predict(X_val)\n",
    "confidence_interval(best_rnd, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il miglior modello individuato è capace di distinguere tra gli altri gli episodi con valutazione \"brutto\" con una *precision* che è più bassa di tutti gli altri modelli, mentre quella degli episodi con valutazione \"bello\" è più alta rispetto a tutti gli altri modelli. La *recall* per la classe \"bad\" è molto alta, mentre invece quella della classe \"good\" cola a picco, trascinando con sè il valore di *f1-score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>89</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>25</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    89     7\n",
       "good   25    49"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_rnd.classes_, columns=best_rnd.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.78      0.93      0.85        96\n",
      "        good       0.88      0.66      0.75        74\n",
      "\n",
      "    accuracy                           0.81       170\n",
      "   macro avg       0.83      0.79      0.80       170\n",
      "weighted avg       0.82      0.81      0.81       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confronto tra modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per poter confrontare più accuratamente i modelli definiamo prima la funzione capace di calcolare se la differenza di accuratezza tra due modelli è statisticamente significativa, che ci restituirà gli estremi dell'intervallo di variazione, oltre ad un booleano che ci dice se effettivamente la differenza tra le due accuratezze è statisticamente significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_difference_interval(accuracy1, accuracy2, X_size, confidence_level=0.99):\n",
    "    abs_diff = abs(accuracy1 - accuracy2)\n",
    "    radius = norm.ppf((1 + confidence_level) / 2) * np.sqrt((accuracy1 * (1 - accuracy1) + accuracy2 * (1 - accuracy2)) / X_size)\n",
    "    min_int = abs_diff - radius\n",
    "    max_int = abs_diff + radius\n",
    "    return min_int, max_int, min_int > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poi defininiamo un modello \"dummy\", ovverosia un modello che effettui le sue predizioni senza alcun tipo di conoscenza sulle regole interne al dataset che permettono di associare un'istanza alla sua classe, facendo perciò le sue predizioni in maniera casuale. In realtà, per fare in modo che il nostro modello di benchmark abbia un'accuratezza maggiore, in modo da mettere maggiormente alla prova i modelli precedentemente estratti, non effettuerà la classificazione in maniera completamente casuale, ma  assegnerà tutte le istanze alla classe più frequente nel dataset. Questo perchè sappiamo esistere un leggero sbilanciamento tra le istanze delle due classi e perciò tutti i modelli che sono più propensi ad assegnare le istanze alla classe più frequente piuttosto che all'altra, avranno automaticamente un certo vantaggio sugli altri modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5647058823529412"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", DummyClassifier(strategy=\"most_frequent\", random_state=742))\n",
    "])\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo che tutti i migliori modelli estratti mediante i metodi utilizzati in precedenza sono significativamente migliori a livello statistico del modello di benchmark con un livello di confidenza del 99%, segno che hanno effetivamente appreso qualcosa dal dataset. Designamo come modello migliore perciò quello ottenuto mediante regressione logistica, che è quello dei due con accuratezza più alta. Anche se quest'ultima non è statisticamente più significativa di quella dei migliori modelli ottenuti con XGBoost, Random Forest o Support Vector Machines, il modello è stato comunque capace meglio degli altri tre di bilanciare *precision* e *recall* delle due classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>lower_bound_interval_dummy</th>\n",
       "      <th>upper_bound_interval_dummy</th>\n",
       "      <th>statistically_significative_dummy</th>\n",
       "      <th>lower_bound_interval_best</th>\n",
       "      <th>upper_bound_interval_best</th>\n",
       "      <th>statistically_significative_best</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.564706</td>\n",
       "      <td>-0.138519</td>\n",
       "      <td>0.138519</td>\n",
       "      <td>False</td>\n",
       "      <td>0.183877</td>\n",
       "      <td>0.369064</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.747059</td>\n",
       "      <td>0.052089</td>\n",
       "      <td>0.312617</td>\n",
       "      <td>True</td>\n",
       "      <td>0.008743</td>\n",
       "      <td>0.179492</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.122329</td>\n",
       "      <td>0.371789</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.051035</td>\n",
       "      <td>0.109859</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machines</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.148264</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.072397</td>\n",
       "      <td>0.084162</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.154783</td>\n",
       "      <td>0.398159</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.077703</td>\n",
       "      <td>0.077703</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.154783</td>\n",
       "      <td>0.398159</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.077703</td>\n",
       "      <td>0.077703</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         accuracy  lower_bound_interval_dummy  \\\n",
       "model                                                           \n",
       "Dummy                    0.564706                   -0.138519   \n",
       "Perceptron               0.747059                    0.052089   \n",
       "Random Forest            0.811765                    0.122329   \n",
       "Support Vector Machines  0.835294                    0.148264   \n",
       "Logistic Regression      0.841176                    0.154783   \n",
       "XGBoost                  0.841176                    0.154783   \n",
       "\n",
       "                         upper_bound_interval_dummy  \\\n",
       "model                                                 \n",
       "Dummy                                      0.138519   \n",
       "Perceptron                                 0.312617   \n",
       "Random Forest                              0.371789   \n",
       "Support Vector Machines                    0.392913   \n",
       "Logistic Regression                        0.398159   \n",
       "XGBoost                                    0.398159   \n",
       "\n",
       "                         statistically_significative_dummy  \\\n",
       "model                                                        \n",
       "Dummy                                                False   \n",
       "Perceptron                                            True   \n",
       "Random Forest                                         True   \n",
       "Support Vector Machines                               True   \n",
       "Logistic Regression                                   True   \n",
       "XGBoost                                               True   \n",
       "\n",
       "                         lower_bound_interval_best  upper_bound_interval_best  \\\n",
       "model                                                                           \n",
       "Dummy                                     0.183877                   0.369064   \n",
       "Perceptron                                0.008743                   0.179492   \n",
       "Random Forest                            -0.051035                   0.109859   \n",
       "Support Vector Machines                  -0.072397                   0.084162   \n",
       "Logistic Regression                      -0.077703                   0.077703   \n",
       "XGBoost                                  -0.077703                   0.077703   \n",
       "\n",
       "                         statistically_significative_best  \n",
       "model                                                      \n",
       "Dummy                                                True  \n",
       "Perceptron                                           True  \n",
       "Random Forest                                       False  \n",
       "Support Vector Machines                             False  \n",
       "Logistic Regression                                 False  \n",
       "XGBoost                                             False  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [dummy, best_perc, best_logreg, best_svm, best_xgb, best_rnd]\n",
    "scores = list(map(lambda model: model.score(X_val, y_val), models))\n",
    "data = []\n",
    "dummy_accuracy = dummy.score(X_val, y_val)\n",
    "best_accuracy = max(scores)\n",
    "for model in models:\n",
    "    accuracy = model.score(X_val, y_val)\n",
    "    min_int_dummy, max_int_dummy, significative_dummy = models_difference_interval(accuracy, dummy_accuracy, X_val.shape[0])\n",
    "    min_int_best, max_int_best, significative_best = models_difference_interval(accuracy, best_accuracy, X_val.shape[0], 0.95)\n",
    "    data.append((accuracy, min_int_dummy, max_int_dummy, significative_dummy, min_int_best, max_int_best, significative_best))\n",
    "pd.DataFrame(data,\n",
    "             index=pd.Index([\"Dummy\", \"Perceptron\", \"Logistic Regression\", \"Support Vector Machines\", \"XGBoost\", \"Random Forest\"], name=\"model\"),\n",
    "             columns=[\"accuracy\", \"lower_bound_interval_dummy\", \"upper_bound_interval_dummy\", \"statistically_significative_dummy\", \"lower_bound_interval_best\", \"upper_bound_interval_best\", \"statistically_significative_best\"]) \\\n",
    "  .sort_values(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretazione della conoscenza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scelto il modello migliore, osserviamo perciò quali sono le feature che sono per esso più incisive per la scelta dell'una o dell'altra classe. Le feature che maggiormente portano un episodio ad essere considerato \"bello\" sembrano essere le esclamazioni (\"ah\", \"hmm\", \"oh\", \"huh\", \"hey\", \"uh huh\") e le loro varianti, le risate (\"heh heh heh\"), ma anche altre parole come \"minute\", \"sir\", \"come\", \"everybody\" ed \"hello\". Tutto sommato, queste informazioni sono in linea con quanto ci aspettavamo: gli episodi migliori della serie sono quelli dove vengono trasmesse più emozioni e il pubblico si sente più coinvolto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ah                0.034467\n",
       "heh               0.032724\n",
       "etc               0.030716\n",
       "hmmm              0.029666\n",
       "heh heh           0.029116\n",
       "oh                0.028722\n",
       "ohh               0.025645\n",
       "hey wait          0.025180\n",
       "Maude Flanders    0.025160\n",
       "minut             0.024720\n",
       "sir               0.024362\n",
       "hey               0.024027\n",
       "huh               0.023988\n",
       "heh heh heh       0.023742\n",
       "come              0.023456\n",
       "uh huh            0.023438\n",
       "ahh               0.023336\n",
       "everybodi         0.022798\n",
       "Miss Hoover       0.022102\n",
       "hello             0.021954\n",
       "dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = pd.Series(best_logreg.named_steps[\"classifier\"].coef_[0], index).sort_values(ascending=False)\n",
    "coefs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se consideriamo solo gli 1-gram che rendono più probabile che l'episodio abbia un buon rating, vediamo come questa teoria è confermata, trovando oltre alle già citate parole anche \"ewww\". Troviamo però anche due cognomi, \"mcclure\", appartenente al personaggio di Troy McClure, e \"simpson\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ah           0.034467\n",
       "heh          0.032724\n",
       "etc          0.030716\n",
       "hmmm         0.029666\n",
       "oh           0.028722\n",
       "ohh          0.025645\n",
       "minut        0.024720\n",
       "sir          0.024362\n",
       "hey          0.024027\n",
       "huh          0.023988\n",
       "come         0.023456\n",
       "ahh          0.023336\n",
       "everybodi    0.022798\n",
       "hello        0.021954\n",
       "ooooh        0.021757\n",
       "ewww         0.021706\n",
       "mcclure      0.021094\n",
       "uh-huh       0.021083\n",
       "simpson      0.021045\n",
       "hmm          0.020582\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs[~coefs.index.str.contains(r\"[A-Z ]\")].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al contrario, le feature che portano un episodio ad essere considerato \"brutto\" sembrano essere parole che contengono giudizi o in generale aggettivi come \"great\", \"huge\", \"awesome\", \"final\", \"sweet\", ma anche parole di uso comune come \"everyone\", \"back\", \"made\", \"use\", \"move\", \"welcome\"... Forse il loro uso in maniera sovrabbondante potrebbe far perdere di significato l'episodio. Troviamo inoltre quella che sembra essere una parola che Marge usa spesso per apostrofare Homer, \"homie\", nonchè il nome di un personaggio, ovvero \"lenny\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lenni           -0.018631\n",
       "na              -0.019044\n",
       "huge            -0.019112\n",
       "oh god          -0.019215\n",
       "great           -0.019457\n",
       "back            -0.019477\n",
       "everyon         -0.019611\n",
       "made            -0.019913\n",
       "Springfield     -0.020288\n",
       "Carl Carlson    -0.020388\n",
       "make            -0.020888\n",
       "use             -0.021535\n",
       "move            -0.022276\n",
       "Lenny Leonard   -0.023877\n",
       "final           -0.024303\n",
       "awesom          -0.024430\n",
       "homi            -0.024815\n",
       "welcom          -0.026008\n",
       "sweeti          -0.026392\n",
       "till            -0.028860\n",
       "dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se consideriamo solo gli 1-gram, oltre alle già discusse parole, troviamo anche \"disco\", \"god\", \"never\" e \"bully\". Evidentemente gli episodi dove si parla di temi religiosi o di bullismo non sono stati molto apprezzati. La parola \"disco\" fa probabilmente anch'essa riferimento al nome di un personaggio, \"Disco Stu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bulli     -0.017704\n",
       "disco     -0.017892\n",
       "god       -0.018175\n",
       "never     -0.018369\n",
       "lenni     -0.018631\n",
       "na        -0.019044\n",
       "huge      -0.019112\n",
       "great     -0.019457\n",
       "back      -0.019477\n",
       "everyon   -0.019611\n",
       "made      -0.019913\n",
       "make      -0.020888\n",
       "use       -0.021535\n",
       "move      -0.022276\n",
       "final     -0.024303\n",
       "awesom    -0.024430\n",
       "homi      -0.024815\n",
       "welcom    -0.026008\n",
       "sweeti    -0.026392\n",
       "till      -0.028860\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs[~coefs.index.str.contains(r\"[A-Z ]\")].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ci concentriamo esclusivamente sui personaggi osserviamo che tra quelli che più portano una puntata ad essere apprezzata sono i già visti \"Maude Flanders\" e la \"signorina Hoover\", ma anche la \"signora della mensa\", il \"signor Smithers\", gli anziani e \"Jasper\", uno degli ospiti della casa di riposo di Springfield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Maude Flanders     0.025160\n",
       "Miss Hoover        0.022102\n",
       "Lunchlady Doris    0.020281\n",
       "Lewis Clark        0.019196\n",
       "Waylon Smithers    0.019190\n",
       "Singers            0.017886\n",
       "Old People         0.016665\n",
       "Chuck              0.015853\n",
       "Jasper Beardly     0.015104\n",
       "JANEY              0.014704\n",
       "dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_characters = coefs[characters_cols].sort_values(ascending=False)\n",
    "best_characters.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tra i personaggi che invece abbassano il gradimento di una puntata troviamo il \"ricco texano\", la businesswoman \"Lindsey Naegle\", \"Brandine\", cioè la moglie di Cletus il bifolco, nonchè \"Cletus il bifolco\" stesso. Quello che hanno in comune questi personaggi è di essere tutti inerentemente repubblicani, rappresentando la parte più conservatrice degli Stati Uniti. La serie, di chiaro stampo democratico, satirizza molto questi personaggi e questo potrebbe non essere gradito al pubblico fortemente repubblicano della rete televisiva. Tra gli altri personaggi a sorpresa troviamo \"Secco\", la madre di Nelson, l'\"adolescente dalla voce stridula\", ma anche \"Disco Stu\" e \"Lenny\" e \"Carl\", i nomi dei quali li avevamo già incontrati in precedenza, come personaggi o come 1-gram dei loro nomi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "DOLPH                 -0.012477\n",
       "The Rich Texan        -0.013465\n",
       "Mrs. Muntz            -0.014650\n",
       "Lindsay Naegle        -0.014871\n",
       "Brandine Del Roy      -0.015295\n",
       "Disco Stu             -0.016245\n",
       "Squeaky-Voiced Teen   -0.016470\n",
       "Cletus Spuckler       -0.017380\n",
       "Carl Carlson          -0.020388\n",
       "Lenny Leonard         -0.023877\n",
       "dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_characters.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tra i luoghi in cui si svolgono le puntate dei Simpson che più portano ad alzare il gradimento di una puntata ci sono le stanze di casa Simpson, le stanze della scuola elementare di Springfield, ma anche altri luoghi meno riconoscibili come le vie della città o le automobili. I primi due luoghi, casa Simpson e la scuola elementare, sono i luoghi dove chiaramente si svolgono il maggior numero di scene degli episodi, i luoghi più ricorrenti, perchè è dove i personaggi passano maggiormente il loro tempo. Evidentemente quando non è chiaro il luogo in cui si svolge l'azione nel dataset è stato semplicemente etichettato come \"Somewhere\" e così lì si svolgono molte scene della serie tv e questo porta il suo coefficiente ad essere alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Somewhere                                   0.019343\n",
       "Simpson Kitchen                             0.015701\n",
       "Simpson Dining Room                         0.015625\n",
       "Miss Hoover's Classroom                     0.014054\n",
       "Street                                      0.013217\n",
       "Downtown Street                             0.012969\n",
       "Main Street                                 0.012756\n",
       "PARADE GROUNDS                              0.012410\n",
       "Selma's Car                                 0.012400\n",
       "Springfield Elementary School Auditorium    0.012330\n",
       "dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_locations = coefs[locations_cols].sort_values(ascending=False)\n",
    "best_locations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tra i luoghi che invece portano il pubblico a dire che un episodio è \"brutto\" troviamo invece molti più luoghi di contorno come un generico \"negozio\", la spiaggia, le automobili di Homer e di Marge, il tribunale, il garage di casa Simpson, la via dove abitano i Simpson, \"Evergreen Terrace\", lo stadio, la città di Springfield in generale, che avevamo già incontrato. Evidentemente in questi luoghi si svolgono scene più irrilevanti ai fini della trama o comunque sono luoghi dove più spesso si svolgono scene non interessanti per il pubblico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Store               -0.011815\n",
       "Marge's Car         -0.011896\n",
       "Beach               -0.012011\n",
       "Homer's Car         -0.012339\n",
       "Announcer's Booth   -0.012750\n",
       "Court               -0.012867\n",
       "Simpson Garage      -0.014189\n",
       "Evergreen Terrace   -0.015063\n",
       "STADIUM             -0.015708\n",
       "Springfield         -0.020288\n",
       "dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_locations.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo come l'intercetta del nostro modello sia una valore molto piccolo, approssimabile con 0. Questo significa che il nostro modello non conferisce a priori un bonus o un malus ad un episodio prima di classificarlo e perciò entrambe le valutazioni, \"bello\" o \"brutto\", hanno la stessa probabilità di essere assegnate ad un episodio a partire dalle parole conenute nel copione di quest'ultimo, dai personaggi e dalle location che compaiono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.356339858905328e-06"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logreg.named_steps[\"classifier\"].intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test di reti neurali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione tramite multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo quindi un approccio con una maggior numero di feature, ma dove le feature sono autonomamente apprese dal metodo stesso, quindi utilizzando una rete neurale. Perdiamo chiaramente la possibilità di interpretare il nostro modello. Prima di tutto testiamo quella che scikit-learn ci fornisce di base, ovverosia il multi-layer perceptron. Lo score medio con i parametri scelti non è troppo distante da quello degli altri modelli valutati precedentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.046017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score\n",
       "0         0.852957        0.046017"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", MLPClassifier(shuffle=True, hidden_layer_sizes=(500, 20), batch_size=10, random_state=742))\n",
    "])\n",
    "gs = GridSearchCV(mlp, {}, cv=skf, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "pd.DataFrame(gs.cv_results_)[[\"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche l'accuratezza non è male, ha un valore comparabile con quella del miglior metodo ottenuto con Random Forest, però non è chiaramente capace di superare il miglior modello in assoluto che siamo riusciti ad individuare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8176470588235294"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione mediante reti neurali convoluzionali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In seconda battuta proviamo a costruire una rete neurale più complessa con l'aiuto di Keras e TensorFlow, vedendo se un approccio che sfrutta una rete neurale convoluzionale è capace di ottenere dei risultati migliori. Inseriamo un primo livello di reshape per ottenere una matrice 2D da ciascuna istanza, applichiamo la convoluzione, poi linearizziamo gli output per farli passare attraverso due strati simili a quelli inseriti nel multi-layer perceptron. Su uno applichiamo la regolarizzazione come per il modello migliore trovato in precedenza e prima del secondo applichiamo uno strato di \"dropout\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 163, 52)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 154, 500)          260500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 154, 500)          250500    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 77000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 77000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1540020   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 2,051,062\n",
      "Trainable params: 2,051,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential([\n",
    "    Reshape((163, 52), input_shape=(X_train.shape[1],)),\n",
    "    Conv1D(500, 10, activation=\"relu\"),\n",
    "    Dense(500, activation=\"relu\", kernel_regularizer=l2(100)),\n",
    "    Flatten(),\n",
    "    Dropout(0.1, seed=742),\n",
    "    Dense(20, activation=\"relu\"),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codifichiamo le classi da predire per ciascuna istanza in formato \"one-hot encoding\", compiliamo il modello facendoci restituire anche la metrica di accuratezza e poi effettuiamo il fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "394/394 [==============================] - 3s 8ms/step - loss: 25074.2167 - accuracy: 0.6117\n",
      "Epoch 2/10\n",
      "394/394 [==============================] - 3s 8ms/step - loss: 3066.9455 - accuracy: 0.8173\n",
      "Epoch 3/10\n",
      "394/394 [==============================] - 3s 8ms/step - loss: 197.9233 - accuracy: 0.7766\n",
      "Epoch 4/10\n",
      "394/394 [==============================] - 3s 9ms/step - loss: 6.2784 - accuracy: 0.6269\n",
      "Epoch 5/10\n",
      "394/394 [==============================] - 3s 9ms/step - loss: 0.7516 - accuracy: 0.5888\n",
      "Epoch 6/10\n",
      "394/394 [==============================] - 3s 9ms/step - loss: 0.7016 - accuracy: 0.6396\n",
      "Epoch 7/10\n",
      "394/394 [==============================] - 3s 9ms/step - loss: 0.6805 - accuracy: 0.6904\n",
      "Epoch 8/10\n",
      "394/394 [==============================] - 3s 9ms/step - loss: 0.6698 - accuracy: 0.7132\n",
      "Epoch 9/10\n",
      "394/394 [==============================] - 3s 9ms/step - loss: 0.6325 - accuracy: 0.7919\n",
      "Epoch 10/10\n",
      "394/394 [==============================] - 4s 10ms/step - loss: 0.5473 - accuracy: 0.8528\n"
     ]
    }
   ],
   "source": [
    "y_enc_train = to_categorical(np.where(y_train == \"good\", 1, 0))\n",
    "y_enc_val = to_categorical(np.where(y_val == \"good\", 1, 0))\n",
    "nn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "fit_history = nn.fit(X_train.toarray(), y_enc_train, epochs=10, batch_size=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine valutiamo il modello sul validation set, la *categorical cross-entropy* non sembra essere particolarmente alta, ma nemmeno troppo bassa, mentre l'accuratezza, anche se sempre migliore di quella del modello benchmark, è la più bassa tra tutte quelle dei modelli valutati, anche più bassa dei modelli ottenuti da perceptron, il più scarso tra i metodi che usano statistical learning, e dei modelli ottenuti da multi-layer perceptron, l'altra rete neurale che abbiamo addestrato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7129575715345495, 0.7117646932601929]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.evaluate(X_val.toarray(), y_enc_val, batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataIntensive",
   "language": "python",
   "name": "dataintensive"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
