{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un'analisi predittiva sulla serie tv più amata del mondo - I Simpson\n",
    "## Esame di Programmazione di Applicazioni Data Intensive\n",
    "#### Laurea in Ingegneria e Scienze Informatiche - A.A. 2019/2020\n",
    "#### Matteo Castellucci - matteo.castellucci3@studio.unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrizione del problema e della variabile da predire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vogliono analizzare le caratteristiche degli episodi della serie tv \"I Simpson\" (in originale \"The Simpsons\") con il fine di predire quanto più accuratamente possibile il [*rating* su IMDB](https://www.imdb.com/title/tt0096697/) di ciascun episodio. Immaginiamo perciò che tutti gli episodi non siano ancora andati in onda, in modo tale che gli spettatori non abbiano ancora potuto farsene un'opinione, ma che siano già stati prodotti e ne sia stata programmata la trasmissione, in modo tale da sapere in anticipo il contenuto di ciascuno di essi e la data in cui saranno trasmessi.\n",
    "\n",
    "Poichè il valore assoluto del *rating* di per sè è per noi poco significativo, essendo legato alle esperienze personali degli spettatori, cercheremo di predire una variabile categorica ottenuta da esso che ci dica se l'episodio complessivamente è ritenuto \"bello\" oppure \"brutto\" dal pubblico.\n",
    "Scegliamo le classi secondo la seguente logica:\n",
    "- **Bello**: rating superiore a 7,5 incluso\n",
    "- **Brutto**: rating inferiore a 7,5 escluso  \n",
    "\n",
    "Dovremo perciò affrontare un problema di classificazione binaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importazione delle librerie e impostazione delle opzioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa ci preoccupiamo di importare tutte le librerie, le funzioni e gli oggetti che utilizzeremo. Inoltre, scarichiamo tutti i componenti della libreria \"NLTK\" che ci serviranno e impostiamo tutte le proprietà delle librerie a noi utili, in modo da non doverlo fare più avanti. Sopprimiamo inoltre i *warning* inutili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /home/matteo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/matteo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/matteo/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/matteo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os.path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from scipy.stats import norm\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Conv1D, Flatten, Dropout, Reshape\n",
    "from keras.regularizers import l2\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pd.options.display.max_colwidth = 100\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importazione del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verranno utilizzati i *dataset* contenuti nel post \"The Simpsons by the data\" originariamente pubblicati da William Kukierski e, data la loro cancellazione dal sito \"Kaggle\", aggiunti nuovamente da Prashant Banerjee. Mi sono riservato di applicare alcune modifiche al dataset originale per renderlo importabile tramite la libreria \"Pandas\", visto che presentava alcuni errori di formattazione. Detto questo, lo scarichiamo e lo importiamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_filename = \"the_simpsons.zip\"\n",
    "if not os.path.exists(\"the_simpsons.zip\"):\n",
    "    from urllib.request import urlretrieve\n",
    "    urlretrieve(\"https://drive.google.com/uc?export=download&id=1M4xsnaYl5KfsQ9Wx7wQ9Jc5oBdeCPcTx\", zip_filename)\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(zip_filename) as file:\n",
    "        file.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprensione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo per prima cosa i personaggi. Notiamo che ogni riga ha già un suo identificatore dato dalla colonna ``id`` e che, oltre a questa colonna, nel file sono presenti altre tre *feature*:\n",
    "\n",
    "- ``name``: il nome del personaggio\n",
    "- ``normalized_name``: il nome del personaggio costituito da soli caratteri alfabetici minuscoli\n",
    "- ``gender``: il genere sessuale del personaggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dewey Largo</td>\n",
       "      <td>dewey largo</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Children</td>\n",
       "      <td>children</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>bart simpson</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>lisa simpson</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Patty Bouvier</td>\n",
       "      <td>patty bouvier</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ned Flanders</td>\n",
       "      <td>ned flanders</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mechanical Santa</td>\n",
       "      <td>mechanical santa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tattoo Man</td>\n",
       "      <td>tattoo man</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Waylon Smithers</td>\n",
       "      <td>waylon smithers</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C. Montgomery Burns</td>\n",
       "      <td>c montgomery burns</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name     normalized_name gender\n",
       "id                                                \n",
       "6           Dewey Largo         dewey largo      m\n",
       "7              Children            children    NaN\n",
       "8          Bart Simpson        bart simpson      m\n",
       "9          Lisa Simpson        lisa simpson      f\n",
       "10        Patty Bouvier       patty bouvier      f\n",
       "11         Ned Flanders        ned flanders      m\n",
       "12     Mechanical Santa    mechanical santa    NaN\n",
       "13           Tattoo Man          tattoo man    NaN\n",
       "14      Waylon Smithers     waylon smithers      m\n",
       "15  C. Montgomery Burns  c montgomery burns      m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_characters = pd.read_csv(\"simpsons_characters.csv\", index_col=\"id\").sort_values(\"id\")\n",
    "simpsons_characters.iloc[5:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può notare, la stragrande maggioranza dei personaggi non ha genere - perchè è sconosciuto o perché il personaggio è un insieme di persone - e perciò non è per noi un elemento particolarmente significativo. Questo DataFrame verrà perciò utilizzato solo per identificare il nome dei personaggi, in quanto non riporta altro che tutti i possibili nomi di personaggi contenuti nei copioni dei vari episodi, essendo presenti solo nomi unici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6722</td>\n",
       "      <td>6722</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6722</td>\n",
       "      <td>6722</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name normalized_name gender\n",
       "count   6722            6722    323\n",
       "unique  6722            6722      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_characters.describe().loc[[\"count\", \"unique\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo perciò già provvedere alla rimozione delle colonne non necessarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marge Simpson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homer Simpson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Seymour Skinner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JANEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Todd Flanders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name\n",
       "id                 \n",
       "1     Marge Simpson\n",
       "2     Homer Simpson\n",
       "3   Seymour Skinner\n",
       "4             JANEY\n",
       "5     Todd Flanders"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_characters.drop(columns=[\"normalized_name\", \"gender\"], inplace=True)\n",
    "simpsons_characters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo, come vedremo anche per gli altri, è un *dataset* molto pesante in memoria perchè contiene un numero molto alto di istanze (6722 per la precisione). Per questo, modifichiamo il tipo delle sue colonne in modo da ottimizzare al massimo l'uso della memoria sfruttando i tipi di Pandas. In questo modo, riusciamo a ridurre lo spazio occupato di quasi 5 volte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6722 entries, 1 to 6749\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    6722 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 506.4 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_characters.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6722 entries, 1 to 6749\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    6722 non-null   string\n",
      "dtypes: string(1)\n",
      "memory usage: 105.0 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_characters = simpsons_characters.astype({\n",
    "    \"name\": \"string\",\n",
    "})\n",
    "simpsons_characters.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osservando le location notiamo un pattern simile a quello già visto per i personaggi: abbiamo una colonna ``id`` che rappresenta l'identificatore della *location*, mentre le *feature* vere e proprie sono:\n",
    "\n",
    "- ``name``: il nome della *location*\n",
    "- ``normalized_name``: il nome della *location* fatto di soli caratteri alfabetici in minuscolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Street</td>\n",
       "      <td>street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Car</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>springfield elementary school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Auditorium</td>\n",
       "      <td>auditorium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Simpson Home</td>\n",
       "      <td>simpson home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name                normalized_name\n",
       "id                                                              \n",
       "1                          Street                         street\n",
       "2                             Car                            car\n",
       "3   Springfield Elementary School  springfield elementary school\n",
       "4                      Auditorium                     auditorium\n",
       "5                    Simpson Home                   simpson home"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_locations = pd.read_csv(\"simpsons_locations.csv\", index_col=\"id\").sort_values(\"id\")\n",
    "simpsons_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso, il \"DataFrame\" è solamente utile per identificare il nome dei vari luoghi in cui gli episodi si svolgono, contenendo i luoghi che compaiono nei copioni dei vari episodi un'unica volta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>normalized_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4459</td>\n",
       "      <td>4459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4459</td>\n",
       "      <td>4459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name normalized_name\n",
       "count   4459            4459\n",
       "unique  4459            4459"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_locations.describe().loc[[\"count\", \"unique\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo perciò rimuovere ``normalized_name``, non essendoci di alcuna utilità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Springfield Elementary School</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Auditorium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Simpson Home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name\n",
       "id                               \n",
       "1                          Street\n",
       "2                             Car\n",
       "3   Springfield Elementary School\n",
       "4                      Auditorium\n",
       "5                    Simpson Home"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_locations.drop(columns=\"normalized_name\", inplace=True)\n",
    "simpsons_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso l'uso di memoria incide di meno rispetto al dataset precedente. Decidiamo però lo stesso di scegliere i tipi adeguati per le colonne del DataFrame, anche solo per coerenza con il caso precedente, e otteniamo anche in questo caso una diminuzione nell'uso di memoria di un fattore 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4459 entries, 1 to 4459\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    4459 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 356.8 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_locations.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4459 entries, 1 to 4459\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    4459 non-null   string\n",
      "dtypes: string(1)\n",
      "memory usage: 69.7 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_locations = simpsons_locations.astype({\n",
    "    \"name\": \"string\",\n",
    "})\n",
    "simpsons_locations.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il terzo dataset invece è quello che contiene i dati relativi ai singoli episodi. In esso troviamo un identificatore per ciascun episodio contraddistinto dal nome ``id``, come nei precedenti dataset, mentre le altre sue feature sono:\n",
    "\n",
    "- ``image_url``: l'URL del *frame* che rappresenta l'episodio\n",
    "- ``imdb_rating``: il *rating* che l'episodio ha su IMDB\n",
    "- ``imdb_votes``: il numero di voti che hanno contribuito al *rating* su IMDB\n",
    "- ``number_in_season``: il numero dell'episodio all'interno della stagione\n",
    "- ``number_in_series``: il numero dell'episodio all'interno dell'intera serie\n",
    "- ``original_air_date``: la data della prima trasmissione dell'episodio\n",
    "- ``original_air_year``: l'anno della prima trasmissione dell'episodio\n",
    "- ``production_code``: il codice di produzione dell'episodio\n",
    "- ``season``: il numero della stagione dell'episodio\n",
    "- ``title``: il titolo dell'episodio\n",
    "- ``us_viewers_in_millions``: il numero di spettatori negli Stati Uniti per quell'episodio in milioni\n",
    "- ``video_url``: l'URL per vedere l'episodio sul sito del canale televisivo \"Fox\"\n",
    "- ``views``: il numero di visualizzazioni dell'episodio sul sito del canale televisivo \"Fox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>number_in_season</th>\n",
       "      <th>number_in_series</th>\n",
       "      <th>original_air_date</th>\n",
       "      <th>original_air_year</th>\n",
       "      <th>production_code</th>\n",
       "      <th>season</th>\n",
       "      <th>title</th>\n",
       "      <th>us_viewers_in_millions</th>\n",
       "      <th>video_url</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/617/479/Simpsons_01_08.jpg</td>\n",
       "      <td>8.2</td>\n",
       "      <td>3734.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1989-12-17</td>\n",
       "      <td>1989</td>\n",
       "      <td>7G08</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>26.7</td>\n",
       "      <td>http://www.simpsonsworld.com/video/273376835817</td>\n",
       "      <td>171408.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/265/167/Simpsons_01_02.jpg</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1990-01-14</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G02</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>24.5</td>\n",
       "      <td>http://www.simpsonsworld.com/video/283744835990</td>\n",
       "      <td>91423.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/621/883/Simpsons_01_03.jpg</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1990-01-21</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G03</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer's Odyssey</td>\n",
       "      <td>27.5</td>\n",
       "      <td>http://www.simpsonsworld.com/video/273381443699</td>\n",
       "      <td>78072.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/632/119/Simpsons_01_04__343617.jpg</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1701.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1990-01-28</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G04</td>\n",
       "      <td>1</td>\n",
       "      <td>There's No Disgrace Like Home</td>\n",
       "      <td>20.2</td>\n",
       "      <td>http://www.simpsonsworld.com/video/273392195780</td>\n",
       "      <td>67378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://static-media.fxx.com/img/FX_Networks_-_FXX/274/735/Simpsons_01_05.jpg</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1732.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1990-02-04</td>\n",
       "      <td>1990</td>\n",
       "      <td>7G05</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart the General</td>\n",
       "      <td>27.1</td>\n",
       "      <td>http://www.simpsonsworld.com/video/300934723994</td>\n",
       "      <td>63129.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               image_url  \\\n",
       "id                                                                                         \n",
       "1           http://static-media.fxx.com/img/FX_Networks_-_FXX/617/479/Simpsons_01_08.jpg   \n",
       "2           http://static-media.fxx.com/img/FX_Networks_-_FXX/265/167/Simpsons_01_02.jpg   \n",
       "3           http://static-media.fxx.com/img/FX_Networks_-_FXX/621/883/Simpsons_01_03.jpg   \n",
       "4   http://static-media.fxx.com/img/FX_Networks_-_FXX/632/119/Simpsons_01_04__343617.jpg   \n",
       "5           http://static-media.fxx.com/img/FX_Networks_-_FXX/274/735/Simpsons_01_05.jpg   \n",
       "\n",
       "    imdb_rating  imdb_votes  number_in_season  number_in_series  \\\n",
       "id                                                                \n",
       "1           8.2      3734.0                 1                 1   \n",
       "2           7.8      1973.0                 2                 2   \n",
       "3           7.5      1709.0                 3                 3   \n",
       "4           7.8      1701.0                 4                 4   \n",
       "5           8.1      1732.0                 5                 5   \n",
       "\n",
       "   original_air_date  original_air_year production_code  season  \\\n",
       "id                                                                \n",
       "1         1989-12-17               1989            7G08       1   \n",
       "2         1990-01-14               1990            7G02       1   \n",
       "3         1990-01-21               1990            7G03       1   \n",
       "4         1990-01-28               1990            7G04       1   \n",
       "5         1990-02-04               1990            7G05       1   \n",
       "\n",
       "                                title  us_viewers_in_millions  \\\n",
       "id                                                              \n",
       "1   Simpsons Roasting on an Open Fire                    26.7   \n",
       "2                     Bart the Genius                    24.5   \n",
       "3                     Homer's Odyssey                    27.5   \n",
       "4       There's No Disgrace Like Home                    20.2   \n",
       "5                    Bart the General                    27.1   \n",
       "\n",
       "                                          video_url     views  \n",
       "id                                                             \n",
       "1   http://www.simpsonsworld.com/video/273376835817  171408.0  \n",
       "2   http://www.simpsonsworld.com/video/283744835990   91423.0  \n",
       "3   http://www.simpsonsworld.com/video/273381443699   78072.0  \n",
       "4   http://www.simpsonsworld.com/video/273392195780   67378.0  \n",
       "5   http://www.simpsonsworld.com/video/300934723994   63129.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes = pd.read_csv(\"simpsons_episodes.csv\", index_col=\"id\").sort_values(\"id\")\n",
    "simpsons_episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colonne come ``image_url`` e ``video_url`` non sono di alcuna utilità, così come gli identificatori alternativi per l'episodio - ovvero ``number_in_season``, ``number_in_series``, ``production_code`` - essendoci già un identificatore. Le feature ``imdb_votes``, ``us_viewers_in_millions`` e ``views`` non sono note in anticipo per come abbiamo descritto il problema e ``original_air_year`` è una feature duplicata. Per tutte queste ragioni, le colonne descritte verranno eliminate dal DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>original_air_date</th>\n",
       "      <th>season</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.2</td>\n",
       "      <td>1989-12-17</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>1990-01-14</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart the Genius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.5</td>\n",
       "      <td>1990-01-21</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer's Odyssey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.8</td>\n",
       "      <td>1990-01-28</td>\n",
       "      <td>1</td>\n",
       "      <td>There's No Disgrace Like Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>1990-02-04</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart the General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    imdb_rating original_air_date  season                              title\n",
       "id                                                                          \n",
       "1           8.2        1989-12-17       1  Simpsons Roasting on an Open Fire\n",
       "2           7.8        1990-01-14       1                    Bart the Genius\n",
       "3           7.5        1990-01-21       1                    Homer's Odyssey\n",
       "4           7.8        1990-01-28       1      There's No Disgrace Like Home\n",
       "5           8.1        1990-02-04       1                   Bart the General"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes.drop(columns=simpsons_episodes.columns.difference([\"imdb_rating\", \"original_air_date\", \"season\", \"title\"]), inplace=True)\n",
    "simpsons_episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso l'uso di memoria è insignificante, ma per poter essere compatibile con i precedenti dataset dobbiamo trasformarlo utilizzando i tipi di dato adeguati, che inevitabilmente ci fa risparmiare non poca memoria, ben più dei tre quarti dell'originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 600 entries, 1 to 600\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   imdb_rating        597 non-null    float64\n",
      " 1   original_air_date  600 non-null    object \n",
      " 2   season             600 non-null    int64  \n",
      " 3   title              600 non-null    object \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 99.0 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_episodes.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 600 entries, 1 to 600\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   imdb_rating        597 non-null    float64       \n",
      " 1   original_air_date  600 non-null    datetime64[ns]\n",
      " 2   season             600 non-null    int64         \n",
      " 3   title              600 non-null    string        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), string(1)\n",
      "memory usage: 23.4 KB\n"
     ]
    }
   ],
   "source": [
    "simpsons_episodes = simpsons_episodes.astype({\n",
    "    \"title\": \"string\"\n",
    "})\n",
    "simpsons_episodes[\"original_air_date\"] = pd.to_datetime(simpsons_episodes[\"original_air_date\"])\n",
    "simpsons_episodes.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nell'ultimo dataset sono invece contenute tutte le battute dei copioni dei vari episodi. Oltre alla colonna ``id`` che fa da identificatore, già vista nei precedenti DataFrame, troviamo:\n",
    "- ``episode_id``: l'identificatore dell'episodio, utile per effettuare join con il DataFrame che contiene gli episodi\n",
    "- ``number``: il numero della battuta all'interno del copione dell'episodio\n",
    "- ``raw_text``: il testo della battuta così com'è stato trovato nel copione\n",
    "- ``timestamp_in_ms``: il *timestamp* dell'istante in cui la battuta viene pronunciata all'interno dell'episodio\n",
    "- ``speaking_line``: se la battuta viene pronunciata da qualcuno o semplicemente sono appunti di sceneggiatura\n",
    "- ``character_id``: l'identificatore del personaggio che pronuncia la battuta\n",
    "- ``location_id``: l'identificatore della location in cui viene pronunciata la battuta\n",
    "- ``raw_character_text``: il nome del personaggio che pronuncia la battuta\n",
    "- ``raw_location_text``: il nome della location in cui la battuta viene pronunciata\n",
    "- ``spoken_words``: il testo della battuta con rimossi i suggerimenti di interpretazione e il nome di chi la pronuncia\n",
    "- ``normalized_text``: come ``spoken_words`` ma sono stati rimossi tutti i caratteri non alfabetici e quelli rimasti sono stati convertiti in minuscolo\n",
    "- ``word_count``: il conteggio delle parole pronunciate nella battuta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>(Springfield Elementary School: Ext. springfield elementary school - establishing - night)</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>(Auditorium: int. auditorium - night)</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Marge Simpson: (HUSHED VOICE) Sorry, Excuse us. Pardon me...</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>Sorry, Excuse us. Pardon me...</td>\n",
       "      <td>sorry excuse us pardon me</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Homer Simpson: (SIMULTANEOUSLY) Hey, Norman. How's it going? So you got dragged down here, too.....</td>\n",
       "      <td>26000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? ...</td>\n",
       "      <td>hey norman hows it going so you got dragged down here too heh heh how ya doing fred excuse me fred</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Homer Simpson: Pardon my galoshes. (CHUCKLES)</td>\n",
       "      <td>34000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Auditorium</td>\n",
       "      <td>Pardon my galoshes.</td>\n",
       "      <td>pardon my galoshes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number  \\\n",
       "id                       \n",
       "6            1       5   \n",
       "7            1       6   \n",
       "8            1       7   \n",
       "9            1       8   \n",
       "10           1       9   \n",
       "\n",
       "                                                                                               raw_text  \\\n",
       "id                                                                                                        \n",
       "6            (Springfield Elementary School: Ext. springfield elementary school - establishing - night)   \n",
       "7                                                                 (Auditorium: int. auditorium - night)   \n",
       "8                                          Marge Simpson: (HUSHED VOICE) Sorry, Excuse us. Pardon me...   \n",
       "9   Homer Simpson: (SIMULTANEOUSLY) Hey, Norman. How's it going? So you got dragged down here, too.....   \n",
       "10                                                        Homer Simpson: Pardon my galoshes. (CHUCKLES)   \n",
       "\n",
       "    timestamp_in_ms  speaking_line  character_id  location_id  \\\n",
       "id                                                              \n",
       "6           24000.0          False           NaN          3.0   \n",
       "7           24000.0          False           NaN          4.0   \n",
       "8           24000.0           True           1.0          4.0   \n",
       "9           26000.0           True           2.0          4.0   \n",
       "10          34000.0           True           2.0          4.0   \n",
       "\n",
       "   raw_character_text              raw_location_text  \\\n",
       "id                                                     \n",
       "6                 NaN  Springfield Elementary School   \n",
       "7                 NaN                     Auditorium   \n",
       "8       Marge Simpson                     Auditorium   \n",
       "9       Homer Simpson                     Auditorium   \n",
       "10      Homer Simpson                     Auditorium   \n",
       "\n",
       "                                                                                           spoken_words  \\\n",
       "id                                                                                                        \n",
       "6                                                                                                   NaN   \n",
       "7                                                                                                   NaN   \n",
       "8                                                                        Sorry, Excuse us. Pardon me...   \n",
       "9   Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? ...   \n",
       "10                                                                                  Pardon my galoshes.   \n",
       "\n",
       "                                                                                       normalized_text  \\\n",
       "id                                                                                                       \n",
       "6                                                                                                  NaN   \n",
       "7                                                                                                  NaN   \n",
       "8                                                                            sorry excuse us pardon me   \n",
       "9   hey norman hows it going so you got dragged down here too heh heh how ya doing fred excuse me fred   \n",
       "10                                                                                  pardon my galoshes   \n",
       "\n",
       "    word_count  \n",
       "id              \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          5.0  \n",
       "9         21.0  \n",
       "10         3.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines = pd.read_csv(\"simpsons_script_lines.csv\", index_col=\"id\").sort_values(\"id\")\n",
    "simpsons_script_lines.iloc[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La colonna ``raw_text``, benchè possa sembrare utile, in realtà può essere eliminata, perchè sono le colonne successive a mantenere i suoi dati utili in una forma più leggibile. L'unico caso in cui sembrerebbero perdersi delle informazioni si ha quando la battuta non è pronunciata ed indica un cambio di location o lo specifico momento del giorno in cui si svolge la scena. Il luogo che può contenere però è presente nelle feature ``location_id`` e ``raw_location_text`` delle battute seguenti, mentre il momento del giorno non è un'informazione particolarmente significativa e possiamo anche rinunciarvi. Inoltre, ``spoken_words`` è migliore di ``raw_text`` anche perchè non presenta i suggerimenti di interpretazione, che non servono per far capire che cosa sta succedendo durante quella battuta, e perchè chi pronuncia la battuta è già indicato dalle colonne ``character_id`` e ``raw_character_text``. Tratterremo ``raw_text`` solo per il momento perchè ci servirà per l'elaborazione dei dati succssiva, ma poi ce ne sbarazzeremo. La feature ``speaking_line`` non è necessaria da trattenere poichè tutte le righe che hanno la colonna ``spoken_words`` diversa da NA hanno anche ``speaking_line`` posto a True, rendendolo un attributo derivato al pari di ``normalized_text``, ``word_count``, ``raw_character_text`` e ``raw_location_text``. Possiamo perciò eliminare tutte queste colonne tranquillamente. La feature ``timestamp_in_ms`` non ci dà alcuna informazione utile e perciò rimuoviamo anche questa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines[(simpsons_script_lines[\"speaking_line\"] == True) & (simpsons_script_lines[\"spoken_words\"].isna())].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>We're late.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number                                       raw_text  \\\n",
       "id                                                                      \n",
       "1            1       0   (Street: ext. street - establishing - night)   \n",
       "2            1       1                        (Car: int. car - night)   \n",
       "3            1       2            Marge Simpson: Ooo, careful, Homer.   \n",
       "4            1       3  Homer Simpson: There's no time to be careful.   \n",
       "5            1       4                     Homer Simpson: We're late.   \n",
       "\n",
       "    character_id  location_id                    spoken_words  \n",
       "id                                                             \n",
       "1            NaN          1.0                             NaN  \n",
       "2            NaN          2.0                             NaN  \n",
       "3            1.0          2.0            Ooo, careful, Homer.  \n",
       "4            2.0          2.0  There's no time to be careful.  \n",
       "5            2.0          2.0                     We're late.  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines.drop(columns=simpsons_script_lines.columns.difference([\"episode_id\", \"number\", \"raw_text\", \"character_id\", \"location_id\", \"spoken_words\"]), inplace=True)\n",
    "simpsons_script_lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per quest'ultimo DataFrame l'uso della memoria è importantissimo, dato che occupa circa 40 MiB. Anche in questo caso ci affidiamo all'uso dei tipi di dato di Pandas per migliorare l'uso della memoria, che però viene ridotto solo di 13 MiB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158301 entries, 1 to 158315\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   episode_id    158301 non-null  int64  \n",
      " 1   number        158301 non-null  int64  \n",
      " 2   raw_text      158301 non-null  object \n",
      " 3   character_id  140760 non-null  float64\n",
      " 4   location_id   157878 non-null  float64\n",
      " 5   spoken_words  132139 non-null  object \n",
      "dtypes: float64(2), int64(2), object(2)\n",
      "memory usage: 39.7 MB\n"
     ]
    }
   ],
   "source": [
    "simpsons_script_lines.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>We're late.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number                                       raw_text  \\\n",
       "id                                                                      \n",
       "1            1       0   (Street: ext. street - establishing - night)   \n",
       "2            1       1                        (Car: int. car - night)   \n",
       "3            1       2            Marge Simpson: Ooo, careful, Homer.   \n",
       "4            1       3  Homer Simpson: There's no time to be careful.   \n",
       "5            1       4                     Homer Simpson: We're late.   \n",
       "\n",
       "    character_id  location_id                    spoken_words  \n",
       "id                                                             \n",
       "1           <NA>            1                            <NA>  \n",
       "2           <NA>            2                            <NA>  \n",
       "3              1            2            Ooo, careful, Homer.  \n",
       "4              2            2  There's no time to be careful.  \n",
       "5              2            2                     We're late.  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines = simpsons_script_lines.astype({\n",
    "    \"character_id\": pd.Int64Dtype(),\n",
    "    \"location_id\": pd.Int64Dtype(),\n",
    "    \"spoken_words\": \"string\"\n",
    "})\n",
    "simpsons_script_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 158301 entries, 1 to 158315\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   episode_id    158301 non-null  int64 \n",
      " 1   number        158301 non-null  int64 \n",
      " 2   raw_text      158301 non-null  object\n",
      " 3   character_id  140760 non-null  Int64 \n",
      " 4   location_id   157878 non-null  Int64 \n",
      " 5   spoken_words  132139 non-null  string\n",
      "dtypes: Int64(2), int64(2), object(1), string(1)\n",
      "memory usage: 26.5 MB\n"
     ]
    }
   ],
   "source": [
    "simpsons_script_lines.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli episodi contenuti nel dataset sono 600, ma solo i primi 597 hanno un rating su IMDB. Questo è segno del fatto che quando questi dati sono stati raccolti quegli episodi erano ancora molto recenti e non c'erano sufficienti valutazioni per estrarre un rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>597.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.386097</td>\n",
       "      <td>14.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.732439</td>\n",
       "      <td>7.755444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.900000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       imdb_rating      season\n",
       "count   597.000000  600.000000\n",
       "mean      7.386097   14.100000\n",
       "std       0.732439    7.755444\n",
       "min       4.500000    1.000000\n",
       "25%       6.900000    7.000000\n",
       "50%       7.300000   14.000000\n",
       "75%       8.000000   21.000000\n",
       "max       9.200000   28.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes[simpsons_episodes[\"imdb_rating\"].isna()].equals(simpsons_episodes.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sui 597 episodi, in media le recensioni li hanno considerati nella media, cioè tra il \"bello\" e il \"brutto\", con un valore però che tende maggiormente verso il \"brutto\", usando la terminologia delle classi che abbiamo definito all'inizio. Questo vuol dire che il valore di separazione tra i *rating* inclusi nelle due classi è corretto, nell'ottica di avere due classi all'incirca bilanciate. La mediana non si discosta molto dal valore medio, segno che le recensioni sono state molto equilibrate, senza troppi giudizi estremi, come ci conferma anche il grafico \"boxplot\". Esistono però due episodi, considerati i peggiori della serie, che sono *outliers* rispetto alla distribuzione di tutti i voti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAMAUlEQVR4nO3dXYxcdRnH8d/PbbEWQSldXzDWakJwZIKIIxGtmBU1QQ3GlygkXmAmVhOyiNHEl7kAL3ph9AYbta5WEo3MhShooiJeTExGATOtBRdXowJVRGGJK+/VsX282Nl2qdvu2XbOzjOz30+yYXvOmTPPNvDN4T8zexwRAgDk9axBDwAAOD5CDQDJEWoASI5QA0ByhBoAkltXxkk3b94cW7duLePUADCS9uzZ80hEjC+1r5RQb926VZ1Op4xTA8BIsr3/WPtY+gCA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCSI9QAkFwpH3gBVoPtVXsufm87BolQY2idSDxtE10MHZY+ACA5Qg0AyRFqAEiuUKhtf9z2tO17bF9T9lAAgCOWDbXtqqSPSLpQ0qslvcv22WUPBgCYV+SKuiLpjoh4KiL+K+kXkt5T7lgAgAVFQj0t6WLbZ9reKOkdkl5a7lgAgAXLvo86ImZsf0HSzyU9IekuSf89+jjb2yVtl6QtW7b0eUwAWLsKvZgYEbsj4oKIuFjSPyX9cYljpiKiFhG18fElb/sFADgBhT6ZaPsFEfGw7S2S3ivponLHAgAsKPoR8u/bPlNSV9JVETFX4kwAgEUKhToi3lT2IACApfHJRABIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoVCbfsTtu+xPW27aXtD2YMBAOYtG2rbL5F0taRaRFQljUm6vOzBAADzii59rJP0HNvrJG2U9GB5IwEAFls21BHxN0lfkvQXSX+X9GhE3Hb0cba32+7Y7szOzvZ/Uoy8TZs2yXapX5JKf45NmzYN+G8So6bI0scZkt4t6eWSzpJ0qu0PHX1cRExFRC0iauPj4/2fFCNvbm5OETH0X3Nzc4P+q8SIKbL08VZJ90XEbER0Jf1A0hvKHQsAsKBIqP8i6fW2N3r+/x0vkTRT7lgAgAVF1qjvlHSTpL2Sftt7zFTJcwEAetYVOSgirpV0bcmzAACWwCcTASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEgOUKNNWP2qVldeeuVeuTpRwY9CrAihBprxq67d2nvQ3u1665dgx4FWBFCjTVh9qlZ/fBPP1QodMufbuGqGkOFUGNN2HX3Lh2KQ5KkQ3GIq2oMFUKNkbdwNd091JUkdQ91uarGUCHUGHmLr6YXcFWNYbJu0AMAC+La06Xrntf389511ovUffYpz9jWPdTVvru/I936xb4/X1x7et/PibWNUCMNf/4xRUTfz3tT3894fLYV163yk2KksfQBAMkRagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASC5ZUNt+xzb+xZ9PWb7mtUYDgBQ4JOJEfEHSedLku0xSX+TdHPJcwEAela69HGJpD9HxP4yhgEA/L+VhvpySc2ldtjebrtjuzM7O3vykwEAJK0g1LZPkXSZpO8ttT8ipiKiFhG18fHxfs0HAGveSq6oL5W0NyIeKmsYAMD/W0mor9Axlj0AAOUp9PuobW+U9DZJHy13HKx1tgc9wkk744wzBj0CRkyhUEfEU5LOLHkWrHFl3DTgaLZX5XmAfuKTiQCQHKEGgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJBcoVDbfr7tm2z/3vaM7YvKHgwAMG9dweOul3RrRLzf9imSNpY4EwBgkWVDbft0SRdLulKSIuI/kv5T7lgAgAVFlj5eIWlW0g22f2P7m7ZPPfog29ttd2x3Zmdn+z4oAKxVRUK9TtIFkr4WEa+R9KSkzxx9UERMRUQtImrj4+N9HhMA1q4ioX5A0gMRcWfvzzdpPtwAgFWwbKgj4h+S/mr7nN6mSyT9rtSpAACHFX3Xx6Sk7/be8XGvpA+XNxIAYLFCoY6IfZJqJc8CAFgCn0wEgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEguaK/lAlIx/aqPS4iTui5gH4g1BhaxBNrBUsfAJAcoQaA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoQaAJIj1FgTms2mqtWqxsbGVK1W1Ww2Bz0SUBh3eMHIazabajQa2r17t7Zt26Z2u616vS5JuuKKKwY8HbA8l3E7o1qtFp1Op+/nBU5EtVrVzp07NTExcXhbq9XS5OSkpqenBzgZcITtPRFRW3IfocaoGxsb04EDB7R+/frD27rdrjZs2KCDBw8OcDLgiOOFutAate37bf/W9j7bFBhDpVKpqN1uP2Nbu91WpVIZ0ETAyqzkxcSJiDj/WMUHsmo0GqrX62q1Wup2u2q1WqrX62o0GoMeDSiEFxMx8hZeMJycnNTMzIwqlYp27NjBC4kYGoXWqG3fJ2lOUkj6ekRMLXHMdknbJWnLli2v3b9/f59HBYDRddJr1JLeGBEXSLpU0lW2Lz76gIiYiohaRNTGx8dPYlwAwGKFQh0RD/b++bCkmyVdWOZQAIAjlg217VNtn7bwvaS3S+LNpwCwSoq8mPhCSTfbXjj+xoi4tdSpAACHLRvqiLhX0qtXYRYAwBL4pUwAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBIDlCjTWh2WyqWq1qbGxM1WpVzWZz0CMBhXHPRIy8ZrOpRqOh3bt3a9u2bWq326rX65LEfRMxFArdM3GlarVadDqdvp8XOBHValU7d+7UxMTE4W2tVkuTk5OanuYeGMjhePdMJNQYeWNjYzpw4IDWr19/eFu329WGDRt08ODBAU4GHNGPm9sCQ6tSqajdbj9jW7vdVqVSGdBEwMoQaoy8RqOher2uVqulbrerVquler2uRqMx6NGAQngxESNv4QXDyclJzczMqFKpaMeOHbyQiKHBGjUAJMAaNQAMMUINAMkRagBIjlADQHKEGgCSK+VdH7ZnJe3v+4mBk7dZ0iODHgJYwssiYnypHaWEGsjKdudYb4ECsmLpAwCSI9QAkByhxlozNegBgJVijRoAkuOKGgCSI9QAkByhxsixfY3tjYv+/BPbzx/kTMDJYI0aQ8m2Nf/v76El9t0vqRYRfLAFI4EragwN21ttz9j+qqS9knbb7ti+x/bne8dcLeksSS3brd62+21vXvT4b/Qec5vt5/SOeZ3tu23fbvuLtqd728+1/Wvb+3r7zx7MT4+1jFBj2Jwj6dsR8RpJn+x9yvA8SW+2fV5EfFnSg5ImImJiicefLekrEXGupH9Jel9v+w2SPhYRF0lafMfbj0m6PiLOl1ST9EApPxVwHIQaw2Z/RNzR+/4DtvdK+o2kcyW9qsDj74uIfb3v90ja2lu/Pi0iftXbfuOi42+X9Dnbn9b872J4+uR/BGBlCDWGzZOSZPvlkj4l6ZKIOE/SjyVtKPD4fy/6/qDm7xvqYx0cETdKukzS05J+ZvstJzg3cMIINYbV6ZqP9qO2Xyjp0kX7Hpd0WtETRcScpMdtv7636fKFfbZfIene3pLKjzS/zAKsKkKNoRQRd2l+yeMeSd+S9MtFu6ck/XThxcSC6pKmbN+u+SvsR3vbPyhp2vY+Sa+U9O2TnR1YKd6eB0iy/dyIeKL3/WckvTgiPj7gsQBJ8+tzAKR32v6s5v+b2C/pysGOAxzBFTUAJMcaNQAkR6gBIDlCDQDJEWoASI5QA0By/wPzRnaz2mD/DwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(simpsons_episodes[\"imdb_rating\"].dropna(), showmeans=True, labels=[\"ratings\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>imdb_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Lisa Goes Gaga</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>All Singing, All Dancing</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        title  imdb_rating\n",
       "id                                        \n",
       "508            Lisa Goes Gaga          4.5\n",
       "189  All Singing, All Dancing          5.1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes.sort_values(\"imdb_rating\").head(2)[[\"title\", \"imdb_rating\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osservando l'istogramma, si nota come la distribuzione dei valori si avvicini sufficientemente a quella di una gaussiana, fatto testimoniato anche dalla deviazione standard che ha un valore prossimo a 1 (0,73). Possiamo perciò considerare le recensioni come sufficientemente ben distribuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes[\"imdb_rating\"].plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes[\"imdb_rating\"].plot.density();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset considera gli episodi de \"I Simpson\" delle prime 28 stagioni, e nell'ultima non tiene in considerazione tutti gli episodi, ma solamente i primi quattro. È facile osservare come la [ventottesima stagione](https://it.wikipedia.org/wiki/Episodi_de_I_Simpson_(ventottesima_stagione)) abbia infatti ben più di quattro episodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes[\"season\"].value_counts().sort_index().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni nuovo episodio è andato in onda esclusivamente di domenica, fatta eccezione per un centinaio di episodi, andati in onda di giovedì, e di un numero insignificante alcuni altri nuovi episodi che sono andati in onda nei restanti giorni della settimana. Questo ci dice che questa possibile feature ha scarsa variabilità e non è utile ternerne conto durante la classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes.original_air_date.dt.day_name().value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I nuovi episodi sono andati in onda durante tutto l'anno con all'incirca sempre la stessa frequenza, ad eccezione del periodo esitvo. Questi dati non ci sorprendono però, dato che la stagione televisiva si svolge da settembre a maggio. Quindi anche questo dato non è particolarmente significativo per la classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes.original_air_date.dt.month.value_counts().sort_index().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un'altra particolarità che notiamo è un fatto di cui tutti gli appassionati della serie sono convinti: le prime stagioni sono considerate mediamente più belle di quelle successive. Dal grafico a barre si nota infatti che per le prime 10 stagioni il punteggio medio è pari o superiore a 7,5 (cioè in media gli episodi sono considerati \"belli\"), mentre per le stagioni successive la media dei punteggi scende decisamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simpsons_episodes.groupby(\"season\").mean().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediamente, nel copione di ogni episodio vengono inserite 281 battute di cui 234 vengono pronunciate da un personaggio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_text        280.675532\n",
       "spoken_words    234.289007\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines.groupby(\"episode_id\")[[\"raw_text\", \"spoken_words\"]].count().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può osservare, il numero di battute tra tutti i copioni che vengono pronunciate è inferiore di 26.000 istanze a quello delle battute in generale nei copioni. Questo per noi non rappresenta un problema, il calo non è significativo, ciò che invece potrebbe rappresentare un problema sono quelle battute che vengono pronunciate dove però non è dato il personaggio che le pronuncia, visto che non può essere sconosciuto. Discorso diverso vale per i luoghi, infatti è possibile che la battuta sia pronunciata in un luogo che nel copione si ignora, perchè magari non viene mostrato. Si nota infatti che le battute pronunciate senza il corrispettivo personaggio sono pochissime, segno di una mancanza di dati dovuta a degli errori, mentre invece le battute senza un luogo specificato sono molte di più, anche se sempre molto poche in confronto a quelle dell'intero *dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_text        158301\n",
       "spoken_words    132139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines[[\"raw_text\", \"spoken_words\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4213</th>\n",
       "      <td>14</td>\n",
       "      <td>205</td>\n",
       "      <td>Martin Prince: Who would have thought that pushing a boy into the girls' lavatory could be such ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Who would have thought that pushing a boy into the girls' lavatory could be such a thrill? The s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>14</td>\n",
       "      <td>254</td>\n",
       "      <td>Entire Town: A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND.\" /</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND. /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>14</td>\n",
       "      <td>258</td>\n",
       "      <td>Bart Simpson: (READING ALOUD, WITH HEART) Chapter Six: Four Days in Philadelphia. The first Cont...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Chapter Six: Four Days in Philadelphia. The first Continental Congress faced a difficult job. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>18</td>\n",
       "      <td>200</td>\n",
       "      <td>Tony Bennett: CALLED CAPITAL CITY...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CALLED CAPITAL CITY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5349</th>\n",
       "      <td>18</td>\n",
       "      <td>203</td>\n",
       "      <td>Tony Bennett: IN CAPITAL CITY...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>IN CAPITAL CITY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_id  number  \\\n",
       "id                         \n",
       "4213          14     205   \n",
       "4262          14     254   \n",
       "4266          14     258   \n",
       "5346          18     200   \n",
       "5349          18     203   \n",
       "\n",
       "                                                                                                 raw_text  \\\n",
       "id                                                                                                          \n",
       "4213  Martin Prince: Who would have thought that pushing a boy into the girls' lavatory could be such ...   \n",
       "4262             Entire Town: A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND.\" /   \n",
       "4266  Bart Simpson: (READING ALOUD, WITH HEART) Chapter Six: Four Days in Philadelphia. The first Cont...   \n",
       "5346                                                                 Tony Bennett: CALLED CAPITAL CITY...   \n",
       "5349                                                                     Tony Bennett: IN CAPITAL CITY...   \n",
       "\n",
       "      character_id  location_id  \\\n",
       "id                                \n",
       "4213          <NA>         <NA>   \n",
       "4262          <NA>         <NA>   \n",
       "4266          <NA>         <NA>   \n",
       "5346          <NA>         <NA>   \n",
       "5349          <NA>         <NA>   \n",
       "\n",
       "                                                                                             spoken_words  \n",
       "id                                                                                                         \n",
       "4213  Who would have thought that pushing a boy into the girls' lavatory could be such a thrill? The s...  \n",
       "4262                           A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND. /  \n",
       "4266  Chapter Six: Four Days in Philadelphia. The first Continental Congress faced a difficult job. Co...  \n",
       "5346                                                                               CALLED CAPITAL CITY...  \n",
       "5349                                                                                   IN CAPITAL CITY...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines[~simpsons_script_lines[\"spoken_words\"].isna() & simpsons_script_lines[\"character_id\"].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4213</th>\n",
       "      <td>14</td>\n",
       "      <td>205</td>\n",
       "      <td>Martin Prince: Who would have thought that pushing a boy into the girls' lavatory could be such ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Who would have thought that pushing a boy into the girls' lavatory could be such a thrill? The s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>14</td>\n",
       "      <td>254</td>\n",
       "      <td>Entire Town: A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND.\" /</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND. /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>14</td>\n",
       "      <td>258</td>\n",
       "      <td>Bart Simpson: (READING ALOUD, WITH HEART) Chapter Six: Four Days in Philadelphia. The first Cont...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Chapter Six: Four Days in Philadelphia. The first Continental Congress faced a difficult job. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>Marge Simpson: (CLEARING HER THROAT) Hello, everyone. (CLEARING HER THROAT) You know, Halloween ...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Hello, everyone. You know, Halloween is a very strange holiday. Personally, I don't understand i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>18</td>\n",
       "      <td>200</td>\n",
       "      <td>Tony Bennett: CALLED CAPITAL CITY...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CALLED CAPITAL CITY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_id  number  \\\n",
       "id                         \n",
       "4213          14     205   \n",
       "4262          14     254   \n",
       "4266          14     258   \n",
       "4588          16       0   \n",
       "5346          18     200   \n",
       "\n",
       "                                                                                                 raw_text  \\\n",
       "id                                                                                                          \n",
       "4213  Martin Prince: Who would have thought that pushing a boy into the girls' lavatory could be such ...   \n",
       "4262             Entire Town: A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND.\" /   \n",
       "4266  Bart Simpson: (READING ALOUD, WITH HEART) Chapter Six: Four Days in Philadelphia. The first Cont...   \n",
       "4588  Marge Simpson: (CLEARING HER THROAT) Hello, everyone. (CLEARING HER THROAT) You know, Halloween ...   \n",
       "5346                                                                 Tony Bennett: CALLED CAPITAL CITY...   \n",
       "\n",
       "      character_id  location_id  \\\n",
       "id                                \n",
       "4213          <NA>         <NA>   \n",
       "4262          <NA>         <NA>   \n",
       "4266          <NA>         <NA>   \n",
       "4588             1         <NA>   \n",
       "5346          <NA>         <NA>   \n",
       "\n",
       "                                                                                             spoken_words  \n",
       "id                                                                                                         \n",
       "4213  Who would have thought that pushing a boy into the girls' lavatory could be such a thrill? The s...  \n",
       "4262                           A BEAUTIFUL SIGHT / WE'RE HAPPY TONIGHT/ WALKIN' IN A WINTER WONDERLAND. /  \n",
       "4266  Chapter Six: Four Days in Philadelphia. The first Continental Congress faced a difficult job. Co...  \n",
       "4588  Hello, everyone. You know, Halloween is a very strange holiday. Personally, I don't understand i...  \n",
       "5346                                                                               CALLED CAPITAL CITY...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines[~simpsons_script_lines[\"spoken_words\"].isna() & simpsons_script_lines[\"location_id\"].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È possibile osservare che in media all'interno di ciascun episodio della serie della serie i personaggi che compaiono hanno 7 battute a testa, anche se questo dato è fortemente sbilanciato: ad esempio la metà dei personaggi a livello di singolo episodio ha una o due battute al massimo e ci sono invece personaggi che in un singolo episodio pronunciano fino a 131 battute. Questo significa che pochi personaggi sono veramente rilevanti nella serie tv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19995.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.607702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.653093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>131.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               size\n",
       "count  19995.000000\n",
       "mean       6.607702\n",
       "std       12.653093\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        2.000000\n",
       "75%        5.000000\n",
       "max      131.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.DataFrame(simpsons_script_lines.dropna(axis=\"index\", subset=[\"spoken_words\"]) \\\n",
    "                                          .groupby([\"episode_id\", \"character_id\"]) \\\n",
    "                                          .size(),\n",
    "                     columns=[\"size\"])\n",
    "lines.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lines.plot.hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discorso analogo si può fare per le location. In media in ciascuna location a livello di singolo episodio si pronunciano 13 battute nel copione, però un quarto delle location non compare mai più di tre volte ed esistono location che compaiono fino a 203 volte nelle battute del copione. Quindi, poche location nella serie sono veramente importanti per caratterizzare un episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10041.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.120805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.007592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>203.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               size\n",
       "count  10041.000000\n",
       "mean      13.120805\n",
       "std       18.007592\n",
       "min        1.000000\n",
       "25%        3.000000\n",
       "50%        7.000000\n",
       "75%       15.000000\n",
       "max      203.000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = pd.DataFrame(simpsons_script_lines.dropna(axis=\"index\", subset=[\"spoken_words\"]) \\\n",
    "                                              .groupby([\"episode_id\", \"location_id\"]) \\\n",
    "                                              .size(), columns=[\"size\"])\n",
    "locations.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "locations.plot.hist(bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rimozione dei valori mancanti e preparazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sulla base di quanto detto, possiamo rimuovere ``original_air_date`` e ``season`` dal DataFrame degli episodi perchè le feature che potremmo estrarre hanno una scarsa variabilità o comunque presentano delle tendenze che vorremmo non influenzassero la nostra classificazione. Inoltre, possiamo rimuovere ``title`` perchè non ci serve più."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    imdb_rating\n",
       "id             \n",
       "1           8.2\n",
       "2           7.8\n",
       "3           7.5\n",
       "4           7.8\n",
       "5           8.1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_episodes.drop(columns=[\"original_air_date\", \"title\", \"season\"], inplace=True)\n",
    "simpsons_episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto, cerchiamo di sostituire i valori NA di ``character_id`` con quelli corretti quando anche ``spoken_words`` è diverso da NA utilizzando le peculiarità di ``raw_text``. Infatti, generalmente le battute pronunciate nella colonna ``raw_text`` sono precedute dal nome di chi le pronuncia, seguito dai due punti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = simpsons_script_lines[(~simpsons_script_lines[\"spoken_words\"].isna()) & (simpsons_script_lines[\"character_id\"].isna())]\n",
    "for index, row in missing.iterrows():\n",
    "    name = row[\"raw_text\"].split(\":\")[0]\n",
    "    if name != \"\":\n",
    "        simpsons_script_lines.loc[index, \"character_id\"] = simpsons_characters[simpsons_characters[\"name\"] == name].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatto questo, ``raw_text`` non ci serve più e può essere eliminato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>We're late.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Sorry, Excuse us. Pardon me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number  character_id  location_id  \\\n",
       "id                                                  \n",
       "3            1       2             1            2   \n",
       "4            1       3             2            2   \n",
       "5            1       4             2            2   \n",
       "8            1       7             1            4   \n",
       "9            1       8             2            4   \n",
       "\n",
       "                                                                                           spoken_words  \n",
       "id                                                                                                       \n",
       "3                                                                                  Ooo, careful, Homer.  \n",
       "4                                                                        There's no time to be careful.  \n",
       "5                                                                                           We're late.  \n",
       "8                                                                        Sorry, Excuse us. Pardon me...  \n",
       "9   Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons_script_lines.drop(columns=\"raw_text\", inplace=True)\n",
    "simpsons_script_lines.dropna(axis=\"index\", subset=[\"spoken_words\"], inplace=True)\n",
    "simpsons_script_lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selezione delle feature rilevanti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto, abbiamo ripulito i nostri DataFrame in modo da poterli rielaborare per estrarre la variabile da predire e le feature di nostro interesse. Possiamo procedere dunque con l'unione delle singole battute per ottenere nuovamente i copioni originali di ciascun episodio e successivamente effettuare un *join* tra il DataFrame così ottenuto e quello contenente le informazioni per ciascun episodio così da associare a ciascun copione il suo *rating* su IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.2</td>\n",
       "      <td>Ooo, careful, Homer. There's no time to be careful. We're late. Sorry, Excuse us. Pardon me... H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>Come on, Mom. Yeah, Mom, hurry up. All right... hmmm... How about \"he\"? Two points. Your turn, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.5</td>\n",
       "      <td>Now, class, I don't want this field trip to be a repeat of our infamous visit to the Springfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.8</td>\n",
       "      <td>Oh, yeah? Yeah! Oh, yeah? Yeah! Oh, yeah? Yeah! Oh, yeah? Yeah! Hey! What's the problem here? We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>Do I smell cupcakes? Oooo, Do I ever! Uh-uh, Homer. Lisa's making these for her teacher. Ah. Say...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    imdb_rating  \\\n",
       "id                \n",
       "1           8.2   \n",
       "2           7.8   \n",
       "3           7.5   \n",
       "4           7.8   \n",
       "5           8.1   \n",
       "\n",
       "                                                                                           spoken_words  \n",
       "id                                                                                                       \n",
       "1   Ooo, careful, Homer. There's no time to be careful. We're late. Sorry, Excuse us. Pardon me... H...  \n",
       "2   Come on, Mom. Yeah, Mom, hurry up. All right... hmmm... How about \"he\"? Two points. Your turn, d...  \n",
       "3   Now, class, I don't want this field trip to be a repeat of our infamous visit to the Springfield...  \n",
       "4   Oh, yeah? Yeah! Oh, yeah? Yeah! Oh, yeah? Yeah! Oh, yeah? Yeah! Hey! What's the problem here? We...  \n",
       "5   Do I smell cupcakes? Oooo, Do I ever! Uh-uh, Homer. Lisa's making these for her teacher. Ah. Say...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts = pd.DataFrame()\n",
    "simpsons_script_lines.sort_values([\"episode_id\", \"number\"], inplace=True)\n",
    "scripts[\"spoken_words\"] = simpsons_script_lines.groupby(\"episode_id\")[\"spoken_words\"].apply(' '.join)\n",
    "episodes = simpsons_episodes.join(scripts)\n",
    "episodes = episodes.astype({\n",
    "    \"spoken_words\": \"string\"\n",
    "})\n",
    "episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può notare, ci sono 597 episodi con *rating* su IMDB, mentre quelli di cui abbiamo il copione sono solo 564. Non è possibile effetuare la classificazione se il *rating* IMDB è assente e non è molto utile tentare di farlo se il copione non è presente e non abbiamo modo di sapere cosa conteneva. Eliminiamo dunque tutte le righe che presentano valori NA, consapevoli che ci rimangono comunque molte istanze da sfruttare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 600 entries, 1 to 600\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   imdb_rating   597 non-null    float64\n",
      " 1   spoken_words  564 non-null    string \n",
      "dtypes: float64(1), string(1)\n",
      "memory usage: 14.1 KB\n"
     ]
    }
   ],
   "source": [
    "episodes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerchiamo quindi di costruire altre feature rilevanti che possiamo estrarre da ``simpsons_script_lines``. Possiamo ad esempio associare a ciascun episodio il numero di volte che ciascun personaggio pronuncia una battuta. Poichè i personaggi sono molti, salviamo questi dati in una matrice sparsa e, poichè sappiamo che a livello di singolo episodio un personaggio non pronuncia mai più di 131 battute, possiamo scegliere un tipo di dato più adeguato per salvare questi valori per risparmiare spazio in memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = simpsons_script_lines.drop(columns=[\"number\", \"location_id\", \"spoken_words\"]) \\\n",
    "                                  .dropna() \\\n",
    "                                  .join(simpsons_characters, on=\"character_id\") \\\n",
    "                                  .drop(columns=\"character_id\") \\\n",
    "                                  .groupby([\"episode_id\", \"name\"]) \\\n",
    "                                  .size() \\\n",
    "                                  .unstack() \\\n",
    "                                  .fillna(0) \\\n",
    "                                  .astype(np.uint8) \\\n",
    "                                  .reindex(episodes.index)\n",
    "characters_cols = characters.columns\n",
    "characters = csr_matrix(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo ripetere lo stesso procedimento per le location, associando a ciascun episodio il numero di volte che una battuta è pronunciata in una determinata location, per ciascuna location. Poichè le location sono molte, salviamo anche questi dati in una matrice sparsa. Inoltre, sappiamo che a livello di singolo episodio in una data location non sono mai pronunciate più di 203 battute, perciò possiamo scegliere un tipo di dato più adeguato per salvare questi valori e risparmiare così memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = simpsons_script_lines.drop(columns=[\"number\", \"character_id\", \"spoken_words\"]) \\\n",
    "                                 .dropna() \\\n",
    "                                 .join(simpsons_locations, on=\"location_id\") \\\n",
    "                                 .drop(columns=\"location_id\") \\\n",
    "                                 .groupby([\"episode_id\", \"name\"]) \\\n",
    "                                 .size() \\\n",
    "                                 .unstack() \\\n",
    "                                 .fillna(0) \\\n",
    "                                 .astype(np.uint8) \\\n",
    "                                 .reindex(episodes.index)\n",
    "locations_cols = locations.columns\n",
    "locations = csr_matrix(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatto questo, possiamo anche sbarazzarci di ``simpsons_episodes`` e ``simpsons_script_lines``, perchè abbiamo estratto tutto ciò che potevamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "del simpsons_episodes\n",
    "del simpsons_script_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiungiamo quindi la variabile categorica da predire ed eliminiamo la feature del *rating* originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lisa! Lisa, are you still in there? What's the problem? Did you fall in? Lisa! Sorry, Dad. Women...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rusty old hunk of junk. Howdy, Bart. Hot enough for ya? Shut up, Flanders. Hey dad, how come we ...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You know, Bart, when I was your age, I pulled a few boners, but I think you'll find that people ...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Whoops! Whoops! Oh, whoops... whoops! This is gonna be the best birthday breakfast Mom ever had....</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>So how was the office birthday party? Oh, it was delightful. The frosting on the cake was this t...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           spoken_words  \\\n",
       "id                                                                                                        \n",
       "6   Lisa! Lisa, are you still in there? What's the problem? Did you fall in? Lisa! Sorry, Dad. Women...   \n",
       "7   Rusty old hunk of junk. Howdy, Bart. Hot enough for ya? Shut up, Flanders. Hey dad, how come we ...   \n",
       "8   You know, Bart, when I was your age, I pulled a few boners, but I think you'll find that people ...   \n",
       "9   Whoops! Whoops! Oh, whoops... whoops! This is gonna be the best birthday breakfast Mom ever had....   \n",
       "10  So how was the office birthday party? Oh, it was delightful. The frosting on the cake was this t...   \n",
       "\n",
       "   label  \n",
       "id        \n",
       "6   good  \n",
       "7   good  \n",
       "8   good  \n",
       "9   good  \n",
       "10   bad  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[\"label\"] = np.where(episodes[\"imdb_rating\"] >= 7.5, \"good\", \"bad\")\n",
    "episodes.drop(columns=\"imdb_rating\", inplace=True)\n",
    "episodes = episodes.astype({\"label\": \"string\"})\n",
    "episodes.iloc[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il diagramma a torta ci mostra come le deduzioni sulle distribuzioni dei *rating* fatte in precedenza siano corrette. Infatti, le due classi sono legermente sbilanciate - si trovano in un rapporto 60-40 - verso il \"brutto\". Non ci preoccuperemo perciò di bilanciare il numero di istanze tra le due classi, essendo molto vicine al perfetto bilanciamento, però terremo conto di questa proporzione quando effettueremo il calcolo dell'accuratezza nei modelli classificazione. Infatti, sarà molto più facile che un modello sia più corretto se tende a classificare molti episodi come \"brutto\" rispetto ad uno che li classifica invece come \"bello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bad     325\n",
       "good    239\n",
       "Name: label, dtype: Int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, _ = plt.subplots()\n",
    "plt.pie(episodes[\"label\"].value_counts(), labels=[\"bad\", \"good\"], autopct=\"%1.0f%%\")\n",
    "fig.set_facecolor(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione delle feature testuali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto cerchiamo di estrarre le feature testuali, ovverosia le parole o quelle parti di esse, che sono più rilevanti dai copioni precedentemente ricostruiti. Prima di tutto dobbiamo ottenere i nostri train e validation set usando sia i copioni originali sia i valori di *rating*, successivamente dobbiamo costruire i *fold* per la *cross-validation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = episodes[\"spoken_words\"]\n",
    "y = episodes[\"label\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=742)\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=742)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scelta del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa cerchiamo di trovare il miglior *tokenizer* da applicare sui copioni. Per questo motivo al momento non ci preoccupiamo dell'indice di importanza delle parole, scegliamo il *tf-idf index* perchè il suo calcolo risulta più veloce su questo *dataset*. Detto questo, analizziamo il \"caso base\" del *vectorizer* e cerchiamo di ridurre via via il numero di feature senza mai compromettere in maniera significativa l'accuratezza del modello preso in esame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['00', '000', '007', '07s', '10'],\n",
       " ['_______', 'a647253', 'aa', 'aaaa', 'aaaaa'],\n",
       " ['aaannnd', 'aaannnnd', 'aaanyway', 'aag', 'aagh'],\n",
       " ['abbie', 'abbotan', 'abbots', 'abbreviate', 'abbreviations'],\n",
       " ['éclairios', 'êtes', 'être', 'ĉu', 'ĝi'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_base = TfidfVectorizer()\n",
    "vectorizer_base.fit_transform(X_train)\n",
    "features = vectorizer_base.get_feature_names()\n",
    "features[0:5], features[327:332], features[350:355], features[370:375], features[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Senza applicare nessuna variante, otteniamo 32.933 features, ovvero 32.933 parole, molte delle quali osserviamo essere numeri oppure storpiature di parole inserite solo per rendere meglio l'interpretazione delle frasi. Ha perciò senso provare un metodo di *tokenization* differente per osservare se le feature che estrae sono più significative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32993"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiamo allora il *tokenizer* del modulo \"NLTK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['!', '#', '$', '%', '&'],\n",
       " [\"'allo\", \"'angry\", \"'appen\", \"'applause\", \"'at\"],\n",
       " ['1977', '1977.', '1979', '1979.', '1980'],\n",
       " ['19:19', '19:19.', '19th', '1:00', '1:30'],\n",
       " ['é', 'éclairios', 'êtes', 'ĉu', 'ĝi'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tokenizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "vectorizer_tokenizer.fit_transform(X_train)\n",
    "features = vectorizer_tokenizer.get_feature_names()\n",
    "features[0:5], features[30:35], features[350:355], features[370:375], features[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'uso del *tokenizer* di NLTK ha generato all'incirca 5.000 feature in più, anche se notiamo che in questo caso sembrano essere create con più criterio. Compaiono dei simboli di punteggiatura che prima non avevamo incontrato, che ha senso che vengano considerati in maniera a sè stante, dato che non fanno parte di alcuna parola, e non troviamo più numeri dall'aspetto casuale. Notiamo che sono state aggiunte molte parole apostrofate che il *tokenizer* standard non aveva saputo riconoscere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37661"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a questo punto a rimuovere gli accenti e le *stop words*, per cercare di trattenere tutte e sole quelle parole che identificano con chiarezza di cosa si sta parlando, eliminando perciò le parole di contorno. Proviamo inoltre a rimuovere la punteggiatura, ovvero sia tutti quei token che non contengono neanche una lettera o un numero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_nltk(text):\n",
    "    return [token for token in nltk.word_tokenize(text) if re.match(r\"[a-zA-Z0-9]+\", token) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stoplist = nltk.corpus.stopwords.words(\"english\")\n",
    "stoplist = list(set([token for word in nltk_stoplist for token in tokenize_with_nltk(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['0', '0.', '007', '1', '1,000'],\n",
       " ['10th', '11', '111238,390,17', '1132.', '117th'],\n",
       " ['60', '600', '6000', '605', '6051'],\n",
       " ['69808,243,293', '6:45', '6th', '7', '7-10'],\n",
       " ['zz', 'zz-99', 'zzyzwiski', 'zzzapp', 'zzzzapp'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_stopwords = TfidfVectorizer(tokenizer=tokenize_with_nltk, stop_words=stoplist, strip_accents=\"unicode\")\n",
    "vectorizer_stopwords.fit_transform(X_train)\n",
    "features = vectorizer_stopwords.get_feature_names()\n",
    "features[0:5], features[30:35], features[350:355], features[370:375], features[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La rimozione dei *token* descritti in precedenza non ha giovato particolarmente. Sono state rimosse solamente 400 feature, all'incirca. Evidentemente il metodo usato finora non è stato capace di cogliere tutti quegli elementi che aveva senso rimuovere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37257"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applicazione di meccanismi avanzati: POS tagging, lemmatization, stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a questo punto a complicare la *tokenization* applicandole i meccanismi di *POS tagging*, di lemmatizzazione e di *stemming*. Cerchiamo perciò di ridurre ancora il numero delle feature togliendo anche tutte quelle parole che non sono altro che la variante della stessa dove però tra di esse cambia la declinazione o il tipo dell'elemento grammaticale, caratteristiche che non vanno ad incidere particolarmente sul contenuto generale del discorso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_pos(text):\n",
    "    return nltk.pos_tag(tokenize_with_nltk(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_stoplist = list(set([token for word in nltk_stoplist for token in tokenize_with_pos(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "penn_to_wn = {\"N\": \"n\", \"V\": \"v\", \"J\": \"a\", \"R\": \"r\"}\n",
    "def tokenize_with_lemmatization(text):\n",
    "    return [(wnl.lemmatize(token, penn_to_wn[tag[0]]) if tag[0] in penn_to_wn else token) for token, tag in tokenize_with_pos(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization_stoplist = list(set([token for word in nltk_stoplist for token in tokenize_with_lemmatization(word)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "def tokenize_with_stemming(text):\n",
    "    return [ps.stem(word) for word in tokenize_with_nltk(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_stoplist = list(set([token for word in nltk_stoplist for token in tokenize_with_stemming(word)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa volta, avendo a disposizione un insieme di valori tra cui scegliere, il *tokenizer* migliore sarà determinato effettuando delle regressioni logistiche di prova, che usano anche la standardizzazione delle feature. Notiamo come l'uso del *POS tagging* aumenta il numero delle feature perchè introduce varianti della stessa parola con però associati diversi tag, dato che una stessa parola può ricoprire più ruoli all'interno del discorso. Il risultato è che l'accuratezza scende, come potevamo aspettarci, anche se non in maniera significativa. La lemmatizzazione invece riesce a ridurre il numero di feature con un aumento dell'accuratezza, ma il metodo migliore si rivela lo *stemming*. Esso infatti riduce più di tutti gli altri il numero delle feature, avendo però l'accuratezza più alta. Useremo perciò quest'ultimo metodo nel prosieguo dell'eliminazione delle feature non rilevanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nltk</th>\n",
       "      <td>0.733441</td>\n",
       "      <td>0.017348</td>\n",
       "      <td>29885.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>0.690338</td>\n",
       "      <td>0.013318</td>\n",
       "      <td>45446.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatization</th>\n",
       "      <td>0.743562</td>\n",
       "      <td>0.032009</td>\n",
       "      <td>25304.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stemming</th>\n",
       "      <td>0.761335</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>22438.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean_test_score  std_test_score  number_features\n",
       "nltk                  0.733441        0.017348     29885.666667\n",
       "pos                   0.690338        0.013318     45446.333333\n",
       "lemmatization         0.743562        0.032009     25304.666667\n",
       "stemming              0.761335        0.032526     22438.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "values = [(tokenize_with_nltk, stoplist),\n",
    "          (tokenize_with_pos, pos_stoplist),\n",
    "          (tokenize_with_lemmatization, lemmatization_stoplist),\n",
    "          (tokenize_with_stemming, stemming_stoplist)]\n",
    "for i, value in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"vectorizer\", TfidfVectorizer(tokenizer=value[0], stop_words=value[1], strip_accents=\"unicode\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000))\n",
    "    ])\n",
    "    result = cross_validate(model, X_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: len(e.named_steps[\"vectorizer\"].get_feature_names()), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=[\"nltk\", \"pos\", \"lemmatization\", \"stemming\"], columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ricerca della minimum document frequency, degli ngram, dell'indice di importanza dei termini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo poi ad eliminare le parole che compaiono troppo di rado, così da rimuovere quelle feature che non sono veramente tali, ma che probabilmente sono solo uno scarto del processo di *tokenization* o che comunque non sono significative perchè sono presenti poche volte in assoluto. Osserviamo che se scegliessimo come ``min_df`` il valore 5, lo score medio sarebbe il più alto in assoluto con una riduzione delle *feature* di circa un fattore 4, ma con il valore 10 avremmo un calo di solo l'1,7% dello *score* rispetto al caso precedente, ma ridurremmo di un fattore 7 le *feature* utilizzate. Scegliamo perciò quest'ultimo come valore per il parametro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.761335</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>22438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.824736</td>\n",
       "      <td>0.041275</td>\n",
       "      <td>4915.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.807059</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>2959.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.814635</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>2135.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  std_test_score  number_features\n",
       "1          0.761335        0.032526     22438.000000\n",
       "5          0.824736        0.041275      4915.666667\n",
       "10         0.807059        0.025509      2959.333333\n",
       "15         0.814635        0.024138      2135.000000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "values = [1, 5, 10, 15]\n",
    "for i, value in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"vectorizer\", TfidfVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=value)),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000))\n",
    "    ])\n",
    "    result = cross_validate(model, X_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: len(e.named_steps[\"vectorizer\"].get_feature_names()), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=values, columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerchiamo poi di capire se l'utilizzo degli \"ngram\" permette di migliorare significativamente l'accuratezza del nostro modello oppure no. Come potevamo aspettarci, l'uso degli \"ngram\" riesce ad aumentare lo *score*, grazie all'aumento delle *feature* nell'operazione di classificazione. Dato che l'aumento è significativo, di circa il 3% se usiamo sia i \"3-gram\" che i \"2-gram\", decidiamo di utilizzarli da ora in poi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.837555</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>4411.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.832485</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>4338.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.807059</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>2959.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score  number_features\n",
       "3         0.837555        0.021943      4411.666667\n",
       "2         0.832485        0.012480      4338.333333\n",
       "1         0.807059        0.025509      2959.333333"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "values = [(1, 1), (1, 2), (1, 3)]\n",
    "for i, value in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"vectorizer\", TfidfVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=10, ngram_range=value)),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=5000))\n",
    "    ])\n",
    "    result = cross_validate(model, X_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: len(e.named_steps[\"vectorizer\"].get_feature_names()), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=[1, 2, 3], columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"]) \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, cerchiamo di capire quale indice di importanza delle parole, se il semplice conteggio delle stesse o il *tf-idf index*, è il più adatto per l'utilizzo durante la regressione. L'indice *tf-idf* risulta migliore di circa il 2%, perciò continueremo perciò ad utilizzare \"TfidfVectorizer\" per la costruzione della nostra matrice documenti-termini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CountVectorizer</th>\n",
       "      <td>0.817237</td>\n",
       "      <td>0.011122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TfidfVectorizer</th>\n",
       "      <td>0.837555</td>\n",
       "      <td>0.021943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_test_score  std_test_score\n",
       "vectorizer                                      \n",
       "CountVectorizer         0.817237        0.011122\n",
       "TfidfVectorizer         0.837555        0.021943"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", None),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=5000))\n",
    "])\n",
    "grid = {\n",
    "    \"vectorizer\": [TfidfVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=10, ngram_range=(1, 3)),\n",
    "                   CountVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=10, ngram_range=(1, 3))]\n",
    "}\n",
    "gs = GridSearchCV(model, grid, cv=skf)\n",
    "gs.fit(X_train, y_train)\n",
    "results = pd.DataFrame(gs.cv_results_)\n",
    "results[\"vectorizer\"] = [\"TfidfVectorizer\", \"CountVectorizer\"]\n",
    "results[[\"vectorizer\", \"mean_test_score\", \"std_test_score\"]].set_index(\"vectorizer\").sort_values(\"mean_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unione di feature testuali e non"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridotte al minimo le feature testuali, si tratta di unirle con quelle non testuali precedentemente estratte per poter eseguire la classificazione. Ci occupiamo allora di ridurre anche il numero di feature non testuali, visto che dall'analisi dei dati molte sono risultate scarsissimamente popolate. Per prima cosa riduciamo il numero dei personaggi tramite l'uso di regolarizzazione \"Lasso\". L'uso di una regolarizzazione \"L1\" più forte rende lo score più alto, ma solo se non è troppo forte. Questo è evidentemente segno del fatto che moltissimi personaggi sono irrilevanti ai fini dell'identificazione della qualità di un episodio. Usiamo perciò il coefficiente che ci dà lo score più alto, anche se questo significa sacrificare molti personaggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.733403</td>\n",
       "      <td>0.031875</td>\n",
       "      <td>134.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.00</th>\n",
       "      <td>0.720661</td>\n",
       "      <td>0.043197</td>\n",
       "      <td>1907.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.00</th>\n",
       "      <td>0.718136</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>2140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.00</th>\n",
       "      <td>0.718116</td>\n",
       "      <td>0.044412</td>\n",
       "      <td>2453.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.707957</td>\n",
       "      <td>0.045905</td>\n",
       "      <td>1070.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.652171</td>\n",
       "      <td>0.033029</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_test_score  std_test_score  number_features\n",
       "C                                                      \n",
       "0.10          0.733403        0.031875       134.333333\n",
       "5.00          0.720661        0.043197      1907.666667\n",
       "7.00          0.718136        0.038728      2140.000000\n",
       "10.00         0.718116        0.044412      2453.333333\n",
       "1.00          0.707957        0.045905      1070.666667\n",
       "0.05          0.652171        0.033029         9.333333"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_train, characters_val, y_train, y_val = train_test_split(characters, episodes[\"label\"], test_size=0.3, random_state=742)\n",
    "results = []\n",
    "values = [1, 5, 7, 10, 0.1, 0.05]\n",
    "for i, C in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000, penalty=\"l1\", C=C))\n",
    "    ])\n",
    "    result = cross_validate(model, characters_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: (e.named_steps[\"classifier\"].coef_[0] != 0).sum(), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=pd.Index(values, name=\"C\"), columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"]) \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo che tra i personaggi che abbiamo scartato, moltissimi apparivano solo in una puntata e perciò non erano personaggi ricorrenti, ma semplici comparse a cui erano date delle battute e che non danno quindi alcun contributo nel capire di cosa parla un episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           \"For Dummies\" Author\n",
       "1    \"Just Stamp the Ticket\" Man\n",
       "2                     \"Mario\" #2\n",
       "3                  \"Shorts\" Bart\n",
       "4                 \"Shorts\" Homer\n",
       "5                  \"Shorts\" Lisa\n",
       "6                 \"Shorts\" Marge\n",
       "7                \"Yeeeessss\" Man\n",
       "8                    \"Yesss\" Man\n",
       "9                \"mad\" Writer #1\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000, penalty=\"l1\", C=0.1))\n",
    "    ])\n",
    "model.fit(characters_train, y_train)\n",
    "pd.Series(characters_cols[model.named_steps[\"classifier\"].coef_[0] == 0]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procediamo quindi alla rimozione dei personaggi irrilevanti, ovvero tutti quelli che si trovano in corrispondenza di *feature* il cui coefficiente è 0, cioè non danno alcun contributo al risultato della classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<564x384 sparse matrix of type '<class 'numpy.uint8'>'\n",
       "\twith 4464 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = csr_matrix(pd.DataFrame(characters.toarray(), columns=characters_cols) \\\n",
    "                          .drop(columns=characters_cols[model.named_steps[\"classifier\"].coef_[0] == 0]))\n",
    "characters_cols = characters_cols.drop(characters_cols[model.named_steps[\"classifier\"].coef_[0] == 0])\n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ripetiamo quindi lo stesso procedimento con le *location* per eliminare quelle non particolarmente significative. Anche in questo caso, riusciamo a ridurre significativamente le feature da usare, anche se meno rispetto al caso precedente, usando una regolarizzazione un po' meno intensa. In effetti, le location erano molto sparse su varie feature, ma non così tanto come per i personaggi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>number_features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.654773</td>\n",
       "      <td>0.020793</td>\n",
       "      <td>890.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.654715</td>\n",
       "      <td>0.038914</td>\n",
       "      <td>1019.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.639660</td>\n",
       "      <td>0.044496</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.637038</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>728.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.00</th>\n",
       "      <td>0.636980</td>\n",
       "      <td>0.051243</td>\n",
       "      <td>1376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.00</th>\n",
       "      <td>0.609049</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>1581.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_test_score  std_test_score  number_features\n",
       "C                                                     \n",
       "0.50         0.654773        0.020793       890.666667\n",
       "1.00         0.654715        0.038914      1019.333333\n",
       "0.10         0.639660        0.044496        80.000000\n",
       "0.25         0.637038        0.007916       728.333333\n",
       "3.00         0.636980        0.051243      1376.000000\n",
       "5.00         0.609049        0.052857      1581.666667"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_train, locations_val, y_train, y_val = train_test_split(locations, episodes[\"label\"], test_size=0.3, random_state=742)\n",
    "results = []\n",
    "values = [1, 3, 5, 0.5, 0.25, 0.1]\n",
    "for i, C in enumerate(values):\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=50000, penalty=\"l1\", C=C))\n",
    "    ])\n",
    "    result = cross_validate(model, locations_train, y_train, cv=skf, return_estimator=True, n_jobs=-1)\n",
    "    score = result[\"test_score\"]\n",
    "    features = np.mean(list(map(lambda e: (e.named_steps[\"classifier\"].coef_[0] != 0).sum(), result[\"estimator\"])))\n",
    "    results.append((score.mean(), score.std(), features))\n",
    "pd.DataFrame(results, index=pd.Index(values, name=\"C\"), columns=[\"mean_test_score\", \"std_test_score\", \"number_features\"]) \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso le *feature* che vogliamo rimuovere sembrano essere quelle legate a *location* che compaiono solo in una puntata e che perciò non incidono sull'apprezzamento delle varie puntate in generale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \"ALL THE WRONG REASONS\" MOVIE THEATER\n",
       "1              \"BOOKS FOR DUMMIES\" SECTION\n",
       "2                          \"GUT CHECK\" SET\n",
       "3                           \"HEADBUTT\" SET\n",
       "4                  \"INCEPTION\"-STYLE BEACH\n",
       "5                      \"IT NEVER ENDS\" SET\n",
       "6                         \"LAUGH IN\" STAGE\n",
       "7                           \"MAD\" BUILDING\n",
       "8                     \"MOON BOUNCE\" CASTLE\n",
       "9                             \"WALT'S\" CAR\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000, penalty=\"l1\", C=1.0))\n",
    "])\n",
    "model.fit(locations_train, y_train)\n",
    "pd.Series(locations_cols[model.named_steps[\"classifier\"].coef_.sum(axis=0) == 0]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procediamo quindi come prima ad eliminare tutte quelle *location* non usate durante la classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<564x1476 sparse matrix of type '<class 'numpy.uint8'>'\n",
       "\twith 4654 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = csr_matrix(pd.DataFrame(locations.toarray(), columns=locations_cols) \\\n",
    "                         .drop(columns=locations_cols[model.named_steps[\"classifier\"].coef_[0] == 0]))\n",
    "locations_cols = locations_cols.drop(locations_cols[model.named_steps[\"classifier\"].coef_[0] == 0])\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finita questa procedura di estrazione, possiamo procedere alla fusione dei nostri tre *set* di *feature* e alla costruzione di *training set* e *validation set* finali. Questo implica, per quanto riguarda i copioni, che dopo aver effettuato il loro *split* costruiamo anche la matrice documenti-termini su quelli presenti nel *training set* e poi ci basiamo su questa per costruire quella del validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train, words_val, y_train, y_val, characters_train, characters_val, locations_train, locations_val \\\n",
    "    = train_test_split(episodes[\"spoken_words\"],\n",
    "                       episodes[\"label\"],\n",
    "                       characters,\n",
    "                       locations,\n",
    "                       test_size=0.3,\n",
    "                       random_state=742)\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_with_stemming, stop_words=stemming_stoplist, strip_accents=\"unicode\", min_df=10, ngram_range=(1, 3))\n",
    "dtm_train = vectorizer.fit_transform(words_train)\n",
    "dtm_val = vectorizer.transform(words_val)\n",
    "X_train = hstack([dtm_train, characters_train, locations_train])\n",
    "X_val = hstack([dtm_val, characters_val, locations_val])\n",
    "index = vectorizer.get_feature_names() + characters_cols.to_list() + locations_cols.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo perciò a disposizione circa 8.000 feature sulle quali effettuare la nostra classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8389"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test di modelli di classificazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto, ottenute tutte le *feature* che abbiamo intenzione di usare, possiamo cercare i modelli che classificano meglio i copioni degli episodi all'interno di ciascuna classe. Per prima cosa definiamo alcune funzioni, come quella per il calcolo dell'intervallo di confidenza, che utilizzeremo per valutare i nostri modelli. In ciascun test faremo inoltre attenzione a bilanciare i pesi delle classi. Infatti, pur non essendoci forti sbilanciamenti tra le due, cerchiamo comunque di favorire quei modelli che riescono ad individuare con maggior accuratezza la classe dalla dimensione più piccola. Nei modelli seguenti non è stato considerato l'uso di *feature* polinomiali perchè avrebbe portato ad un esplosione del loro numero anche con grado basso. Le feature sarebbero diventate infatti circa 30 milioni solamente usando un grado 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(model, X_val, y_val, confidence_level=0.95):\n",
    "    accuracy = model.score(X_val, y_val)\n",
    "    size = X_val.shape[0]\n",
    "    Z = norm.ppf((1 + confidence_level) / 2)\n",
    "    b = (2 * size * accuracy + Z ** 2) \n",
    "    delta_root = Z * np.sqrt(Z ** 2 + 4 * size * accuracy - 4 * size * accuracy ** 2) \n",
    "    return (b - delta_root) / (2 * (size + Z ** 2)), (b + delta_root) / (2 * (size + Z**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione tramite Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo i nostri test a partire dall'utilizzo di \"Perceptron\" come metodo per l'individuazione dei modelli migliori, il più semplice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perc = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", Perceptron(random_state=742, max_iter=10000, n_jobs=-1, class_weight=\"balanced\"))\n",
    "])\n",
    "grid = [{\n",
    "    \"classifier__penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "    \"classifier__alpha\": np.logspace(-2, 2, 5),\n",
    "    \"classifier__eta0\": [0.1, 0.5, 0.9]\n",
    "}, {\n",
    "    \"classifier__penalty\": [None],\n",
    "    \"classifier__eta0\": [0.1, 0.5, 0.9]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sua semplicità si riflette nei risultati che otteniamo: nessun tipo di regolarizzazione riesce a migliorare gli score dei modelli testati, che rimangono sempre uguali nonostante la variazione del parametro ``eta0``. Inoltre, i risultati si aggirano attorno all'83,3%, sono perciò decisamente migliorabili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__penalty</th>\n",
       "      <th>param_classifier__alpha</th>\n",
       "      <th>param_classifier__eta0</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.832504</td>\n",
       "      <td>0.022330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.832504</td>\n",
       "      <td>0.022330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.832504</td>\n",
       "      <td>0.022330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.758925</td>\n",
       "      <td>0.012077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.758925</td>\n",
       "      <td>0.012077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.758925</td>\n",
       "      <td>0.012077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.746183</td>\n",
       "      <td>0.053047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.746183</td>\n",
       "      <td>0.053047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.738646</td>\n",
       "      <td>0.028769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.738646</td>\n",
       "      <td>0.028769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_classifier__penalty param_classifier__alpha param_classifier__eta0  \\\n",
       "47                      None                     NaN                    0.9   \n",
       "46                      None                     NaN                    0.5   \n",
       "45                      None                     NaN                    0.1   \n",
       "1                         l1                    0.01                    0.1   \n",
       "4                         l1                    0.01                    0.5   \n",
       "7                         l1                    0.01                    0.9   \n",
       "6                         l2                    0.01                    0.9   \n",
       "8                 elasticnet                    0.01                    0.9   \n",
       "3                         l2                    0.01                    0.5   \n",
       "5                 elasticnet                    0.01                    0.5   \n",
       "\n",
       "    mean_test_score  std_test_score  \n",
       "47         0.832504        0.022330  \n",
       "46         0.832504        0.022330  \n",
       "45         0.832504        0.022330  \n",
       "1          0.758925        0.012077  \n",
       "4          0.758925        0.012077  \n",
       "7          0.758925        0.012077  \n",
       "6          0.746183        0.053047  \n",
       "8          0.746183        0.053047  \n",
       "3          0.738646        0.028769  \n",
       "5          0.738646        0.028769  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_perc = GridSearchCV(model_perc, grid, cv=skf)\n",
    "gs_perc.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_perc.cv_results_)[[\"param_classifier__penalty\", \"param_classifier__alpha\", \"param_classifier__eta0\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valutando l'intervallo di confidenza basato sul validation set, vediamo che l'accuratezza può oscillare tra il 63,3% e il 76,9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6334475950632291, 0.7692181468999717)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_perc = gs_perc.best_estimator_\n",
    "y_pred = best_perc.predict(X_val)\n",
    "confidence_interval(best_perc, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori di *precision* e *recall* ci dicono che il migliore modello individuato tende ad essere più preciso nel distinguere dagli altri gli episodi \"brutti\", rispetto a quanto accade nell'individuare quelli \"belli\", mentre al contrario tende ad essere più capace, messo di fronte agli episodi \"belli\", di capire che sono effettivamente \"belli\", rispetto a quanto capita con quelli \"brutti\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>57</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>11</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    57    39\n",
       "good   11    63"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_perc.classes_, columns=best_perc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.84      0.59      0.70        96\n",
      "        good       0.62      0.85      0.72        74\n",
      "\n",
      "    accuracy                           0.71       170\n",
      "   macro avg       0.73      0.72      0.71       170\n",
      "weighted avg       0.74      0.71      0.70       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione tramite regressione logistica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo utilizzare al posto di \"Perceptron\" un metodo come la regressione logistica, in concomitanza con tutti i possibili tipi di regolarizzazione ammissibili, cioè \"Lasso\", \"Ridge\" ed \"Elastic Net\". Definiamo quindi il nostro modello generale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", random_state=742, max_iter=10000, class_weight=\"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passiamo poi alla ricerca vera e propria. Se consideriamo le singole regolarizzazioni \"Lasso\" e \"Ridge\", il risultato migliore si ha in corrispondenza di una regolarizzazione \"Ridge\" più intensa. L'uso di una regolarizzazione sempre molto intensa, ma in concomitanza con il tipo \"Lasso\", fa crollare a picco lo score. Il primo modello che ha uno score comparabile con quello del migliore, ma fa uso della regolarizzazione \"Lasso\", ha infatti un peso della stessa molto più basso. Evidentemente limitare la dimensione dei coefficienti ha un effetto decisamente più significativo sulle performance di un ipotetico modello rispetto ad eliminare direttamente le feature inutili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__penalty</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.880716</td>\n",
       "      <td>0.015642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.880716</td>\n",
       "      <td>0.015642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>l1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.880716</td>\n",
       "      <td>0.009448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.878171</td>\n",
       "      <td>0.012473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>l2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.878171</td>\n",
       "      <td>0.012473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>l2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.878171</td>\n",
       "      <td>0.012473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>l1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.875607</td>\n",
       "      <td>0.014603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.842625</td>\n",
       "      <td>0.007485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.784255</td>\n",
       "      <td>0.004035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.525657</td>\n",
       "      <td>0.077068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_classifier__penalty param_classifier__C  mean_test_score  \\\n",
       "1                        l2                0.01         0.880716   \n",
       "3                        l2                 0.1         0.880716   \n",
       "8                        l1                 100         0.880716   \n",
       "5                        l2                   1         0.878171   \n",
       "7                        l2                  10         0.878171   \n",
       "9                        l2                 100         0.878171   \n",
       "6                        l1                  10         0.875607   \n",
       "4                        l1                   1         0.842625   \n",
       "2                        l1                 0.1         0.784255   \n",
       "0                        l1                0.01         0.525657   \n",
       "\n",
       "   std_test_score  \n",
       "1        0.015642  \n",
       "3        0.015642  \n",
       "8        0.009448  \n",
       "5        0.012473  \n",
       "7        0.012473  \n",
       "9        0.012473  \n",
       "6        0.014603  \n",
       "4        0.007485  \n",
       "2        0.004035  \n",
       "0        0.077068  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_l1_l2 = {\n",
    "    \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "    \"classifier__C\": np.logspace(-2, 2, 5)\n",
    "}\n",
    "gs_l1_l2 = GridSearchCV(model_logreg, grid_l1_l2, cv=skf)\n",
    "gs_l1_l2.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_l1_l2.cv_results_)[[\"param_classifier__penalty\", \"param_classifier__C\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'uso della regolarizzazione \"Elastic Net\" riconferma il risultato già trovato in precedenza, ovverosia che i modelli migliori si hanno quando il peso della regolarizzazione è quasi esclusivamente concentrato sul tipo \"Ridge\" anzichè sul tipo \"Lasso\", quindi quelli che presentano un valore di ``l1_ratio`` basso. In questo caso però, il miglior modello estratto si ha in concomitanza con un parametro di regolarizzazione relativamente basso. In ogni caso, il modello con lo *score* migliore lo ha identico ai migliori modelli osservati precedentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__l1_ratio</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.880716</td>\n",
       "      <td>0.009448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.875646</td>\n",
       "      <td>0.006977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.870576</td>\n",
       "      <td>0.005813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.868031</td>\n",
       "      <td>0.003108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.868031</td>\n",
       "      <td>0.012847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.860359</td>\n",
       "      <td>0.013398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.847695</td>\n",
       "      <td>0.006686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.822288</td>\n",
       "      <td>0.013531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.789344</td>\n",
       "      <td>0.003284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_classifier__C param_classifier__l1_ratio  mean_test_score  \\\n",
       "8                  10                        0.1         0.880716   \n",
       "7                  10                        0.5         0.875646   \n",
       "5                   1                        0.1         0.870576   \n",
       "2                 0.1                        0.1         0.868031   \n",
       "6                  10                        0.9         0.868031   \n",
       "4                   1                        0.5         0.860359   \n",
       "3                   1                        0.9         0.847695   \n",
       "1                 0.1                        0.5         0.822288   \n",
       "0                 0.1                        0.9         0.789344   \n",
       "\n",
       "   std_test_score  \n",
       "8        0.009448  \n",
       "7        0.006977  \n",
       "5        0.005813  \n",
       "2        0.003108  \n",
       "6        0.012847  \n",
       "4        0.013398  \n",
       "3        0.006686  \n",
       "1        0.013531  \n",
       "0        0.003284  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_elasticnet = {\n",
    "    \"classifier__penalty\": [\"elasticnet\"],\n",
    "    \"classifier__C\": np.logspace(-1, 1, 3),\n",
    "    \"classifier__l1_ratio\": [0.9, 0.5, 0.1]\n",
    "}\n",
    "gs_elasticnet = GridSearchCV(model_logreg, grid_elasticnet, cv=skf)\n",
    "gs_elasticnet.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_elasticnet.cv_results_)[[\"param_classifier__C\", \"param_classifier__l1_ratio\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non usare nessuna regolarizzazione non porta ad un risultato interessante, perchè il modello ha sì uno score alto, ma è comunque più basso di quelli che utilizzano la regolarizzazione. Perciò, non prenderemo in considerazione quest'ultimo caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.878171</td>\n",
       "      <td>0.012473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score\n",
       "0         0.878171        0.012473"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_none = {\n",
    "    \"classifier__penalty\": [\"none\"]\n",
    "}\n",
    "gs_none = GridSearchCV(model_logreg, grid_none, cv=skf)\n",
    "gs_none.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_none.cv_results_)[[\"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osservando l'intervallo di confidenza, vediamo che il valore reale dell'accuratezza del modello migliore, cioè quello che usa la sola regolarizzazione \"L2\", oscilla tra il 77,2% e l'88,4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7722520572638996, 0.8835178653618844)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logreg = gs_l1_l2.best_estimator_\n",
    "y_pred = best_logreg.predict(X_val)\n",
    "confidence_interval(best_logreg, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dai valori di *precision* e *recall* riusciamo a capire che il migliore modello individuato tramite regressione logstica è superiore a quello individuato tramite \"Perceptron\" per quanto riguarda quasi tutte le metriche. Peggiora, anche se di poco, la *precision* degli episodi \"brutti\", ma questo viene compensato da un valore di *recall* della classe \"bad\" molto più alto. Discorso simile vale per la classe \"good\" dove la *precision* è salita di più del 20% mentre la *recall* è scesa del 9%. Questo miglioramento complessivo si riflette sull'*f1-score* di entrambe le classi, che fa salire decisamente l'accuratezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>86</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>18</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    86    10\n",
       "good   18    56"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_logreg.classes_, columns=best_logreg.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.83      0.90      0.86        96\n",
      "        good       0.85      0.76      0.80        74\n",
      "\n",
      "    accuracy                           0.84       170\n",
      "   macro avg       0.84      0.83      0.83       170\n",
      "weighted avg       0.84      0.84      0.83       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione mediante Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo le Support Vector Machines per poter sfruttare un metodo che usa il *kernel trick* per effettuare la classificazione, utilizzando così molte più *feature* di quelle presenti nel *dataset*, senza però la loro effettiva generazione. Notiamo che l'uso di un kernel di tipo sigmoidale è la scelta migliore, in combinazione con il parametro della regolarizzazione \"Ridge\" pari a 1. Lo score del miglior modello è addirittura superiore allo score del miglior modello ottenuto tramite regressione logistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__kernel</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__degree</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.888388</td>\n",
       "      <td>0.021487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885805</td>\n",
       "      <td>0.010584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885805</td>\n",
       "      <td>0.010584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875665</td>\n",
       "      <td>0.017752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>linear</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875665</td>\n",
       "      <td>0.017752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>linear</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875665</td>\n",
       "      <td>0.017752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875665</td>\n",
       "      <td>0.017752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875665</td>\n",
       "      <td>0.017752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.781826</td>\n",
       "      <td>0.041802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rbf</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.766597</td>\n",
       "      <td>0.041766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_classifier__kernel param_classifier__C param_classifier__degree  \\\n",
       "7                   sigmoid                   1                      NaN   \n",
       "10                  sigmoid                  10                      NaN   \n",
       "13                  sigmoid                 100                      NaN   \n",
       "2                    linear                0.01                      NaN   \n",
       "5                    linear                 0.1                      NaN   \n",
       "8                    linear                   1                      NaN   \n",
       "11                   linear                  10                      NaN   \n",
       "14                   linear                 100                      NaN   \n",
       "6                       rbf                   1                      NaN   \n",
       "12                      rbf                 100                      NaN   \n",
       "\n",
       "    mean_test_score  std_test_score  \n",
       "7          0.888388        0.021487  \n",
       "10         0.885805        0.010584  \n",
       "13         0.885805        0.010584  \n",
       "2          0.875665        0.017752  \n",
       "5          0.875665        0.017752  \n",
       "8          0.875665        0.017752  \n",
       "11         0.875665        0.017752  \n",
       "14         0.875665        0.017752  \n",
       "6          0.781826        0.041802  \n",
       "12         0.766597        0.041766  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", SVC(random_state=742, class_weight=\"balanced\"))\n",
    "])\n",
    "grid_svm = [{\n",
    "    \"classifier__kernel\": [\"rbf\", \"sigmoid\", \"linear\"],\n",
    "    \"classifier__C\": np.logspace(-2, 2, 5)\n",
    "}, {\n",
    "    \"classifier__kernel\": [\"poly\"],\n",
    "    \"classifier__degree\": [2, 3],\n",
    "    \"classifier__C\": np.logspace(-2, 2, 5)\n",
    "}]\n",
    "gs_svm = GridSearchCV(model_svm, grid_svm, cv=skf)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_svm.cv_results_)[[\"param_classifier__kernel\", \"param_classifier__C\", \"param_classifier__degree\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infatti, l'intervallo di confidenza è molto simile a quello del miglior modello ottenuto tramite regressione logistica, è infatti compreso tra il 78,5% e l'89,3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.785338347557347, 0.8934410460377628)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svm = gs_svm.best_estimator_\n",
    "y_pred = best_svm.predict(X_val)\n",
    "confidence_interval(best_svm, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si osserva che l'accuratezza più alta rispetto al miglior modello ottenuto tramite regressione logistica è dovuta ad una maggiore *precision* nell'individuare gli episodi \"brutti\" e ad una maggiore *recall* della classe \"good\", non compensata da una minor *precision* nell'individuare gli episodi \"belli\" e da una minor *recall* della classe \"bad\". Da questo deriva che entrambi gli *f1-score* sono più alti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>80</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    80    16\n",
       "good   10    64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_svm.classes_, columns=best_svm.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.89      0.83      0.86        96\n",
      "        good       0.80      0.86      0.83        74\n",
      "\n",
      "    accuracy                           0.85       170\n",
      "   macro avg       0.84      0.85      0.85       170\n",
      "weighted avg       0.85      0.85      0.85       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione tramite XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ad utilizzare anche \"XGBoost\" per vedere se è capace di darci risultati più interessanti. Per prima cosa lo testiamo senza applicare alcuna regolarizzazione per vedere quali iperparametri propri del classificatore sembrano essere migliori. In un secondo momento vedremo se invece, applicando una regolarizzazione, il risultato è migliorato oppure no. I primi risultati non sono incoraggianti, lo score medio del modello migliore è infatti addirittura inferiore a quello del modello ottenuto con \"Perceptron\". In particolare, il valore massimo dello score si ha in corrispondenza di un learning rate relativamente piccolo e di una profondità degli alberi non necessariamente alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__learning_rate</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.771416</td>\n",
       "      <td>0.043782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.758771</td>\n",
       "      <td>0.032663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.758771</td>\n",
       "      <td>0.032663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.746087</td>\n",
       "      <td>0.029551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.735967</td>\n",
       "      <td>0.020871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.733499</td>\n",
       "      <td>0.012502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.733499</td>\n",
       "      <td>0.012502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.730877</td>\n",
       "      <td>0.024435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>15</td>\n",
       "      <td>0.730877</td>\n",
       "      <td>0.024435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.581213</td>\n",
       "      <td>0.001499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_classifier__learning_rate param_classifier__max_depth  \\\n",
       "3                             0.1                           5   \n",
       "4                             0.1                          10   \n",
       "5                             0.1                          15   \n",
       "6                               1                           5   \n",
       "0                            0.01                           5   \n",
       "7                               1                          10   \n",
       "8                               1                          15   \n",
       "1                            0.01                          10   \n",
       "2                            0.01                          15   \n",
       "9                              10                           5   \n",
       "\n",
       "   mean_test_score  std_test_score  \n",
       "3         0.771416        0.043782  \n",
       "4         0.758771        0.032663  \n",
       "5         0.758771        0.032663  \n",
       "6         0.746087        0.029551  \n",
       "0         0.735967        0.020871  \n",
       "7         0.733499        0.012502  \n",
       "8         0.733499        0.012502  \n",
       "1         0.730877        0.024435  \n",
       "2         0.730877        0.024435  \n",
       "9         0.581213        0.001499  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", XGBClassifier(objective=\"binary:logistic\", random_state=742, n_jobs=4))\n",
    "])\n",
    "grid = [{\n",
    "    \"classifier__learning_rate\": np.logspace(-2, 2, 5).tolist(),\n",
    "    \"classifier__max_depth\": [5, 10, 15]\n",
    "}]\n",
    "gs = GridSearchCV(model, grid, cv=skf)\n",
    "gs.fit(X_train, y_train)\n",
    "pd.DataFrame(gs.cv_results_)[[\"param_classifier__learning_rate\", \"param_classifier__max_depth\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'uso dei parametri individuati al passo precedente per costruire un modello su cui testare la regolarizzazione non ci fa ottenere un risultato di tanto migliore, infatti lo *score* aumenta dello 0,3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__reg_alpha</th>\n",
       "      <th>param_classifier__reg_lambda</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.773980</td>\n",
       "      <td>0.038676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.766366</td>\n",
       "      <td>0.036639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.761277</td>\n",
       "      <td>0.040718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.761277</td>\n",
       "      <td>0.042125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.761238</td>\n",
       "      <td>0.051358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.758771</td>\n",
       "      <td>0.031451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.756207</td>\n",
       "      <td>0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753682</td>\n",
       "      <td>0.035034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.748651</td>\n",
       "      <td>0.025619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.746087</td>\n",
       "      <td>0.029551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_classifier__reg_alpha param_classifier__reg_lambda  mean_test_score  \\\n",
       "0                         0.01                         0.01         0.773980   \n",
       "6                          0.1                          0.1         0.766366   \n",
       "1                         0.01                          0.1         0.761277   \n",
       "12                           1                            1         0.761277   \n",
       "2                         0.01                            1         0.761238   \n",
       "5                          0.1                         0.01         0.758771   \n",
       "10                           1                         0.01         0.756207   \n",
       "7                          0.1                            1         0.753682   \n",
       "13                           1                           10         0.748651   \n",
       "8                          0.1                           10         0.746087   \n",
       "\n",
       "    std_test_score  \n",
       "0         0.038676  \n",
       "6         0.036639  \n",
       "1         0.040718  \n",
       "12        0.042125  \n",
       "2         0.051358  \n",
       "5         0.031451  \n",
       "10        0.038592  \n",
       "7         0.035034  \n",
       "13        0.025619  \n",
       "8         0.029551  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", XGBClassifier(objective=\"binary:logistic\", learning_rate=0.1, max_depth=10, random_state=742, n_jobs=4))\n",
    "])\n",
    "grid_xgb = {\n",
    "    \"classifier__reg_alpha\": np.logspace(-2, 2, 5),\n",
    "    \"classifier__reg_lambda\": np.logspace(-2, 2, 5),\n",
    "}\n",
    "gs_xgb = GridSearchCV(model_xgb, grid_xgb, cv=skf)\n",
    "gs_xgb.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_xgb.cv_results_)[[\"param_classifier__reg_alpha\", \"param_classifier__reg_lambda\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'intervallo di confidenza calcolato sul *validation set* però ci rivela che i risultati del miglior modello trovato non sono pessimi, in quanto si colloca davanti al miglior modello individuato con \"Perceptron\", ma si colloca anche dietro ai precedenti due migliori modelli già visti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7335414618685494, 0.8532000478492574)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb = gs_xgb.best_estimator_\n",
    "y_pred = best_xgb.predict(X_val)\n",
    "confidence_interval(best_xgb, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo infatti che ha una *precision* leggermente più bassa del miglior modello ottenuto con \"Perceptron\" per quanto riguarda gli episodi \"brutti\", mentre quella per gli episodi \"belli\" è cresciuta di molto. La *recall* della classe \"bad\" è decisamente più alta che nel caso del \"Perceptron\", mentre quella della classe \"good\" è più bassa. Complessivamente, ciascun *f1-score* è migliorato producendo un'accuratezza migliore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>81</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    81    15\n",
       "good   19    55"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_xgb.classes_, columns=best_xgb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.81      0.84      0.83        96\n",
      "        good       0.79      0.74      0.76        74\n",
      "\n",
      "    accuracy                           0.80       170\n",
      "   macro avg       0.80      0.79      0.80       170\n",
      "weighted avg       0.80      0.80      0.80       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione mediante Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo come ultimo metodo \"Random Forest\" per vedere se, estraendo regole di partizione delle istanze dal nostro *set* di dati, è capace di ottenere dei risultati migliori rispetto ai metodi già utilizzati. Osserviamo che i risultati migliori non si hanno necessariamente in corrispondenza di un maggior numero di alberi e nemmeno con una profondità degli stessi più alta. Ciò che rimane costante è il numero di istanze per considerare un nodo come una foglia, relativamente basso, e il numero di elementi da raggiungere in un nodo per effettuare lo *split*, un valore non troppo alto. I risultati non sono però soddisfacenti, migliori rispetto a quelli ottenuti con \"XGBoost\", ma sempre inferiori a quelli di tutti gli altri metodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>param_classifier__min_samples_leaf</th>\n",
       "      <th>param_classifier__min_samples_split</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.817179</td>\n",
       "      <td>0.028977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.814596</td>\n",
       "      <td>0.040968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.812090</td>\n",
       "      <td>0.031811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>500</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807078</td>\n",
       "      <td>0.014732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.807040</td>\n",
       "      <td>0.019609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.807040</td>\n",
       "      <td>0.019609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807040</td>\n",
       "      <td>0.019609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807001</td>\n",
       "      <td>0.031872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>500</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.806963</td>\n",
       "      <td>0.047197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>500</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.806963</td>\n",
       "      <td>0.041968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_classifier__n_estimators param_classifier__max_depth  \\\n",
       "37                            200                           4   \n",
       "38                            500                           4   \n",
       "40                            200                           4   \n",
       "71                            500                           8   \n",
       "2                             500                           2   \n",
       "5                             500                           2   \n",
       "8                             500                           2   \n",
       "34                            200                           4   \n",
       "35                            500                           4   \n",
       "53                            500                           4   \n",
       "\n",
       "   param_classifier__min_samples_leaf param_classifier__min_samples_split  \\\n",
       "37                                  2                                   3   \n",
       "38                                  2                                   3   \n",
       "40                                  2                                   5   \n",
       "71                                  2                                  10   \n",
       "2                                   1                                   3   \n",
       "5                                   1                                   5   \n",
       "8                                   1                                  10   \n",
       "34                                  1                                  10   \n",
       "35                                  1                                  10   \n",
       "53                                  4                                  10   \n",
       "\n",
       "    mean_test_score  std_test_score  \n",
       "37         0.817179        0.028977  \n",
       "38         0.814596        0.040968  \n",
       "40         0.812090        0.031811  \n",
       "71         0.807078        0.014732  \n",
       "2          0.807040        0.019609  \n",
       "5          0.807040        0.019609  \n",
       "8          0.807040        0.019609  \n",
       "34         0.807001        0.031872  \n",
       "35         0.806963        0.047197  \n",
       "53         0.806963        0.041968  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnd = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", RandomForestClassifier(n_jobs=-1, random_state=742, class_weight=\"balanced\"))\n",
    "])\n",
    "grid_rnd = {\n",
    "    \"classifier__n_estimators\": [100, 200, 500],\n",
    "    \"classifier__max_depth\": [2, 4, 8, 10],\n",
    "    \"classifier__min_samples_leaf\": [1, 2, 4],\n",
    "    \"classifier__min_samples_split\": [3, 5, 10]\n",
    "}\n",
    "gs_rnd = GridSearchCV(model_rnd, grid_rnd, cv=skf)\n",
    "gs_rnd.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_rnd.cv_results_)[[\"param_classifier__n_estimators\", \"param_classifier__max_depth\", \"param_classifier__min_samples_leaf\", \"param_classifier__min_samples_split\", \"mean_test_score\", \"std_test_score\"]] \\\n",
    "  .sort_values(\"mean_test_score\", ascending=False) \\\n",
    "  .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'intervallo di accuratezza mostra un miglioramento della stessa rispetto a \"Perceptron\", ma il valore calcolato è comunque peggiore di quello ottenuto dal miglior modello costruito con \"XGBoost\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7207997534601619, 0.8429322852883191)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnd = gs_rnd.best_estimator_\n",
    "y_pred = best_rnd.predict(X_val)\n",
    "confidence_interval(best_rnd, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il miglior modello individuato ha una *precision* per gli episodi \"belli\" che è la stessa del miglior modello ottenuto con \"XGBoost\", mentre quella degli episodi con valutazione \"brutto\" è di poco inferiore. La *recall* per la classe \"bad\" è leggermente più alta del modello di \"XGBoost\", mentre invece quella della classe \"good\" è al contrario leggermente più bassa. Questo fa sì che le accuratezze di questi due modelli si discostino di solo l'1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>22</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good\n",
       "bad    82    14\n",
       "good   22    52"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_pred), index=best_rnd.classes_, columns=best_rnd.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.79      0.85      0.82        96\n",
      "        good       0.79      0.70      0.74        74\n",
      "\n",
      "    accuracy                           0.79       170\n",
      "   macro avg       0.79      0.78      0.78       170\n",
      "weighted avg       0.79      0.79      0.79       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confronto tra modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per poter confrontare più accuratamente i modelli definiamo prima la funzione capace di calcolare se la differenza di accuratezza tra due modelli è statisticamente significativa. Questa ci restituirà gli estremi dell'intervallo di variazione, oltre ad un booleano che ci dice se effettivamente la differenza tra le due accuratezze è statisticamente significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_difference_interval(accuracy1, accuracy2, X_size, confidence_level=0.99):\n",
    "    abs_diff = abs(accuracy1 - accuracy2)\n",
    "    radius = norm.ppf((1 + confidence_level) / 2) * np.sqrt((accuracy1 * (1 - accuracy1) + accuracy2 * (1 - accuracy2)) / X_size)\n",
    "    min_int = abs_diff - radius\n",
    "    max_int = abs_diff + radius\n",
    "    return min_int, max_int, min_int > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poi defininiamo un modello \"dummy\", ovverosia un modello che effettui le sue predizioni senza alcun tipo di conoscenza sulle regole interne al *dataset* che permettono di associare un'istanza alla sua classe, facendo perciò le sue predizioni in maniera semi-casuale. In realtà, per fare in modo che il nostro modello di *benchmark* abbia un'accuratezza maggiore, in modo da mettere maggiormente alla prova i modelli precedentemente estratti, non effettuerà la classificazione in maniera completamente casuale, ma assegnerà tutte le istanze alla classe più frequente nel *dataset*. Questo perchè sappiamo esistere un leggero sbilanciamento tra le istanze delle due classi e perciò tutti i modelli che sono più propensi ad assegnare le istanze alla classe più frequente piuttosto che all'altra, avranno automaticamente un certo vantaggio sugli altri modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5647058823529412"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", DummyClassifier(strategy=\"most_frequent\", random_state=742))\n",
    "])\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo che tutti i migliori modelli estratti mediante i metodi utilizzati in precedenza sono significativamente migliori a livello statistico del modello di benchmark con un livello di confidenza del 99%, segno che hanno effetivamente appreso qualcosa dal dataset. Designamo come modello migliore quello ottenuto mediante regressione logistica, che è il secondo con accuratezza più alta. Anche se quest'ultima non è diversa in maniera statisticamente significativa da quella dei migliori modelli ottenuti con \"XGBoost\", \"Random Forest\" o \"Support Vector Machines\", il modello è stato comunque molto capace nel bilanciare *precision* e *recall* delle due classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>lower_bound_interval_dummy</th>\n",
       "      <th>upper_bound_interval_dummy</th>\n",
       "      <th>statistically_significative_dummy</th>\n",
       "      <th>lower_bound_interval_best</th>\n",
       "      <th>upper_bound_interval_best</th>\n",
       "      <th>statistically_significative_best</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.564706</td>\n",
       "      <td>-0.138519</td>\n",
       "      <td>0.138519</td>\n",
       "      <td>False</td>\n",
       "      <td>0.190255</td>\n",
       "      <td>0.374451</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.274205</td>\n",
       "      <td>True</td>\n",
       "      <td>0.053891</td>\n",
       "      <td>0.228462</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.096610</td>\n",
       "      <td>0.350449</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.023026</td>\n",
       "      <td>0.140673</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.109443</td>\n",
       "      <td>0.361145</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.033829</td>\n",
       "      <td>0.127947</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.148264</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.065929</td>\n",
       "      <td>0.089458</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machines</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.161316</td>\n",
       "      <td>0.403390</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.076517</td>\n",
       "      <td>0.076517</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         accuracy  lower_bound_interval_dummy  \\\n",
       "model                                                           \n",
       "Dummy                    0.564706                   -0.138519   \n",
       "Perceptron               0.705882                    0.008148   \n",
       "Random Forest            0.788235                    0.096610   \n",
       "XGBoost                  0.800000                    0.109443   \n",
       "Logistic Regression      0.835294                    0.148264   \n",
       "Support Vector Machines  0.847059                    0.161316   \n",
       "\n",
       "                         upper_bound_interval_dummy  \\\n",
       "model                                                 \n",
       "Dummy                                      0.138519   \n",
       "Perceptron                                 0.274205   \n",
       "Random Forest                              0.350449   \n",
       "XGBoost                                    0.361145   \n",
       "Logistic Regression                        0.392913   \n",
       "Support Vector Machines                    0.403390   \n",
       "\n",
       "                         statistically_significative_dummy  \\\n",
       "model                                                        \n",
       "Dummy                                                False   \n",
       "Perceptron                                            True   \n",
       "Random Forest                                         True   \n",
       "XGBoost                                               True   \n",
       "Logistic Regression                                   True   \n",
       "Support Vector Machines                               True   \n",
       "\n",
       "                         lower_bound_interval_best  upper_bound_interval_best  \\\n",
       "model                                                                           \n",
       "Dummy                                     0.190255                   0.374451   \n",
       "Perceptron                                0.053891                   0.228462   \n",
       "Random Forest                            -0.023026                   0.140673   \n",
       "XGBoost                                  -0.033829                   0.127947   \n",
       "Logistic Regression                      -0.065929                   0.089458   \n",
       "Support Vector Machines                  -0.076517                   0.076517   \n",
       "\n",
       "                         statistically_significative_best  \n",
       "model                                                      \n",
       "Dummy                                                True  \n",
       "Perceptron                                           True  \n",
       "Random Forest                                       False  \n",
       "XGBoost                                             False  \n",
       "Logistic Regression                                 False  \n",
       "Support Vector Machines                             False  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [dummy, best_perc, best_logreg, best_svm, best_xgb, best_rnd]\n",
    "scores = list(map(lambda model: model.score(X_val, y_val), models))\n",
    "data = []\n",
    "dummy_accuracy = dummy.score(X_val, y_val)\n",
    "best_accuracy = max(scores)\n",
    "for model in models:\n",
    "    accuracy = model.score(X_val, y_val)\n",
    "    min_int_dummy, max_int_dummy, significative_dummy = models_difference_interval(accuracy, dummy_accuracy, X_val.shape[0])\n",
    "    min_int_best, max_int_best, significative_best = models_difference_interval(accuracy, best_accuracy, X_val.shape[0], 0.95)\n",
    "    data.append((accuracy, min_int_dummy, max_int_dummy, significative_dummy, min_int_best, max_int_best, significative_best))\n",
    "pd.DataFrame(data,\n",
    "             index=pd.Index([\"Dummy\", \"Perceptron\", \"Logistic Regression\", \"Support Vector Machines\", \"XGBoost\", \"Random Forest\"], name=\"model\"),\n",
    "             columns=[\"accuracy\", \"lower_bound_interval_dummy\", \"upper_bound_interval_dummy\", \"statistically_significative_dummy\", \"lower_bound_interval_best\", \"upper_bound_interval_best\", \"statistically_significative_best\"]) \\\n",
    "  .sort_values(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretazione della conoscenza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scelto il modello migliore, osserviamo perciò quali sono le *feature* che sono per esso più incisive per la scelta dell'una o dell'altra classe. Le *feature* che maggiormente portano un episodio ad essere considerato \"bello\" sembrano essere le esclamazioni - come \"ah\", \"hmmm\", \"oh\", \"huh\", \"hey\", \"uh huh\" - e le loro varianti, le risate - ad esempio \"heh heh heh\" -, ma anche altre parole come \"minute\", \"sir\", \"come\" ed \"everybody\". Tutto sommato, queste informazioni sono in linea con quanto ci aspettavamo: gli episodi migliori della serie sono quelli dove vengono trasmesse più emozioni e il pubblico si sente più coinvolto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ah                0.034807\n",
       "heh               0.033439\n",
       "etc               0.030776\n",
       "hmmm              0.030340\n",
       "heh heh           0.029744\n",
       "oh                0.028886\n",
       "Simpson Home      0.026432\n",
       "ohh               0.025481\n",
       "minut             0.025284\n",
       "hey wait          0.025239\n",
       "Maude Flanders    0.025042\n",
       "sir               0.024916\n",
       "huh               0.024537\n",
       "hey               0.024314\n",
       "heh heh heh       0.024228\n",
       "uh huh            0.024035\n",
       "come              0.023422\n",
       "everybodi         0.023223\n",
       "ahh               0.023176\n",
       "uh oh             0.022366\n",
       "dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = pd.Series(best_logreg.named_steps[\"classifier\"].coef_[0], index).sort_values(ascending=False)\n",
    "coefs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se consideriamo solo gli \"1-gram\" che rendono più probabile che l'episodio abbia un buon *rating*, vediamo come questa teoria è confermata, trovando oltre alle già citate parole anche \"ewww\". Troviamo però anche due cognomi, \"mcclure\", appartenente al personaggio di Troy McClure, e \"simpson\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ah           0.034807\n",
       "heh          0.033439\n",
       "etc          0.030776\n",
       "hmmm         0.030340\n",
       "oh           0.028886\n",
       "ohh          0.025481\n",
       "minut        0.025284\n",
       "sir          0.024916\n",
       "huh          0.024537\n",
       "hey          0.024314\n",
       "come         0.023422\n",
       "everybodi    0.023223\n",
       "ahh          0.023176\n",
       "ewww         0.022293\n",
       "hello        0.022077\n",
       "ooooh        0.021631\n",
       "simpson      0.021411\n",
       "mcclure      0.021081\n",
       "hmm          0.020794\n",
       "problemo     0.020686\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs[~coefs.index.str.contains(r\"[A-Z ]\")].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al contrario, le *feature* che portano un episodio ad essere considerato \"brutto\" sembrano essere parole che rappresentano giudizi o in generale aggettivi come \"great\", \"huge\", \"awesome\", \"final\", \"sweet\", ma anche parole di uso comune come \"everyone\", \"back\", \"made\", \"use\", \"move\", \"welcome\"... Forse il loro uso in maniera sovrabbondante potrebbe far perdere di significato l'episodio. Troviamo inoltre quella che sembra essere una parola che Marge usa spesso per apostrofare Homer, \"homie\", nonchè il nome di un personaggio, ovvero \"lenny\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lenni           -0.018666\n",
       "oh god          -0.019223\n",
       "na              -0.019448\n",
       "back            -0.019579\n",
       "huge            -0.019656\n",
       "everyon         -0.019709\n",
       "made            -0.020135\n",
       "Carl Carlson    -0.020175\n",
       "great           -0.020190\n",
       "Springfield     -0.020316\n",
       "make            -0.021421\n",
       "use             -0.021571\n",
       "move            -0.021936\n",
       "Lenny Leonard   -0.023717\n",
       "final           -0.024238\n",
       "awesom          -0.024263\n",
       "homi            -0.024809\n",
       "welcom          -0.026117\n",
       "sweeti          -0.026576\n",
       "till            -0.028848\n",
       "dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se consideriamo solo gli \"1-gram\", oltre alle già discusse parole, troviamo anche \"disco\" e \"god\". Evidentemente gli episodi dove si parla di temi religiosi non sono stati molto apprezzati. La parola \"disco\" fa probabilmente anch'essa riferimento al nome di un personaggio, \"Disco Stu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "focu      -0.017902\n",
       "disco     -0.018030\n",
       "god       -0.018382\n",
       "never     -0.018621\n",
       "lenni     -0.018666\n",
       "na        -0.019448\n",
       "back      -0.019579\n",
       "huge      -0.019656\n",
       "everyon   -0.019709\n",
       "made      -0.020135\n",
       "great     -0.020190\n",
       "make      -0.021421\n",
       "use       -0.021571\n",
       "move      -0.021936\n",
       "final     -0.024238\n",
       "awesom    -0.024263\n",
       "homi      -0.024809\n",
       "welcom    -0.026117\n",
       "sweeti    -0.026576\n",
       "till      -0.028848\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs[~coefs.index.str.contains(r\"[A-Z ]\")].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ci concentriamo esclusivamente sui personaggi osserviamo che tra quelli che più portano una puntata ad essere apprezzata troviamo i già visti \"Maude Flanders\" e la \"signorina Hoover\", ma anche la \"signora della mensa\", il \"signor Smithers\", gli anziani e \"Jasper\", uno degli ospiti della casa di riposo di Springfield, ma anche \"Chuck\" e \"Janey\", due compagni di scuola di Bart e Lisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Maude Flanders     0.025042\n",
       "Miss Hoover        0.021896\n",
       "Lunchlady Doris    0.020350\n",
       "Waylon Smithers    0.019116\n",
       "Lewis Clark        0.019043\n",
       "Singers            0.017712\n",
       "Old People         0.016658\n",
       "Chuck              0.015798\n",
       "JANEY              0.014545\n",
       "Jasper Beardly     0.014542\n",
       "dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_characters = coefs[characters_cols].sort_values(ascending=False)\n",
    "best_characters.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tra i personaggi che invece abbassano il gradimento di una puntata troviamo il \"ricco texano\", la businesswoman \"Lindsey Naegle\", \"Brandine\", cioè la moglie di Cletus il bifolco, nonchè \"Cletus il bifolco\" stesso. Quello che hanno in comune questi personaggi è di essere tutti intrinsecamente repubblicani, rappresentando la parte più conservatrice degli Stati Uniti. La serie, di chiaro stampo democratico, satirizza molto questi personaggi e questo potrebbe non essere gradito al pubblico fortemente repubblicano della rete televisiva. Tra gli altri personaggi a sorpresa troviamo \"Secco\", la madre di Nelson, l'\"adolescente dalla voce stridula\", ma anche \"Disco Stu\" e \"Lenny\" e \"Carl\", i nomi dei quali avevamo già incontrati in precedenza, come personaggi o come \"1-gram\" dei loro nomi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "DOLPH                 -0.012443\n",
       "The Rich Texan        -0.013368\n",
       "Lindsay Naegle        -0.014731\n",
       "Mrs. Muntz            -0.014794\n",
       "Brandine Del Roy      -0.015579\n",
       "Disco Stu             -0.016485\n",
       "Squeaky-Voiced Teen   -0.016614\n",
       "Cletus Spuckler       -0.017582\n",
       "Carl Carlson          -0.020175\n",
       "Lenny Leonard         -0.023717\n",
       "dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_characters.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tra i luoghi in cui si svolgono le puntate de \"I Simpson\" che più portano ad alzare il gradimento di una puntata ci sono casa Simpson e le sue stanze, le stanze della scuola elementare di Springfield, la centrale nucleare, ma anche altri luoghi meno riconoscibili come le vie della città o le automobili. I primi tre luoghi, casa Simpson, la scuola elementare e la centrale nucleare, sono i luoghi dove chiaramente si svolgono il maggior numero di scene degli episodi, i luoghi più ricorrenti, perchè è dove i personaggi passano maggiormente il loro tempo. Evidentemente, quando non è chiaro il luogo in cui si svolge l'azione, nel *dataset* è stato semplicemente etichettato come \"Somewhere\" e così lì si svolgono molte scene della serie tv e questo porta il suo coefficiente ad essere alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Simpson Home                       0.026432\n",
       "Somewhere                          0.019351\n",
       "Simpson Kitchen                    0.015717\n",
       "Simpson Dining Room                0.015626\n",
       "Miss Hoover's Classroom            0.014022\n",
       "Springfield Nuclear Power Plant    0.013393\n",
       "Street                             0.013048\n",
       "Selma's Car                        0.013000\n",
       "Downtown Street                    0.012837\n",
       "Simpson Bathroom                   0.012661\n",
       "dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_locations = coefs[locations_cols].sort_values(ascending=False)\n",
    "best_locations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tra i luoghi che invece portano il pubblico a dire che un episodio è \"brutto\" troviamo invece molte più *location* di contorno come il castello di riposo, la spiaggia, le automobili di Homer e di Marge, il tribunale, il garage di casa Simpson, la via dove abitano i Simpson, \"Evergreen Terrace\", lo stadio, la città di Springfield in generale, che avevamo già incontrato. Evidentemente in questi luoghi si svolgono scene meno rilevanti ai fini della trama o comunque sono luoghi dove più spesso si svolgono scene non interessanti per il pubblico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Springfield Retirement Castle   -0.011866\n",
       "Marge's Car                     -0.011928\n",
       "Beach                           -0.012087\n",
       "Homer's Car                     -0.012186\n",
       "Announcer's Booth               -0.012697\n",
       "Court                           -0.012950\n",
       "Simpson Garage                  -0.014004\n",
       "Evergreen Terrace               -0.015091\n",
       "STADIUM                         -0.015761\n",
       "Springfield                     -0.020316\n",
       "dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_locations.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo come l'intercetta del nostro modello sia una valore molto piccolo, approssimabile con 0. Questo significa che il nostro modello non conferisce a priori un bonus o un malus ad un episodio prima di classificarlo e perciò entrambe le valutazioni, \"bello\" o \"brutto\", hanno la stessa probabilità di essere assegnate ad un episodio a partire dalle parole conenute nel copione di quest'ultimo, dai personaggi e dalle *location* che compaiono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5737984059353544e-06"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_logreg.named_steps[\"classifier\"].intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test di reti neurali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione tramite multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo quindi un approccio con un maggior numero di *feature*, ma dove queste sono autonomamente apprese dal metodo stesso, quindi utilizzando una rete neurale. In cambio sacrifichiamo la possibilità di interpretare il nostro modello. Prima di tutto testiamo quella che la libreria \"scikit-learn\" ci fornisce di base, ovverosia \"Multi-Layer Perceptron\". Lo *score* medio con i parametri scelti non è troppo distante da quello degli altri modelli valutati precedentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.862981</td>\n",
       "      <td>0.021324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score\n",
       "0         0.862981        0.021324"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"classifier\", MLPClassifier(shuffle=True, hidden_layer_sizes=(500, 20), batch_size=10, random_state=742))\n",
    "])\n",
    "gs = GridSearchCV(mlp, {}, cv=skf, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "pd.DataFrame(gs.cv_results_)[[\"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche l'accuratezza non è male, ha un valore comparabile con quella del miglior modello ottenuto con \"Random Forest\", però non è chiaramente capace di superare il miglior modello in assoluto che siamo riusciti ad individuare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.788235294117647"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificazione mediante reti neurali convoluzionali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In seconda battuta proviamo a costruire una rete neurale più complessa con l'aiuto delle librerie \"Keras\" e \"TensorFlow\", vedendo se un approccio che sfrutta una rete neurale convoluzionale è capace di ottenere dei risultati migliori. Per prima cosa aggiungiamo una colonna fittizia al nostro dataset per rendere le feature scomponibili in sottogruppi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 8390)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nn = hstack([X_train, np.zeros((X_train.shape[0], 1))])\n",
    "X_train_nn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inseriamo un primo livello di \"reshape\" per ottenere una matrice 2D da ciascuna istanza, applichiamo poi la convoluzione, e infine facciamo passare gli output attraverso due strati simili a quelli inseriti nel \"Multi-Layer Perceptron\". Sul primo applichiamo la stessa regolarizzazione del modello migliore trovato in precedenza, mentre prima del secondo applichiamo uno strato di \"dropout\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 839, 10)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 830, 500)          50500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 830, 500)          250500    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 415000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 415000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                8300020   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 8,601,062\n",
      "Trainable params: 8,601,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential([\n",
    "    Reshape((839, 10), input_shape=(X_train_nn.shape[1],)),\n",
    "    Conv1D(500, 10, activation=\"relu\"),\n",
    "    Dense(500, activation=\"relu\", kernel_regularizer=l2(100)),\n",
    "    Flatten(),\n",
    "    Dropout(0.1, seed=742),\n",
    "    Dense(20, activation=\"relu\"),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codifichiamo le classi da predire per ciascuna istanza in formato \"one-hot encoding\", compiliamo il modello facendoci restituire anche la metrica di accuratezza e poi effettuiamo il *fitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "394/394 [==============================] - 12s 30ms/step - loss: 24980.4072 - accuracy: 0.4746\n",
      "Epoch 2/10\n",
      "394/394 [==============================] - 12s 31ms/step - loss: 3053.8943 - accuracy: 0.5838\n",
      "Epoch 3/10\n",
      "394/394 [==============================] - 13s 32ms/step - loss: 197.3589 - accuracy: 0.5812\n",
      "Epoch 4/10\n",
      "394/394 [==============================] - 13s 32ms/step - loss: 6.2652 - accuracy: 0.5812\n",
      "Epoch 5/10\n",
      "394/394 [==============================] - 12s 32ms/step - loss: 0.7331 - accuracy: 0.5812\n",
      "Epoch 6/10\n",
      "394/394 [==============================] - 13s 32ms/step - loss: 0.6883 - accuracy: 0.5812\n",
      "Epoch 7/10\n",
      "394/394 [==============================] - 12s 32ms/step - loss: 0.6838 - accuracy: 0.5812\n",
      "Epoch 8/10\n",
      "394/394 [==============================] - 13s 32ms/step - loss: 0.6850 - accuracy: 0.5812\n",
      "Epoch 9/10\n",
      "394/394 [==============================] - 12s 32ms/step - loss: 0.6856 - accuracy: 0.5812\n",
      "Epoch 10/10\n",
      "394/394 [==============================] - 12s 31ms/step - loss: 0.6864 - accuracy: 0.5812\n"
     ]
    }
   ],
   "source": [
    "y_train_nn = to_categorical(np.where(y_train == \"good\", 1, 0))\n",
    "nn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "fit_history = nn.fit(X_train_nn.toarray(), y_train_nn, epochs=10, batch_size=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine valutiamo il modello sul *validation set*, dopo avere adattato anche la sua *shape*. La *categorical cross-entropy* non sembra essere particolarmente alta, ma nemmeno troppo bassa, mentre l'accuratezza, anche se sempre migliore di quella del modello benchmark, è la più bassa tra tutte quelle dei modelli valutati in precedenza. È infatti più bassa di quella del modello ottenuto da \"Perceptron\", il più scarso tra i metodi che usano *statistical learning*, ma anche di quella del modello ottenuto da \"Multi-Layer Perceptron\", l'altra rete neurale che abbiamo addestrato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6849482024417204, 0.5647059082984924]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_nn = hstack([X_val, np.zeros((X_val.shape[0], 1))])\n",
    "y_val_nn = to_categorical(np.where(y_val == \"good\", 1, 0))\n",
    "nn.evaluate(X_val_nn.toarray(), y_val_nn, batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataIntensive",
   "language": "python",
   "name": "dataintensive"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
